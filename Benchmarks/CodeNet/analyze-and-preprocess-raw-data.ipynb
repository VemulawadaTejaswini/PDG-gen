{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddharthsa/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu101\n",
      "10.1\n",
      "3.8.18\n",
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import glob\n",
    "import tqdm\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "from typing import Callable, List, Optional, Dict\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    Batch\n",
    "    )\n",
    "import torch_geometric.datasets as datasets\n",
    "import torch_geometric.transforms as transforms\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, silhouette_score\n",
    "\n",
    "# To ensure determinism\n",
    "seed = 1234\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed)\n",
    "\n",
    "# Check versions\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(platform.python_version())\n",
    "print(torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Java Code Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all accepted Java submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4053/4053 [10:24<00:00,  6.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "metadata_folder = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/metadata\"\n",
    "dataset_folder = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/Java250_data\"\n",
    "metadata_files = glob.glob(os.path.join(metadata_folder, '*.csv'))\n",
    "allowed_status = [\"Accepted\"]\n",
    "\n",
    "accepted_submissions = {}\n",
    "for file_loc in tqdm.tqdm(metadata_files):\n",
    "    problem_id = file_loc[file_loc.rindex(\"/\") + 1: -4]\n",
    "    metadata_file = pd.read_csv(file_loc)\n",
    "    accepted_submissions[problem_id] = []\n",
    "    for index, row in metadata_file.iterrows():\n",
    "        if row[\"filename_ext\"] == \"java\" and row[\"status\"] in allowed_status:\n",
    "            code_file_location = f\"{dataset_folder}/{row['problem_id']}/{row['submission_id']}.java\"\n",
    "            if os.path.isfile(code_file_location):\n",
    "                accepted_submissions[problem_id].append(code_file_location)\n",
    "            else:\n",
    "                continue\n",
    "                print(code_file_location)\n",
    "                print(\"code file doesn't exist!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the code-snippets that have only one class and one method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4053/4053 [04:10<00:00, 16.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import javalang\n",
    "from javalang.parser import JavaSyntaxError\n",
    "\n",
    "def count_methods(java_file_loc):\n",
    "  \n",
    "  with open(java_file_loc, 'r') as f:\n",
    "    java_source = f.read()\n",
    "\n",
    "  java_class = javalang.parse.parse(java_source)\n",
    "  method_count = sum(1 for _, node in java_class.filter(javalang.tree.MethodDeclaration))\n",
    "  class_count = sum(1 for _, node in java_class.filter(javalang.tree.TypeDeclaration))\n",
    "  return class_count, method_count\n",
    "\n",
    "\n",
    "accepted_submissions_filtered = {}\n",
    "for pid in tqdm.tqdm(accepted_submissions):\n",
    "    accepted_submissions_filtered[pid] = []\n",
    "    for path in accepted_submissions[pid]:\n",
    "        try:\n",
    "            #print(path)\n",
    "            class_count, method_count = count_methods(path)\n",
    "            if class_count == 1 and method_count == 1:\n",
    "                accepted_submissions_filtered[pid].append(path) \n",
    "        except (AssertionError, JavaSyntaxError, javalang.tokenizer.LexerError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top 5 projects by submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('p02381', 280), ('p02419', 280), ('p02396', 277), ('p02400', 277), ('p02399', 277), ('p02415', 275), ('p02421', 275), ('p02416', 274), ('p02407', 272), ('p02417', 271)]\n",
      "Total accepted java submissions in top 10 problems:  2758\n"
     ]
    }
   ],
   "source": [
    "accepted_submissions_count = {}\n",
    "for pid in accepted_submissions_filtered:\n",
    "    accepted_submissions_count[pid] = len(accepted_submissions_filtered[pid])\n",
    "    \n",
    "accepted_submissions_count_sorted = sorted(accepted_submissions_count.items(), key=lambda x:x[1], reverse = True)\n",
    "print(accepted_submissions_count_sorted[:10])\n",
    "\n",
    "sum_ = 0\n",
    "for pid in accepted_submissions_count_sorted[:10]:\n",
    "    sum_ += pid[1]\n",
    "print(\"Total accepted java submissions in top 10 problems: \", sum_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the filtered Java files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4053/4053 [00:03<00:00, 1027.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "output_folder_location = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/Java250_accepted_sols_1_method\"\n",
    "\n",
    "for pid, count in tqdm.tqdm(accepted_submissions_count_sorted):\n",
    "    if count < 20:\n",
    "        continue\n",
    "    output_project_folder_location = output_folder_location + \"/\" + pid\n",
    "    if not os.path.exists(output_project_folder_location):\n",
    "        os.mkdir(output_project_folder_location)\n",
    "    \n",
    "    for original_file in accepted_submissions_filtered[pid]:\n",
    "        file_name = original_file[original_file.rindex(\"/\") + 1:]\n",
    "        shutil.copyfile(original_file, output_project_folder_location + \"/\" + file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess for PDG Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:33<00:00,  3.40s/it]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import glob\n",
    "import tqdm\n",
    "import pyparsing\n",
    "\n",
    "input_folder_location = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/Java250_accepted_sols_1_method\"\n",
    "output_folder_location = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/processed_data_for_pdg_java250\"\n",
    "\n",
    "#projects_to_consider = ['p02388', 'p02389', 'p02391', 'p02393', 'p02396']\n",
    "projects_to_consider = ['p02381', 'p02419', 'p02396', 'p02400', 'p02399', 'p02415', 'p02421', 'p02416', 'p02407', 'p02417']\n",
    "\n",
    "def copyFile(input_file_path, output_file_path):\n",
    "    \n",
    "    input_file = open(input_file_path, \"r\")\n",
    "    newfile = open(output_file_path, \"w+\")\n",
    "    \n",
    "    # Remove all comments\n",
    "    original_code = input_file.read()\n",
    "    commentFilter = pyparsing.javaStyleComment.suppress()\n",
    "    modified_code = commentFilter.transformString(original_code)\n",
    "\n",
    "    for line in modified_code.split(\"\\n\"):\n",
    "        line = line.replace('\\u00A0', \" \")\n",
    "        \n",
    "        # Skip empty lines, lines like @Test\n",
    "        if len(line.strip()) == 0 or line.strip().startswith(\"@\"):\n",
    "            continue\n",
    "        \n",
    "        # Remove import statements and packages\n",
    "        if line.startswith(\"import\") or line.startswith(\"package\") or line.strip().replace(\"\\n\", \"\").strip() == \"\":\n",
    "            continue\n",
    "        \n",
    "        # Add newline at the end\n",
    "        if not line.endswith(\"\\n\"):\n",
    "            line += \"\\n\"\n",
    "        newfile.write(line)\n",
    "    \n",
    "    newfile.close()\n",
    "    input_file.close()\n",
    "\n",
    "for project in tqdm.tqdm(projects_to_consider):\n",
    "    input_project_folder = input_folder_location + \"/\" + project\n",
    "    java_files = glob.glob(os.path.join(input_project_folder, '*.java'))\n",
    "    \n",
    "    output_project_folder_location = output_folder_location + \"/\" + project\n",
    "    if not os.path.exists(output_project_folder_location):\n",
    "        os.mkdir(output_project_folder_location)\n",
    "        \n",
    "    for input_java_file_path in java_files:\n",
    "        file_name = input_java_file_path[input_java_file_path.rindex(\"/\") + 1:]\n",
    "        output_java_file_path = output_project_folder_location + \"/\" + file_name\n",
    "        copyFile(input_java_file_path, output_java_file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get The PGDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocess After PDG Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1007.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training, validation and test data size: 2202, 272 and 282\n",
      "\n",
      "Total dataset size:  2756\n",
      "\n",
      "Processing split:  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2202 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2202/2202 [00:00<00:00, 4667.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GOOD PDG DATA POINTS: 2185\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 2202\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing split:  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:00<00:00, 4694.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GOOD PDG DATA POINTS: 269\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 272\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing split:  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:00<00:00, 4687.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GOOD PDG DATA POINTS: 280\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 282\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "TOTAL GOOD PDG DATA POINTS IN DATASET: 2734\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS IN DATASET: 2756\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR IN DATASET: 0\n",
      "\n",
      "\n",
      "DATASET STATISTICS: {'train': [2202, 2185, 0], 'valid': [272, 269, 0], 'test': [282, 280, 0]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import tqdm\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "\"\"\" ALGORITHM\n",
    "\n",
    "a. Clean the raw edge info (eg. remove wrongly formatted edges, class edges etc.)\n",
    "b. Merge same code-lines into a single line/node\n",
    "e. Remove self-loops and duplicate edges\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "PRUNING_ERROR_COUNT, GOOD_DATA_POINTS, TOTAL_DATA_POINTS = 0, 0, 0\n",
    "PRUNING_ERROR_COUNT_IN_DATASET, GOOD_DATA_POINTS_IN_DATASET, TOTAL_DATA_POINTS_IN_DATASET = 0, 0, 0\n",
    "DATASET_STATISTICS = {}\n",
    "\n",
    "def get_pruned_pdg(pdg_file, output_pdg_file):\n",
    "    \n",
    "    global PRUNING_ERROR_COUNT, GOOD_DATA_POINTS, TOTAL_DATA_POINTS\n",
    "    \n",
    "    # all_edges = [bytes(l, 'utf-8').decode('utf-8', 'ignore').strip()\n",
    "    #              for l in pdg_file.readlines()]\n",
    "    all_edges = [l.replace(\"\\n\", \"\").replace(\"\\r\", \"\").strip()\n",
    "                 for l in pdg_file.readlines()]\n",
    "\n",
    "    # Remove unnecesssary edges(\"Entry\" edge, wrongly formatted edges etc.)\n",
    "    all_edges = all_edges[1:]\n",
    "    all_edges = [edge for edge in all_edges if edge.find(\n",
    "        \"-->\") != -1 and edge.count(\"$$\") == 2]\n",
    "    all_edges = [edge for edge in all_edges if len(edge.split(\"-->\")) == 2 and\n",
    "                 len(edge.split(\"-->\")[0].split(\"$$\")) == 2 and\n",
    "                 len(edge.split(\"-->\")[1].split(\"$$\")) == 2]\n",
    "\n",
    "    # Remove self-loops and duplicate edges\n",
    "    all_edges = [edge for edge in all_edges if edge.split(\"-->\")[0].split(\"$$\")[0].strip() != edge.split(\"-->\")[1].split(\"$$\")[0].strip()]\n",
    "    all_edges = list(set(all_edges))\n",
    "        \n",
    "    if len(all_edges) >= 5:\n",
    "        GOOD_DATA_POINTS += 1\n",
    "\n",
    "    all_edges = [edge + \"\\n\" for edge in all_edges]\n",
    "    output_pdg_file.writelines(all_edges)\n",
    "    if len(all_edges) > 0:\n",
    "        TOTAL_DATA_POINTS += 1\n",
    "\n",
    "    return output_pdg_file, len(all_edges)\n",
    "\n",
    "def split_data(pdg_files):\n",
    "    random.shuffle(pdg_files)\n",
    "    train_split_size, valid_split_size, test_split_size =  int(len(pdg_files) *0.8), \\\n",
    "                                                           int(len(pdg_files) *0.1), \\\n",
    "                                                           int(len(pdg_files) *0.1)\n",
    "    training_data = pdg_files[:train_split_size]\n",
    "    validation_data = pdg_files[train_split_size: train_split_size + valid_split_size]\n",
    "    test_data = pdg_files[train_split_size + valid_split_size:]\n",
    "    return training_data, validation_data, test_data\n",
    "\n",
    "PDG_FOLDER_LOCATION = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/pdg_data_java250\"\n",
    "OUTPUT_FOLDER_LOCATION = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/processed_pdg_data_java250\"\n",
    "#projects_to_consider = {'p02388': 0, 'p02389': 1, 'p02391': 2, 'p02393': 3, 'p02396': 4}\n",
    "projects_to_consider = {'p02381' : 0, 'p02419': 1, 'p02396': 2, 'p02400': 3, 'p02399': 4, 'p02415': 5, 'p02421': 6, 'p02416': 7, 'p02407': 8, 'p02417': 9}\n",
    "\n",
    "training_data, validation_data, test_data = [], [], []\n",
    "project_folders = glob.glob(os.path.join(PDG_FOLDER_LOCATION, '*'))\n",
    "pdg_files_list = []\n",
    "for project_folder in tqdm.tqdm(project_folders):\n",
    "    pdg_files = glob.glob(os.path.join(project_folder, '*.txt'))\n",
    "    s1, s2, s3 = split_data(pdg_files)\n",
    "    training_data.extend(s1)\n",
    "    validation_data.extend(s2)\n",
    "    test_data.extend(s3)\n",
    "    \n",
    "random.shuffle(training_data)\n",
    "random.shuffle(validation_data)\n",
    "random.shuffle(test_data)\n",
    "print(\"\\nTraining, validation and test data size: {}, {} and {}\".format(len(training_data), len(validation_data), len(test_data)))\n",
    "print(\"\\nTotal dataset size: \", len(training_data) + len(validation_data) + len(test_data))\n",
    "\n",
    "for split in [[\"train\", training_data], [\"valid\", validation_data], [\"test\", test_data]]:\n",
    "    print(\"\\nProcessing split: \", split[0])\n",
    "    OUTPUT_SPLIT_FOLDER_LOCATION = OUTPUT_FOLDER_LOCATION + \"/\" + split[0]\n",
    "    if not os.path.exists(OUTPUT_SPLIT_FOLDER_LOCATION):\n",
    "        os.makedirs(OUTPUT_SPLIT_FOLDER_LOCATION)\n",
    "    for pdg_file in tqdm.tqdm(split[1]):\n",
    "        original_pdg_file = open(pdg_file, 'r')\n",
    "        project_id = pdg_file[pdg_file.rindex(\"/\")+1:].split(\"_\")[2]\n",
    "        output_file_location = OUTPUT_SPLIT_FOLDER_LOCATION + \"/\" + pdg_file[pdg_file.rindex(\"/\")+1:-4] + \"_\" + str(projects_to_consider[project_id]) + \".txt\"\n",
    "        output_pdg_file = open(output_file_location, \"+w\")\n",
    "        try:\n",
    "            output_pdg_file, no_of_edges = get_pruned_pdg(original_pdg_file, output_pdg_file)\n",
    "        except Exception as e:\n",
    "            PRUNING_ERROR_COUNT += 1\n",
    "            print(\"\\nERROR WHILE PRUNING PDG\\n\")\n",
    "            print(\"\\nFile: {}\\n\".format(pdg_file))\n",
    "            print(\"\\nERROR: {}\\n\".format(e))\n",
    "            original_pdg_file.close()\n",
    "            output_pdg_file.close()\n",
    "            os.remove(output_file_location)\n",
    "        else:\n",
    "            output_pdg_file.close()\n",
    "            if no_of_edges == 0:\n",
    "                os.remove(output_file_location)\n",
    "            original_pdg_file.close()\n",
    "            \n",
    "    print(\"\\nGOOD PDG DATA POINTS: {}\\n\".format(GOOD_DATA_POINTS))\n",
    "    print(\"\\nTOTAL PDG DATA POINTS: {}\\n\".format(TOTAL_DATA_POINTS))\n",
    "    print(\"\\nTOTAL PRUNING ERROR: {}\\n\".format(PRUNING_ERROR_COUNT))\n",
    "    print(\"\\n=================================================================\\n\")\n",
    "    PRUNING_ERROR_COUNT_IN_DATASET += PRUNING_ERROR_COUNT\n",
    "    GOOD_DATA_POINTS_IN_DATASET += GOOD_DATA_POINTS\n",
    "    TOTAL_DATA_POINTS_IN_DATASET += TOTAL_DATA_POINTS\n",
    "    DATASET_STATISTICS[split[0]] = [TOTAL_DATA_POINTS, GOOD_DATA_POINTS, PRUNING_ERROR_COUNT]\n",
    "    PRUNING_ERROR_COUNT, GOOD_DATA_POINTS, TOTAL_DATA_POINTS = 0, 0, 0\n",
    "    \n",
    "print(\"\\nTOTAL GOOD PDG DATA POINTS IN DATASET: {}\\n\".format(GOOD_DATA_POINTS_IN_DATASET))\n",
    "print(\"\\nTOTAL PDG DATA POINTS IN DATASET: {}\\n\".format(TOTAL_DATA_POINTS_IN_DATASET))\n",
    "print(\"\\nTOTAL PRUNING ERROR IN DATASET: {}\\n\".format(PRUNING_ERROR_COUNT_IN_DATASET))\n",
    "print(\"\\nDATASET STATISTICS: {}\\n\".format(DATASET_STATISTICS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MuGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
