{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu101\n",
      "10.1\n",
      "3.8.18\n",
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import glob\n",
    "import tqdm\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "from typing import Callable, List, Optional, Dict\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    Batch\n",
    "    )\n",
    "import torch_geometric.datasets as datasets\n",
    "import torch_geometric.transforms as transforms\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, silhouette_score\n",
    "\n",
    "# To ensure determinism\n",
    "seed = 1234\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed)\n",
    "\n",
    "# Check versions\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(platform.python_version())\n",
    "print(torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalute Using CodeBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "#Set GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4,5\"\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Initialize the models\n",
    "codebert_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "codebert_model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "codebert_model = codebert_model.to(device)\n",
    "max_source_length= 512\n",
    "\n",
    "def get_code_embeddings_from_codebert(codelines):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    code = \" \".join(codelines)\n",
    "    source_tokens = codebert_tokenizer.tokenize(code)[:max_source_length-2]\n",
    "    source_tokens = [codebert_tokenizer.cls_token]+source_tokens+[codebert_tokenizer.sep_token]\n",
    "    source_ids =  codebert_tokenizer.convert_tokens_to_ids(source_tokens) \n",
    "    padding_length = max_source_length - len(source_ids)\n",
    "    source_ids+=[codebert_tokenizer.pad_token_id]*padding_length\n",
    "    source_ids = torch.tensor(source_ids)\n",
    "    \n",
    "    # tokens = []\n",
    "    # for code_line in codelines:\n",
    "    #     code_tokens = codebert_tokenizer.tokenize(code_line, truncation=True, max_length=510)\n",
    "    #     if tokens == []:\n",
    "    #         tokens = [codebert_tokenizer.cls_token] + code_tokens\n",
    "    #     else:\n",
    "    #         tokens = tokens + [codebert_tokenizer.sep_token] + code_tokens\n",
    "    # tokens = tokens + [codebert_tokenizer.eos_token]\n",
    "    # tokens_ids = torch.tensor(codebert_tokenizer.convert_tokens_to_ids(tokens))\n",
    "    source_ids = source_ids.to(device)\n",
    "    context_embeddings = codebert_model(source_ids[None,:])\n",
    "    cls_token_embedding = context_embeddings.last_hidden_state[0,0,:]\n",
    "    return cls_token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from unixcoder import UniXcoder\n",
    "\n",
    "#Set GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4,5\"\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Initialize the models\n",
    "unixcoder_model = UniXcoder(\"microsoft/unixcoder-base\")\n",
    "unixcoder_model = unixcoder_model.to(device)\n",
    "max_source_length= 512\n",
    "\n",
    "def get_code_embeddings_from_unixcoder(codelines):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    code = \" \".join(codelines)\n",
    "    tokens_ids = unixcoder_model.tokenize([code], max_length=512, mode=\"<encoder-only>\")\n",
    "    source_ids = torch.tensor(tokens_ids).to(device)\n",
    "    tokens_embeddings, code_embedding = unixcoder_model(source_ids)\n",
    "    return torch.flatten(code_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_llms(folders, model):\n",
    "  embeddings = {}\n",
    "  for label, folder in tqdm.tqdm(enumerate(folders)):\n",
    "    folder_name = folder.strip().split(\"/\")[-1]\n",
    "    print(\"\\nProcessing: {}\\n\".format(folder_name))\n",
    "    embeddings[folder_name] = []\n",
    "    files = glob.glob(os.path.join(folder, '*/*.java'))\n",
    "    print(\"\\nNumber of files: {}\\n\".format(len(files)))\n",
    "    count = 1\n",
    "    for file in files:\n",
    "      sample_name = file.split(\"/\")[-2].strip()\n",
    "      file_name = file.split(\"/\")[-1].strip()\n",
    "      if(count % 5 == 0):\n",
    "          print(\"\\nAt file: {}\\n\".format(count))\n",
    "                        \n",
    "      fp = open(file,'r')\n",
    "      lines = fp.readlines()\n",
    "      lines = [line for line in lines if not line.startswith(\"import\") and not len(line.strip('\\n')) == 0]\n",
    "      lines = [line.strip('\\n').strip(\" \") for line in lines]\n",
    "      if model == \"codebert\":\n",
    "        embedding = get_code_embeddings_from_codebert(lines)\n",
    "      elif model == \"unixcoder\":\n",
    "        embedding = get_code_embeddings_from_unixcoder(lines)\n",
    "      embedding = embedding.detach().numpy()\n",
    "      embeddings[folder_name].append([file_name, sample_name, embedding])\n",
    "      count += 1\n",
    "    \n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: ExecutorService.submit\n",
      "\n",
      "\n",
      "Number of files: 26\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:10, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Lock.lock\n",
      "\n",
      "\n",
      "Number of files: 6\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:13,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Timestamp.compareTo\n",
      "\n",
      "\n",
      "Number of files: 23\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:22,  7.45s/it]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FOLDER_LOCATION = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/after_preprocessing\"\n",
    "project_folders = [os.path.join(OUTPUT_FOLDER_LOCATION, name) for name in os.listdir(OUTPUT_FOLDER_LOCATION) if os.path.isdir(os.path.join(OUTPUT_FOLDER_LOCATION, name))]\n",
    "\n",
    "embeddings = get_embeddings_from_llms(project_folders, \"codebert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ExecutorService.submit': 6, 'Lock.lock': 2, 'Timestamp.compareTo': 3}\n"
     ]
    }
   ],
   "source": [
    "ground_truth_cluster_numbers = {}\n",
    "for folder in project_folders:\n",
    "    folder_name = folder.strip().split(\"/\")[-1]\n",
    "    ground_truth_cluster_numbers[folder_name] = len([name for name in os.listdir(folder) if os.path.isdir(os.path.join(folder, name))])\n",
    "print(ground_truth_cluster_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "def cluster_and_compare(embeddings, ground_truth_cluster_number):\n",
    "    \n",
    "    birch_model = Birch(n_clusters = ground_truth_cluster_number)\n",
    "    clusters_birch = birch_model.fit_predict([emb[2] for emb in embeddings])\n",
    "    cluster_count = {}\n",
    "    for i in range(len(clusters_birch)):\n",
    "        try:\n",
    "            cluster_count[clusters_birch[i]] += 1\n",
    "        except:\n",
    "            cluster_count[clusters_birch[i]] = 1\n",
    "    print(\"Cluster Counts: \", cluster_count)\n",
    "    \n",
    "    total_count, currect_count, wrong_count = 0, 0, 0\n",
    "    both_right, both_wrong = 0, 0\n",
    "    confusion_matrix = {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0}\n",
    "\n",
    "    original_one_final_two = []\n",
    "    original_two_final_one = []\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(embeddings))):\n",
    "        for j in range(i+1, len(embeddings)):\n",
    "            total_count += 1\n",
    "            if (embeddings[i][1] == embeddings[j][1]):\n",
    "                if (clusters_birch[i] == clusters_birch[j]):\n",
    "                    both_right += 1\n",
    "                    currect_count += 1\n",
    "                    confusion_matrix[\"TP\"] += 1\n",
    "                else:\n",
    "                    #original_one_final_two.append([embeddings[i][0], embeddings[j][0], embeddings[i][1], clusters_birch[i], clusters_birch[j]])\n",
    "                    wrong_count += 1\n",
    "                    confusion_matrix[\"FN\"] += 1\n",
    "            else:\n",
    "                if (clusters_birch[i] != clusters_birch[j]):\n",
    "                    both_wrong += 1\n",
    "                    currect_count += 1\n",
    "                    confusion_matrix[\"TN\"] += 1\n",
    "                else:\n",
    "                    #original_two_final_one.append([embeddings[i][0], embeddings[j][0], embeddings[i][1], embeddings[j][1], clusters_birch[i]])\n",
    "                    wrong_count += 1\n",
    "                    confusion_matrix[\"FP\"] += 1\n",
    "                    \n",
    "    print(\"total_count = {}, currect_count = {}, wrong_count = {}, both_right = {}, both_wrong = {}\".format(total_count, currect_count, wrong_count, both_right, both_wrong))\n",
    "    print(confusion_matrix)\n",
    "    precision = float(format(confusion_matrix[\"TP\"] / (confusion_matrix[\"TP\"] + confusion_matrix[\"FP\"]), \".3f\"))\n",
    "    recall = float(format(confusion_matrix[\"TP\"] / (confusion_matrix[\"TP\"] + confusion_matrix[\"FN\"]), \".3f\"))\n",
    "    f1_score = float(format(2 * (precision * recall) / (precision + recall), \".3f\"))\n",
    "    accuracy = float(format(currect_count/total_count, \".3f\"))\n",
    "    print(\"Precision: {}, Recall: {} and F1-Score: {}\".format(precision, recall, f1_score))\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    \n",
    "    return precision, recall, f1_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Analyzing:  ExecutorService.submit\n",
      "Cluster Counts:  {4: 8, 2: 6, 1: 4, 3: 4, 0: 3, 5: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 48814.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 325, currect_count = 230, wrong_count = 95, both_right = 12, both_wrong = 218\n",
      "{'TP': 12, 'TN': 218, 'FP': 46, 'FN': 49}\n",
      "Precision: 0.207, Recall: 0.197 and F1-Score: 0.202\n",
      "Accuracy: 0.708\n",
      "\n",
      "\n",
      "Analyzing:  Lock.lock\n",
      "Cluster Counts:  {0: 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 49932.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 15, currect_count = 10, wrong_count = 5, both_right = 10, both_wrong = 0\n",
      "{'TP': 10, 'TN': 0, 'FP': 5, 'FN': 0}\n",
      "Precision: 0.667, Recall: 1.0 and F1-Score: 0.8\n",
      "Accuracy: 0.667\n",
      "\n",
      "\n",
      "Analyzing:  Timestamp.compareTo\n",
      "Cluster Counts:  {1: 12, 2: 8, 0: 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 52485.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 253, currect_count = 132, wrong_count = 121, both_right = 47, both_wrong = 85\n",
      "{'TP': 47, 'TN': 85, 'FP': 50, 'FN': 71}\n",
      "Precision: 0.485, Recall: 0.398 and F1-Score: 0.437\n",
      "Accuracy: 0.522\n",
      "\n",
      "\n",
      "Average Precision: 0.453, Recall: 0.5316666666666667 and F1-Score: 0.4796666666666667 and Accuracy: 0.6323333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_precision, total_recall, total_f1_score, total_accuracy = 0, 0, 0, 0\n",
    "\n",
    "for folder in project_folders:\n",
    "    folder_name = folder.strip().split(\"/\")[-1]\n",
    "    print(\"\\n\\nAnalyzing: \", folder_name)\n",
    "    precision, recall, f1_score, accuracy = cluster_and_compare(embeddings[folder_name], ground_truth_cluster_numbers[folder_name])\n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_f1_score += f1_score\n",
    "    total_accuracy += accuracy\n",
    "    \n",
    "print(\"\\n\\nAverage Precision: {}, Recall: {} and F1-Score: {} and Accuracy: {}\".format(total_precision/len(project_folders), total_recall/len(project_folders), total_f1_score/len(project_folders), total_accuracy/len(project_folders)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Using UnixCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: ExecutorService.submit\n",
      "\n",
      "\n",
      "Number of files: 26\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:08,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Lock.lock\n",
      "\n",
      "\n",
      "Number of files: 6\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:09,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Timestamp.compareTo\n",
      "\n",
      "\n",
      "Number of files: 23\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:17,  5.87s/it]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FOLDER_LOCATION = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/after_preprocessing\"\n",
    "project_folders = [os.path.join(OUTPUT_FOLDER_LOCATION, name) for name in os.listdir(OUTPUT_FOLDER_LOCATION) if os.path.isdir(os.path.join(OUTPUT_FOLDER_LOCATION, name))]\n",
    "\n",
    "embeddings = get_embeddings_from_llms(project_folders, \"unixcoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ExecutorService.submit': 6, 'Lock.lock': 2, 'Timestamp.compareTo': 3}\n"
     ]
    }
   ],
   "source": [
    "ground_truth_cluster_numbers = {}\n",
    "for folder in project_folders:\n",
    "    folder_name = folder.strip().split(\"/\")[-1]\n",
    "    ground_truth_cluster_numbers[folder_name] = len([name for name in os.listdir(folder) if os.path.isdir(os.path.join(folder, name))])\n",
    "print(ground_truth_cluster_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Analyzing:  ExecutorService.submit\n",
      "Cluster Counts:  {4: 6, 0: 9, 2: 3, 3: 2, 1: 4, 5: 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 49681.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 325, currect_count = 236, wrong_count = 89, both_right = 17, both_wrong = 219\n",
      "{'TP': 17, 'TN': 219, 'FP': 45, 'FN': 44}\n",
      "Precision: 0.274, Recall: 0.279 and F1-Score: 0.276\n",
      "Accuracy: 0.726\n",
      "\n",
      "\n",
      "Analyzing:  Lock.lock\n",
      "Cluster Counts:  {0: 4, 1: 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 43690.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 15, currect_count = 6, wrong_count = 9, both_right = 4, both_wrong = 2\n",
      "{'TP': 4, 'TN': 2, 'FP': 3, 'FN': 6}\n",
      "Precision: 0.571, Recall: 0.4 and F1-Score: 0.47\n",
      "Accuracy: 0.4\n",
      "\n",
      "\n",
      "Analyzing:  Timestamp.compareTo\n",
      "Cluster Counts:  {0: 9, 1: 5, 2: 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 52571.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 253, currect_count = 165, wrong_count = 88, both_right = 56, both_wrong = 109\n",
      "{'TP': 56, 'TN': 109, 'FP': 26, 'FN': 62}\n",
      "Precision: 0.683, Recall: 0.475 and F1-Score: 0.56\n",
      "Accuracy: 0.652\n",
      "\n",
      "\n",
      "Average Precision: 0.5093333333333333, Recall: 0.38466666666666666 and F1-Score: 0.43533333333333335 and Accuracy: 0.5926666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_precision, total_recall, total_f1_score, total_accuracy = 0, 0, 0, 0\n",
    "\n",
    "for folder in project_folders:\n",
    "    folder_name = folder.strip().split(\"/\")[-1]\n",
    "    print(\"\\n\\nAnalyzing: \", folder_name)\n",
    "    precision, recall, f1_score, accuracy = cluster_and_compare(embeddings[folder_name], ground_truth_cluster_numbers[folder_name])\n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_f1_score += f1_score\n",
    "    total_accuracy += accuracy\n",
    "    \n",
    "print(\"\\n\\nAverage Precision: {}, Recall: {} and F1-Score: {} and Accuracy: {}\".format(total_precision/len(project_folders), total_recall/len(project_folders), total_f1_score/len(project_folders), total_accuracy/len(project_folders)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Using MuGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the PDGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import tqdm\n",
    "\n",
    "\"\"\" ALGORITHM\n",
    "\n",
    "a. Clean the raw edge info (eg. remove wrongly formatted edges, class edges etc.)\n",
    "b. Merge same code-lines into a single line/node\n",
    "c. Consider all nodes that are reachable from the API node\n",
    "d. Consider all nodes from which API node is reachable\n",
    "e. Add the all the edges(CD/FD) in the current subgraph\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "PRUNING_ERROR_COUNT, GOOD_DATA_POINTS, TOTAL_DATA_POINTS = 0, 0, 0\n",
    "PRUNING_ERROR_COUNT_IN_DATASET, GOOD_DATA_POINTS_IN_DATASET, TOTAL_DATA_POINTS_IN_DATASET = 0, 0, 0\n",
    "DATASET_STATISTICS = {}\n",
    "\n",
    "def get_pruned_pdg(pdg_file, output_pdg_file, api_name):\n",
    "    \n",
    "    global PRUNING_ERROR_COUNT, GOOD_DATA_POINTS, TOTAL_DATA_POINTS\n",
    "    \n",
    "    # all_edges = [bytes(l, 'utf-8').decode('utf-8', 'ignore').strip()\n",
    "    #              for l in pdg_file.readlines()]\n",
    "    all_edges = [l.replace(\"\\n\", \"\").replace(\"\\r\", \"\").strip()\n",
    "                 for l in pdg_file.readlines()]\n",
    "\n",
    "    # Remove unnecesssary edges(\"class\" edge, wrongly formatted edges etc.)\n",
    "    all_edges = [edge for edge in all_edges if edge.find(\n",
    "        \"-->\") != -1 and edge.count(\"$$\") == 2]\n",
    "    all_edges = [edge for edge in all_edges if len(edge.split(\"-->\")) == 2 and\n",
    "                 len(edge.split(\"-->\")[0].split(\"$$\")) == 2 and\n",
    "                 len(edge.split(\"-->\")[1].split(\"$$\")) == 2]\n",
    "    all_edges = [edge for edge in all_edges if edge.split(\"-->\")[0].find(\"Entry\") == -1 and\n",
    "                 edge.split(\"-->\")[0].find(\"class\") == -1]\n",
    "    #print(\"ALL EDGES : \\n\")\n",
    "    #print(all_edges, \"\\n\")\n",
    "\n",
    "    # Merge nodes referring to same code-line\n",
    "    line_mapping, edge_mapping = {}, {}\n",
    "    for edge in all_edges:\n",
    "        node_1, node_2 = edge[:edge.rindex(\"[\")].strip().split(\"-->\")\n",
    "        edge_type = edge[edge.rindex(\"[\") + 1: -1].strip()\n",
    "        line_numbers = []\n",
    "        for node in [node_1, node_2]:\n",
    "            line_number, line_code = node.strip().split(\"$$\")\n",
    "            line_number, line_code = line_number.strip(), line_code.strip()\n",
    "            line_numbers.append(line_number)\n",
    "            if line_number in line_mapping:\n",
    "                if line_mapping[line_number] != line_code:\n",
    "                    line_mapping[line_number] = line_code if len(line_code) > len(\n",
    "                        line_mapping[line_number]) else line_mapping[line_number]\n",
    "            else:\n",
    "                line_mapping[line_number] = line_code\n",
    "        if tuple(line_numbers) in edge_mapping:\n",
    "            edge_mapping[tuple(line_numbers)] = list(set(edge_mapping[tuple(line_numbers)] + [edge_type]))\n",
    "        else:\n",
    "            edge_mapping[tuple(line_numbers)] = [edge_type]\n",
    "\n",
    "    #print(\"NODE MAPPING : \\n\")\n",
    "    #print(line_mapping, \"\\n\")\n",
    "    #print(\"EDGE MAPPING : \\n\")\n",
    "    #print(edge_mapping, \"\\n\")\n",
    "\n",
    "    # Add all the nodes that are reachable to or from the API-NODE\n",
    "    api_nodes = []\n",
    "    for line in line_mapping:\n",
    "        if line_mapping[line].find(\".\" + api_name + \"(\") != -1:\n",
    "            api_nodes.append(line)\n",
    "    #print(\"API NODES : \\n\")\n",
    "    #print(api_nodes, \"\\n\")\n",
    "    \n",
    "    # Get vertices that are reachable from the API-NODE\n",
    "    vertices_from_api_node, previous_vertices = set(api_nodes), set(api_nodes)\n",
    "    while(True):\n",
    "        next_vertices = set([])\n",
    "        for edge in edge_mapping:\n",
    "            if edge[0] in list(previous_vertices) and edge[1] not in list(vertices_from_api_node):\n",
    "                next_vertices.add(edge[1])\n",
    "        if len(next_vertices) == 0:\n",
    "            break\n",
    "        else:\n",
    "            vertices_from_api_node = vertices_from_api_node.union(next_vertices)\n",
    "            previous_vertices = next_vertices\n",
    "    \n",
    "    # Get vertices from which the API-NODE is reachable\n",
    "    vertices_to_api_node, next_vertices = set(api_nodes), set(api_nodes)\n",
    "    while(True):\n",
    "        previous_vertices = set([])\n",
    "        for edge in edge_mapping:\n",
    "            if edge[1] in list(next_vertices) and edge[0] not in list(vertices_to_api_node):\n",
    "                previous_vertices.add(edge[0])\n",
    "        if len(previous_vertices) == 0:\n",
    "            break\n",
    "        else:\n",
    "            vertices_to_api_node = vertices_to_api_node.union(previous_vertices)\n",
    "            next_vertices = previous_vertices\n",
    "    \n",
    "    # All nodes in the final sub-graph\n",
    "    subgraph_vertices = list(vertices_from_api_node.union(vertices_to_api_node))\n",
    "\n",
    "    # Add all the edges(CD/FD) between the subgraph vertices\n",
    "    sub_graph_edges = {}\n",
    "    for edge in edge_mapping:\n",
    "        if edge[0] in subgraph_vertices and edge[1] in subgraph_vertices:\n",
    "            if edge in sub_graph_edges:\n",
    "                sub_graph_edges[edge] = list(set(sub_graph_edges[edge] + edge_mapping[edge]))\n",
    "            else:\n",
    "                sub_graph_edges[edge] = edge_mapping[edge]\n",
    "    #print(\"AFTER ADDING REST OF THE EDGES : \\n\")\n",
    "    #print(sub_graph_edges, \"\\n\")\n",
    "\n",
    "    # Remove self-loops from subgraph\n",
    "    sub_graph_edges_temp = {}\n",
    "    for edge in sub_graph_edges:\n",
    "        if edge[0] != edge[1]:\n",
    "            sub_graph_edges_temp[edge] = sub_graph_edges[edge]\n",
    "    sub_graph_edges = sub_graph_edges_temp\n",
    "    #print(\"AFTER REMOVING SELF-LOOPS : \\n\")\n",
    "    #print(sub_graph_edges, \"\\n\")\n",
    "\n",
    "    # Save the pruned PDG\n",
    "    edge_data_list = []\n",
    "    for edge in sub_graph_edges:\n",
    "        for edge_type in sub_graph_edges[edge]:\n",
    "            edge_data = edge[0].strip() + \" $$ \" + \\\n",
    "                        line_mapping[edge[0]].strip() + \" --> \" + \\\n",
    "                        edge[1].strip() + \" $$ \" + \\\n",
    "                        line_mapping[edge[1]].strip() + \" [\" + \\\n",
    "                        edge_type.strip() + \"]\\n\"\n",
    "            edge_data_list.append(edge_data)\n",
    "    #print(\"FINAL EDGE LIST: \\n\")\n",
    "    #print(edge_data_list, \"\\n\")\n",
    "    if len(edge_data_list) >= 3:\n",
    "        GOOD_DATA_POINTS += 1\n",
    "        \n",
    "    output_pdg_file.writelines(edge_data_list)\n",
    "    if len(edge_data_list) > 0:\n",
    "        TOTAL_DATA_POINTS += 1\n",
    "\n",
    "    return output_pdg_file, len(edge_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of total APIs: 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 119.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/before_pruning/ExecutorService.submit/\n",
      "\n",
      "\n",
      "GOOD PDG DATA POINTS: 7\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 24\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing: /home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/before_pruning/Lock.lock/\n",
      "\n",
      "\n",
      "GOOD PDG DATA POINTS: 6\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 6\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing: /home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/before_pruning/Timestamp.compareTo/\n",
      "\n",
      "\n",
      "GOOD PDG DATA POINTS: 19\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 23\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "TOTAL GOOD PDG DATA POINTS IN DATASET: 32\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS IN DATASET: 53\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR IN DATASET: 0\n",
      "\n",
      "\n",
      "DATASET STATISTICS: {'ExecutorService.submit': [24, 7, 0], 'Lock.lock': [6, 6, 0], 'Timestamp.compareTo': [23, 19, 0]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PDG_FOLDER_LOCATION = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/before_pruning\"\n",
    "OUTPUT_FOLDER_LOCATION = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/after_pruning\"\n",
    "pdg_folders_list = glob.glob(PDG_FOLDER_LOCATION + \"/*/\")\n",
    "print(\"\\nNumber of total APIs: {}\\n\".format(len(pdg_folders_list)))\n",
    "for folder in tqdm.tqdm(pdg_folders_list):\n",
    "    print(\"\\nProcessing: {}\\n\".format(folder))\n",
    "    api_name = folder[folder.rindex(\"/\", 0, len(folder) - 1) + 1 : -1]\n",
    "    pdg_files_list = glob.glob(os.path.join(folder, '*.txt'))\n",
    "    OUTPUT_API_FOLDER_LOCATION = OUTPUT_FOLDER_LOCATION + \"/\" + api_name\n",
    "    if not os.path.exists(OUTPUT_API_FOLDER_LOCATION):\n",
    "        os.makedirs(OUTPUT_API_FOLDER_LOCATION)\n",
    "    for pdg_file_location in pdg_files_list:\n",
    "        pdg_file = open(pdg_file_location, 'r')\n",
    "        output_file_location = OUTPUT_API_FOLDER_LOCATION + \"/\" + pdg_file_location[pdg_file_location.rindex(\"/\")+1:]\n",
    "        output_pdg_file = open(output_file_location, \"+w\")\n",
    "        try:\n",
    "            output_pdg_file, no_of_edges = get_pruned_pdg(pdg_file, output_pdg_file, api_name[api_name.rindex(\".\") + 1 :].strip())\n",
    "        except Exception as e:\n",
    "            PRUNING_ERROR_COUNT += 1\n",
    "            print(\"\\nERROR WHILE PRUNING PDG\\n\")\n",
    "            print(\"\\nFile: {}\\n\".format(pdg_file_location))\n",
    "            print(\"\\nERROR: {}\\n\".format(e))\n",
    "            pdg_file.close()\n",
    "            output_pdg_file.close()\n",
    "            os.remove(output_file_location)\n",
    "        else:\n",
    "            output_pdg_file.close()\n",
    "            if no_of_edges == 0:\n",
    "                os.remove(output_file_location)\n",
    "            pdg_file.close()\n",
    "\n",
    "    print(\"\\nGOOD PDG DATA POINTS: {}\\n\".format(GOOD_DATA_POINTS))\n",
    "    print(\"\\nTOTAL PDG DATA POINTS: {}\\n\".format(TOTAL_DATA_POINTS))\n",
    "    print(\"\\nTOTAL PRUNING ERROR: {}\\n\".format(PRUNING_ERROR_COUNT))\n",
    "    print(\"\\n=================================================================\\n\")\n",
    "    PRUNING_ERROR_COUNT_IN_DATASET += PRUNING_ERROR_COUNT\n",
    "    GOOD_DATA_POINTS_IN_DATASET += GOOD_DATA_POINTS\n",
    "    TOTAL_DATA_POINTS_IN_DATASET += TOTAL_DATA_POINTS\n",
    "    DATASET_STATISTICS[api_name] = [TOTAL_DATA_POINTS, GOOD_DATA_POINTS, PRUNING_ERROR_COUNT]\n",
    "    PRUNING_ERROR_COUNT, GOOD_DATA_POINTS, TOTAL_DATA_POINTS = 0, 0, 0\n",
    "    \n",
    "print(\"\\nTOTAL GOOD PDG DATA POINTS IN DATASET: {}\\n\".format(GOOD_DATA_POINTS_IN_DATASET))\n",
    "print(\"\\nTOTAL PDG DATA POINTS IN DATASET: {}\\n\".format(TOTAL_DATA_POINTS_IN_DATASET))\n",
    "print(\"\\nTOTAL PRUNING ERROR IN DATASET: {}\\n\".format(PRUNING_ERROR_COUNT_IN_DATASET))\n",
    "print(\"\\nDATASET STATISTICS: {}\\n\".format(DATASET_STATISTICS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_edges(inTextFile, add_reverse_edges = False):\n",
    "  # FD = 0, CD = 1\n",
    "  # to support the hetero data object as suggested by the documentation \n",
    "  nodes_dict = {}\n",
    "  edge_indices_CD = []\n",
    "  edge_indices_FD = []\n",
    "\n",
    "  #to support the Data object as used by the Entities dat object as used in RGAT source code\n",
    "  edge_indices = []\n",
    "  edge_type = []\n",
    "  \n",
    "  # nodes_dict is an index_map\n",
    "  node_count=0\n",
    "  with open(inTextFile) as fp:\n",
    "    \n",
    "    file_name = inTextFile.split(\"/\")[-1].strip()\n",
    "\n",
    "    Lines = fp.readlines()\n",
    "    for line in Lines:\n",
    "\n",
    "      N = line.split('-->')\n",
    "      N[0], N[1] = N[0].strip(), N[1].strip()\n",
    "      \n",
    "      #t1 = N[0].split('$$')   \n",
    "      src = N[0].strip()   \n",
    "      if src not in nodes_dict.keys():\n",
    "        nodes_dict[src] = node_count\n",
    "        node_count+=1\n",
    "        \n",
    "      #t2 = N[1].split('$$')\n",
    "      right_idx = N[1].rfind('[')\n",
    "      dst = N[1][:right_idx].strip()\n",
    "      if dst not in nodes_dict.keys():\n",
    "        nodes_dict[dst] = node_count\n",
    "        node_count+=1\n",
    "\n",
    "      x = N[1].strip()[right_idx + 1 : -1].strip()\n",
    "      if(x == 'FD'):\n",
    "        y=0\n",
    "        edge_type.append(y)\n",
    "        edge_indices.append([nodes_dict[src], nodes_dict[dst]])\n",
    "        if add_reverse_edges:\n",
    "          edge_type.append(y)\n",
    "          edge_indices.append([nodes_dict[dst], nodes_dict[src]])\n",
    "        edge_indices_FD.append([nodes_dict[src], nodes_dict[dst]])\n",
    "      else: \n",
    "        y=1\n",
    "        edge_type.append(y)\n",
    "        edge_indices.append([nodes_dict[src], nodes_dict[dst]])\n",
    "        if add_reverse_edges:\n",
    "          edge_type.append(y)\n",
    "          edge_indices.append([nodes_dict[dst], nodes_dict[src]])\n",
    "        edge_indices_CD.append([nodes_dict[src], nodes_dict[dst]])\n",
    "     \n",
    "  return nodes_dict, edge_indices_FD, edge_indices_CD, edge_indices, edge_type, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the models\n",
    "codebert_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "codebert_model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "codebert_model = codebert_model.to(device)\n",
    "\n",
    "def get_node_embedding_from_codebert(nodes):\n",
    "    list_of_embeddings = []\n",
    "    for code_line in nodes.keys():\n",
    "        code_line = code_line.split(\"$$\")[1].strip()\n",
    "        code_tokens = codebert_tokenizer.tokenize(code_line, truncation=True, max_length=510)\n",
    "        tokens = [codebert_tokenizer.cls_token]+code_tokens+[codebert_tokenizer.eos_token]\n",
    "        tokens_ids = torch.tensor(codebert_tokenizer.convert_tokens_to_ids(tokens))\n",
    "        tokens_ids = tokens_ids.to(device)\n",
    "        context_embeddings = codebert_model(tokens_ids[None,:])\n",
    "        cls_token_embedding = context_embeddings.last_hidden_state[0,0,:]\n",
    "        list_of_embeddings.append(cls_token_embedding)\n",
    "    return torch.stack(list_of_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dataset(folders):\n",
    "  dataset =[]\n",
    "  for label, folder in tqdm.tqdm(enumerate(folders)):\n",
    "    print(\"\\nProcessing: {}\\n\".format(folder))\n",
    "    files = glob.glob(os.path.join(folder, '*.txt'))\n",
    "    print(\"\\nNumber of files: {}\\n\".format(len(files)))\n",
    "    count = 0\n",
    "    for file in files:\n",
    "\n",
    "      if(count % 5 == 0):\n",
    "          print(\"\\nAt file: {}\\n\".format(count))\n",
    "                        \n",
    "      try:\n",
    "          nodes_dict, edge_indices_FD, edge_indices_CD, edge_indices, edge_type, file_name = get_nodes_edges(file, add_reverse_edges = True)\n",
    "      except Exception as e:\n",
    "          print(\"\\nError: \", e)\n",
    "          continue\n",
    "                    \n",
    "      if(len(nodes_dict) == 0):\n",
    "          print(\"\\nNo Data: \", file)\n",
    "          continue\n",
    "      #print(nodes_dict, edge_indices_CD, edge_indices_FD, edge_type)\n",
    "\n",
    "      # Node feature matrix with shape [num_nodes, num_node_features]=(N, 768).\n",
    "      try:\n",
    "          CodeEmbedding = get_node_embedding_from_codebert(nodes_dict)\n",
    "      except Exception as e :\n",
    "          print(\"\\nError: \", e)\n",
    "          print(nodes_dict)\n",
    "          continue\n",
    "      #print(CodeEmbedding.shape)\n",
    "\n",
    "      # FIXING DATA FOTMATS AND SHAPE\n",
    "      x = torch.tensor(CodeEmbedding)\n",
    "      # print(x.shape)\n",
    "  \n",
    "      # data.y: Target to train against (may have arbitrary shape),\n",
    "      # graph-level targets of shape [1, *]\n",
    "      label = 1\n",
    "      y = torch.tensor([label], dtype=torch.long)\n",
    "      #print(type(y))\n",
    "\n",
    "      # edge_index (LongTensor, optional) – Graph connectivity in COO format with shape [2, num_edges]\n",
    "      edge_index_CD = torch.tensor(edge_indices_CD, dtype=torch.long).t().contiguous()\n",
    "      edge_index_FD = torch.tensor(edge_indices_FD, dtype=torch.long).t().contiguous()\n",
    "      edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "      edge_attr = torch.tensor(edge_type, dtype=torch.long).t().contiguous()\n",
    "      #print(edge_index_CD, edge_index_FD, edge_index, edge_type)\n",
    "  \n",
    "      data = Data(edge_index=edge_index, edge_attr=edge_attr, x=x)\n",
    "      data.id = torch.tensor([count])\n",
    "      data.y = y\n",
    "      # data.num_nodes = len(nodes_dict)\n",
    "      data.api = file_name\n",
    "      dataset.append(data)\n",
    "      count += 1\n",
    "    \n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/after_pruning/ExecutorService.submit', '/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/after_pruning/Lock.lock', '/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/after_pruning/Timestamp.compareTo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/after_pruning/ExecutorService.submit\n",
      "\n",
      "\n",
      "Number of files: 24\n",
      "\n",
      "\n",
      "At file: 0\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/after_pruning/Lock.lock\n",
      "\n",
      "\n",
      "Number of files: 6\n",
      "\n",
      "\n",
      "At file: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "Processing: /home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/after_pruning/Timestamp.compareTo\n",
      "\n",
      "\n",
      "Number of files: 23\n",
      "\n",
      "\n",
      "At file: 0\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:03,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "Length of the dataset:  53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FOLDER_LOCATION = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/Code-Kernel-Relabelled/after_pruning\"\n",
    "project_folders = [os.path.join(OUTPUT_FOLDER_LOCATION, name) for name in os.listdir(OUTPUT_FOLDER_LOCATION) if os.path.isdir(os.path.join(OUTPUT_FOLDER_LOCATION, name))]\n",
    "print(project_folders)\n",
    "\n",
    "gnn_dataset = create_graph_dataset(project_folders)\n",
    "print(\"\\nLength of the dataset: \", len(gnn_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build/Load The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model!!\n"
     ]
    }
   ],
   "source": [
    "from model import GNN, GNN_graphpred\n",
    "\n",
    "#set up model\n",
    "num_layer = 3\n",
    "emb_dim = 768\n",
    "gnn_type = \"gcn\"\n",
    "num_tasks = 1\n",
    "JK = \"last\"\n",
    "dropout_ratio = 0.5\n",
    "graph_pooling = \"mean\"\n",
    "input_model_file = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/output/saved_models/gcn_1_3_5_e100_model_ck_code2seq.pth\"\n",
    "\n",
    "gnn_graphpred_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gnn_graphpred_model.from_pretrained(input_model_file)\n",
    "\n",
    "gnn_model = GNN(num_layer, emb_dim, JK, drop_ratio = dropout_ratio, gnn_type = gnn_type)\n",
    "gnn_model.load_state_dict(torch.load(input_model_file))\n",
    "\n",
    "print(\"Loaded the model!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_embeddings = {}\n",
    "for i in range(len(gnn_dataset)):\n",
    "    node_representation = gnn_model(gnn_dataset[i].x, gnn_dataset[i].edge_index, gnn_dataset[i].edge_attr)\n",
    "    graph_representation = global_mean_pool(x = node_representation, batch = torch.tensor([0]*(len(node_representation))))[0]\n",
    "    gnn_dataset[i].embedding = graph_representation.detach().numpy()\n",
    "    sample_name = gnn_dataset[i].api.split(\"_\")[1].strip()\n",
    "    api_name = gnn_dataset[i].api.split(\"_\")[2].strip()\n",
    "    try:\n",
    "        gnn_embeddings[api_name].append([gnn_dataset[i].api, sample_name, gnn_dataset[i].embedding])\n",
    "    except:\n",
    "        gnn_embeddings[api_name] = [[gnn_dataset[i].api, sample_name, gnn_dataset[i].embedding]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ExecutorService.submit': 6, 'Lock.lock': 2, 'Timestamp.compareTo': 3}\n"
     ]
    }
   ],
   "source": [
    "ground_truth_cluster_numbers = {}\n",
    "for folder in project_folders:\n",
    "    folder_name = folder.strip().split(\"/\")[-1]\n",
    "    sample_set = set([name.split(\"_\")[1] for name in os.listdir(folder)])\n",
    "    ground_truth_cluster_numbers[folder_name] = len(sample_set)\n",
    "print(ground_truth_cluster_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Analyzing:  ExecutorService.submit\n",
      "Cluster Counts:  {0: 19, 4: 1, 5: 1, 2: 1, 3: 1, 1: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 51098.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 276, currect_count = 128, wrong_count = 148, both_right = 36, both_wrong = 92\n",
      "{'TP': 36, 'TN': 92, 'FP': 135, 'FN': 13}\n",
      "Precision: 0.211, Recall: 0.735 and F1-Score: 0.328\n",
      "Accuracy: 0.464\n",
      "\n",
      "\n",
      "Analyzing:  Lock.lock\n",
      "Cluster Counts:  {0: 4, 1: 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 44462.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 15, currect_count = 10, wrong_count = 5, both_right = 6, both_wrong = 4\n",
      "{'TP': 6, 'TN': 4, 'FP': 1, 'FN': 4}\n",
      "Precision: 0.857, Recall: 0.6 and F1-Score: 0.706\n",
      "Accuracy: 0.667\n",
      "\n",
      "\n",
      "Analyzing:  Timestamp.compareTo\n",
      "Cluster Counts:  {0: 21, 1: 1, 2: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 55156.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 253, currect_count = 107, wrong_count = 146, both_right = 91, both_wrong = 16\n",
      "{'TP': 91, 'TN': 16, 'FP': 119, 'FN': 27}\n",
      "Precision: 0.433, Recall: 0.771 and F1-Score: 0.555\n",
      "Accuracy: 0.423\n",
      "\n",
      "\n",
      "Average Precision: 0.5003333333333334, Recall: 0.702 and F1-Score: 0.5296666666666666 and Accuracy: 0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_precision, total_recall, total_f1_score, total_accuracy = 0, 0, 0, 0\n",
    "\n",
    "for folder in project_folders:\n",
    "    folder_name = folder.strip().split(\"/\")[-1]\n",
    "    print(\"\\n\\nAnalyzing: \", folder_name)\n",
    "    precision, recall, f1_score, accuracy = cluster_and_compare(gnn_embeddings[folder_name], ground_truth_cluster_numbers[folder_name])\n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_f1_score += f1_score\n",
    "    total_accuracy += accuracy\n",
    "    \n",
    "print(\"\\n\\nAverage Precision: {}, Recall: {} and F1-Score: {} and Accuracy: {}\".format(total_precision/len(project_folders), total_recall/len(project_folders), total_f1_score/len(project_folders), total_accuracy/len(project_folders)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MuGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
