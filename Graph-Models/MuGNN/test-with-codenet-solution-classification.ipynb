{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddharthsa/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu101\n",
      "10.1\n",
      "3.8.18\n",
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import glob\n",
    "import tqdm\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "from typing import Callable, List, Optional, Dict\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    Batch\n",
    "    )\n",
    "import torch_geometric.datasets as datasets\n",
    "import torch_geometric.transforms as transforms\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, silhouette_score\n",
    "\n",
    "# To ensure determinism\n",
    "seed = 1234\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed)\n",
    "\n",
    "# Check versions\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(platform.python_version())\n",
    "print(torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process PDGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p00001': 0, 'p00002': 1, 'p00006': 2, 'p02381': 3, 'p02388': 4, 'p02389': 5, 'p02390': 6, 'p02391': 7, 'p02393': 8, 'p02394': 9}\n"
     ]
    }
   ],
   "source": [
    "pdg_data_folder = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/pdg_data_java250_100_class\"\n",
    "project_folders = [name for name in os.listdir(pdg_data_folder) if os.path.isdir(os.path.join(pdg_data_folder, name))]\n",
    "class_id = 0\n",
    "projects_to_consider = {}\n",
    "for project in project_folders[:10]:\n",
    "    projects_to_consider[project] = class_id\n",
    "    class_id += 1\n",
    "    \n",
    "print(projects_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ALGORITHM\n",
    "\n",
    "a. Clean the raw edge info (eg. remove wrongly formatted edges, class edges etc.)\n",
    "b. Merge same code-lines into a single line/node\n",
    "c. Remove the duplicate edges\n",
    "e. Add the all the edges(CD/FD) in the current subgraph\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "PRUNING_ERROR_COUNT, GOOD_DATA_POINTS, TOTAL_DATA_POINTS = 0, 0, 0\n",
    "PRUNING_ERROR_COUNT_IN_DATASET, GOOD_DATA_POINTS_IN_DATASET, TOTAL_DATA_POINTS_IN_DATASET = 0, 0, 0\n",
    "DATASET_STATISTICS = {}\n",
    "\n",
    "def get_pruned_pdg(pdg_file, output_pdg_file):\n",
    "    \n",
    "    global PRUNING_ERROR_COUNT, GOOD_DATA_POINTS, TOTAL_DATA_POINTS\n",
    "    \n",
    "    # all_edges = [bytes(l, 'utf-8').decode('utf-8', 'ignore').strip()\n",
    "    #              for l in pdg_file.readlines()]\n",
    "    all_edges = [l.replace(\"\\n\", \"\").replace(\"\\r\", \"\").strip()\n",
    "                 for l in pdg_file.readlines()]\n",
    "\n",
    "    # Remove unnecesssary edges(\"class\" edge, wrongly formatted edges etc.)\n",
    "    all_edges = [edge for edge in all_edges if edge.find(\n",
    "        \"-->\") != -1 and edge.count(\"$$\") == 2]\n",
    "    all_edges = [edge for edge in all_edges if len(edge.split(\"-->\")) == 2 and\n",
    "                 len(edge.split(\"-->\")[0].split(\"$$\")) == 2 and\n",
    "                 len(edge.split(\"-->\")[1].split(\"$$\")) == 2]\n",
    "    all_edges = [edge for edge in all_edges if edge.split(\"-->\")[0].find(\"Entry\") == -1 and\n",
    "                 edge.split(\"-->\")[0].find(\"class\") == -1]\n",
    "    #print(\"ALL EDGES : \\n\")\n",
    "    #print(all_edges, \"\\n\")\n",
    "\n",
    "    # Merge nodes referring to same code-line\n",
    "    line_mapping, edge_mapping = {}, {}\n",
    "    for edge in all_edges:\n",
    "        node_1, node_2 = edge[:edge.rindex(\"[\")].strip().split(\"-->\")\n",
    "        edge_type = edge[edge.rindex(\"[\") + 1: -1].strip()\n",
    "        line_numbers = []\n",
    "        for node in [node_1, node_2]:\n",
    "            line_number, line_code = node.strip().split(\"$$\")\n",
    "            line_number, line_code = line_number.strip(), line_code.strip()\n",
    "            line_numbers.append(line_number)\n",
    "            if line_number in line_mapping:\n",
    "                if line_mapping[line_number] != line_code:\n",
    "                    line_mapping[line_number] = line_code if len(line_code) > len(\n",
    "                        line_mapping[line_number]) else line_mapping[line_number]\n",
    "            else:\n",
    "                line_mapping[line_number] = line_code\n",
    "        if tuple(line_numbers) in edge_mapping:\n",
    "            edge_mapping[tuple(line_numbers)] = list(set(edge_mapping[tuple(line_numbers)] + [edge_type]))\n",
    "        else:\n",
    "            edge_mapping[tuple(line_numbers)] = [edge_type]\n",
    "\n",
    "    # Remove self-loops from subgraph\n",
    "    edges_temp = {}\n",
    "    for edge in edge_mapping:\n",
    "        if edge[0] != edge[1]:\n",
    "            edges_temp[edge] = edge_mapping[edge]\n",
    "    edge_mapping = edges_temp\n",
    "    #print(\"AFTER REMOVING SELF-LOOPS : \\n\")\n",
    "    #print(sub_graph_edges, \"\\n\")\n",
    "\n",
    "    # Save the pruned PDG\n",
    "    edge_data_list = []\n",
    "    for edge in edge_mapping:\n",
    "        for edge_type in edge_mapping[edge]:\n",
    "            edge_data = edge[0].strip() + \" $$ \" + \\\n",
    "                        line_mapping[edge[0]].strip() + \" --> \" + \\\n",
    "                        edge[1].strip() + \" $$ \" + \\\n",
    "                        line_mapping[edge[1]].strip() + \" [\" + \\\n",
    "                        edge_type.strip() + \"]\\n\"\n",
    "            edge_data_list.append(edge_data)\n",
    "    #print(\"FINAL EDGE LIST: \\n\")\n",
    "    #print(edge_data_list, \"\\n\")\n",
    "    if len(edge_data_list) >= 3:\n",
    "        GOOD_DATA_POINTS += 1\n",
    "        \n",
    "    output_pdg_file.writelines(edge_data_list)\n",
    "    if len(edge_data_list) > 0:\n",
    "        TOTAL_DATA_POINTS += 1\n",
    "\n",
    "    return output_pdg_file, len(edge_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:01,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing :  p00001\n",
      "\n",
      "GOOD PDG DATA POINTS: 263\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 263\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing :  p00002\n",
      "\n",
      "GOOD PDG DATA POINTS: 245\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 246\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing :  p00006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:00, 11.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GOOD PDG DATA POINTS: 251\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 255\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing :  p02381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:00<00:00, 10.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GOOD PDG DATA POINTS: 280\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 280\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing :  p02388\n",
      "\n",
      "GOOD PDG DATA POINTS: 233\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 244\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing :  p02389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:00<00:00, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GOOD PDG DATA POINTS: 246\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 248\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing :  p02390\n",
      "\n",
      "GOOD PDG DATA POINTS: 242\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 248\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing :  p02391\n",
      "\n",
      "GOOD PDG DATA POINTS: 237\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 238\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing :  p02393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GOOD PDG DATA POINTS: 250\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 250\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "Processing :  p02394\n",
      "\n",
      "GOOD PDG DATA POINTS: 251\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS: 251\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR: 0\n",
      "\n",
      "\n",
      "=================================================================\n",
      "\n",
      "\n",
      "TOTAL GOOD PDG DATA POINTS IN DATASET: 2498\n",
      "\n",
      "\n",
      "TOTAL PDG DATA POINTS IN DATASET: 2523\n",
      "\n",
      "\n",
      "TOTAL PRUNING ERROR IN DATASET: 0\n",
      "\n",
      "\n",
      "DATASET STATISTICS: {'p00001': [263, 263, 0], 'p00002': [246, 245, 0], 'p00006': [255, 251, 0], 'p02381': [280, 280, 0], 'p02388': [244, 233, 0], 'p02389': [248, 246, 0], 'p02390': [248, 242, 0], 'p02391': [238, 237, 0], 'p02393': [250, 250, 0], 'p02394': [251, 251, 0]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PDG_FOLDER_LOCATION = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/pdg_data_java250_100_class\"\n",
    "OUTPUT_FOLDER_LOCATION = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis\"\n",
    "\n",
    "for project in tqdm.tqdm(projects_to_consider):\n",
    "    print(\"\\nProcessing : \", project)\n",
    "    \n",
    "    OUTPUT_SPLIT_FOLDER_LOCATION = OUTPUT_FOLDER_LOCATION + \"/\" + project\n",
    "    if not os.path.exists(OUTPUT_SPLIT_FOLDER_LOCATION):\n",
    "        os.makedirs(OUTPUT_SPLIT_FOLDER_LOCATION)\n",
    "        \n",
    "    INPUT_PROJECT_FOLDER_LOCATION = PDG_FOLDER_LOCATION + \"/\" + project\n",
    "    pdg_files = glob.glob(os.path.join(INPUT_PROJECT_FOLDER_LOCATION, '*.txt'))\n",
    "    \n",
    "    for pdg_file in pdg_files:\n",
    "        original_pdg_file = open(pdg_file, 'r')\n",
    "        project_id = pdg_file[pdg_file.rindex(\"/\")+1:].split(\"_\")[2]\n",
    "        output_file_location = OUTPUT_SPLIT_FOLDER_LOCATION + \"/\" + pdg_file[pdg_file.rindex(\"/\")+1:-4] + \"_\" + str(projects_to_consider[project_id]) + \".txt\"\n",
    "        output_pdg_file = open(output_file_location, \"+w\")\n",
    "        try:\n",
    "            output_pdg_file, no_of_edges = get_pruned_pdg(original_pdg_file, output_pdg_file)\n",
    "        except Exception as e:\n",
    "            PRUNING_ERROR_COUNT += 1\n",
    "            print(\"\\nERROR WHILE PRUNING PDG\\n\")\n",
    "            print(\"\\nFile: {}\\n\".format(pdg_file))\n",
    "            print(\"\\nERROR: {}\\n\".format(e))\n",
    "            original_pdg_file.close()\n",
    "            output_pdg_file.close()\n",
    "            os.remove(output_file_location)\n",
    "        else:\n",
    "            output_pdg_file.close()\n",
    "            if no_of_edges == 0:\n",
    "                os.remove(output_file_location)\n",
    "            original_pdg_file.close()\n",
    "            \n",
    "    print(\"\\nGOOD PDG DATA POINTS: {}\\n\".format(GOOD_DATA_POINTS))\n",
    "    print(\"\\nTOTAL PDG DATA POINTS: {}\\n\".format(TOTAL_DATA_POINTS))\n",
    "    print(\"\\nTOTAL PRUNING ERROR: {}\\n\".format(PRUNING_ERROR_COUNT))\n",
    "    print(\"\\n=================================================================\\n\")\n",
    "    PRUNING_ERROR_COUNT_IN_DATASET += PRUNING_ERROR_COUNT\n",
    "    GOOD_DATA_POINTS_IN_DATASET += GOOD_DATA_POINTS\n",
    "    TOTAL_DATA_POINTS_IN_DATASET += TOTAL_DATA_POINTS\n",
    "    DATASET_STATISTICS[project] = [TOTAL_DATA_POINTS, GOOD_DATA_POINTS, PRUNING_ERROR_COUNT]\n",
    "    PRUNING_ERROR_COUNT, GOOD_DATA_POINTS, TOTAL_DATA_POINTS = 0, 0, 0\n",
    "    \n",
    "print(\"\\nTOTAL GOOD PDG DATA POINTS IN DATASET: {}\\n\".format(GOOD_DATA_POINTS_IN_DATASET))\n",
    "print(\"\\nTOTAL PDG DATA POINTS IN DATASET: {}\\n\".format(TOTAL_DATA_POINTS_IN_DATASET))\n",
    "print(\"\\nTOTAL PRUNING ERROR IN DATASET: {}\\n\".format(PRUNING_ERROR_COUNT_IN_DATASET))\n",
    "print(\"\\nDATASET STATISTICS: {}\\n\".format(DATASET_STATISTICS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_edges(inTextFile, add_reverse_edges = False):\n",
    "  # FD = 0, CD = 1\n",
    "  # to support the hetero data object as suggested by the documentation \n",
    "  nodes_dict = {}\n",
    "  edge_indices_CD = []\n",
    "  edge_indices_FD = []\n",
    "\n",
    "  #to support the Data object as used by the Entities dat object as used in RGAT source code\n",
    "  edge_indices = []\n",
    "  edge_type = []\n",
    "  \n",
    "  # nodes_dict is an index_map\n",
    "  node_count=0\n",
    "  with open(inTextFile) as fp:\n",
    "    \n",
    "    file_name = inTextFile.split(\"/\")[-1].strip()\n",
    "\n",
    "    Lines = fp.readlines()\n",
    "    for line in Lines:\n",
    "\n",
    "      N = line.split('-->')\n",
    "      N[0], N[1] = N[0].strip(), N[1].strip()\n",
    "      \n",
    "      #t1 = N[0].split('$$')   \n",
    "      src = N[0].strip()   \n",
    "      if src not in nodes_dict.keys():\n",
    "        nodes_dict[src] = node_count\n",
    "        node_count+=1\n",
    "        \n",
    "      #t2 = N[1].split('$$')\n",
    "      right_idx = N[1].rfind('[')\n",
    "      dst = N[1][:right_idx].strip()\n",
    "      if dst not in nodes_dict.keys():\n",
    "        nodes_dict[dst] = node_count\n",
    "        node_count+=1\n",
    "\n",
    "      x = N[1].strip()[right_idx + 1 : -1].strip()\n",
    "      if(x == 'FD'):\n",
    "        y=0\n",
    "        edge_type.append(y)\n",
    "        edge_indices.append([nodes_dict[src], nodes_dict[dst]])\n",
    "        if add_reverse_edges:\n",
    "          edge_type.append(y)\n",
    "          edge_indices.append([nodes_dict[dst], nodes_dict[src]])\n",
    "        edge_indices_FD.append([nodes_dict[src], nodes_dict[dst]])\n",
    "      else: \n",
    "        y=1\n",
    "        edge_type.append(y)\n",
    "        edge_indices.append([nodes_dict[src], nodes_dict[dst]])\n",
    "        if add_reverse_edges:\n",
    "          edge_type.append(y)\n",
    "          edge_indices.append([nodes_dict[dst], nodes_dict[src]])\n",
    "        edge_indices_CD.append([nodes_dict[src], nodes_dict[dst]])\n",
    "     \n",
    "  return nodes_dict, edge_indices_FD, edge_indices_CD, edge_indices, edge_type, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "#Set GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the models\n",
    "codebert_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "codebert_model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "codebert_model = codebert_model.to(device)\n",
    "\n",
    "def get_node_embedding_from_codebert(nodes):\n",
    "    list_of_embeddings = []\n",
    "    for code_line in nodes.keys():\n",
    "        code_line = code_line.split(\"$$\")[1].strip()\n",
    "        code_tokens = codebert_tokenizer.tokenize(code_line, truncation=True, max_length=510)\n",
    "        tokens = [codebert_tokenizer.cls_token]+code_tokens+[codebert_tokenizer.eos_token]\n",
    "        tokens_ids = torch.tensor(codebert_tokenizer.convert_tokens_to_ids(tokens))\n",
    "        tokens_ids = tokens_ids.to(device)\n",
    "        context_embeddings = codebert_model(tokens_ids[None,:])\n",
    "        cls_token_embedding = context_embeddings.last_hidden_state[0,0,:]\n",
    "        list_of_embeddings.append(cls_token_embedding.to(\"cpu\"))\n",
    "        del tokens_ids\n",
    "        del context_embeddings\n",
    "        del cls_token_embedding\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return torch.stack(list_of_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dataset(folders):\n",
    "  dataset =[]\n",
    "  for label, folder in tqdm.tqdm(enumerate(folders)):\n",
    "    print(\"\\nProcessing: {}\\n\".format(folder))\n",
    "    files = glob.glob(os.path.join(folder, '*.txt'))\n",
    "    print(\"\\nNumber of files: {}\\n\".format(len(files)))\n",
    "    count = 0\n",
    "    for file in files:\n",
    "\n",
    "      if(count % 5 == 0):\n",
    "          print(\"\\nAt file: {}\\n\".format(count))\n",
    "                        \n",
    "      try:\n",
    "          nodes_dict, edge_indices_FD, edge_indices_CD, edge_indices, edge_type, file_name = get_nodes_edges(file, add_reverse_edges = True)\n",
    "      except Exception as e:\n",
    "          print(\"\\nError: \", e)\n",
    "          continue\n",
    "                    \n",
    "      if(len(nodes_dict) == 0):\n",
    "          print(\"\\nNo Data: \", file)\n",
    "          continue\n",
    "      #print(nodes_dict, edge_indices_CD, edge_indices_FD, edge_type)\n",
    "\n",
    "      # Node feature matrix with shape [num_nodes, num_node_features]=(N, 768).\n",
    "      try:\n",
    "          with torch.no_grad():\n",
    "            CodeEmbedding = get_node_embedding_from_codebert(nodes_dict)\n",
    "      except Exception as e :\n",
    "          print(\"\\nError: \", e)\n",
    "          print(nodes_dict)\n",
    "          continue\n",
    "      #print(CodeEmbedding.shape)\n",
    "\n",
    "      # FIXING DATA FOTMATS AND SHAPE\n",
    "      x = torch.tensor(CodeEmbedding)\n",
    "      # print(x.shape)\n",
    "  \n",
    "      # data.y: Target to train against (may have arbitrary shape),\n",
    "      # graph-level targets of shape [1, *]\n",
    "      label = 1\n",
    "      y = torch.tensor([label], dtype=torch.long)\n",
    "      #print(type(y))\n",
    "\n",
    "      # edge_index (LongTensor, optional) – Graph connectivity in COO format with shape [2, num_edges]\n",
    "      edge_index_CD = torch.tensor(edge_indices_CD, dtype=torch.long).t().contiguous()\n",
    "      edge_index_FD = torch.tensor(edge_indices_FD, dtype=torch.long).t().contiguous()\n",
    "      edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "      edge_attr = torch.tensor(edge_type, dtype=torch.long).t().contiguous()\n",
    "      #print(edge_index_CD, edge_index_FD, edge_index, edge_type)\n",
    "  \n",
    "      data = Data(edge_index=edge_index, edge_attr=edge_attr, x=x)\n",
    "      data.id = torch.tensor([count])\n",
    "      data.y = y\n",
    "      # data.num_nodes = len(nodes_dict)\n",
    "      data.api = file_name\n",
    "      dataset.append(data)\n",
    "      count += 1\n",
    "    \n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p00001', '/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p00002', '/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p00006', '/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p02381', '/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p02388', '/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p02389', '/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p02390', '/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p02391', '/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p02393', '/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p02394']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p00001\n",
      "\n",
      "\n",
      "Number of files: 263\n",
      "\n",
      "\n",
      "At file: 0\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n",
      "\n",
      "At file: 250\n",
      "\n",
      "\n",
      "At file: 255\n",
      "\n",
      "\n",
      "At file: 260\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:31, 91.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p00002\n",
      "\n",
      "\n",
      "Number of files: 246\n",
      "\n",
      "\n",
      "At file: 0\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [02:49, 83.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p00006\n",
      "\n",
      "\n",
      "Number of files: 255\n",
      "\n",
      "\n",
      "At file: 0\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n",
      "\n",
      "At file: 250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [04:02, 78.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p02381\n",
      "\n",
      "\n",
      "Number of files: 280\n",
      "\n",
      "\n",
      "At file: 0\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n",
      "\n",
      "At file: 250\n",
      "\n",
      "\n",
      "At file: 255\n",
      "\n",
      "\n",
      "At file: 260\n",
      "\n",
      "\n",
      "At file: 265\n",
      "\n",
      "\n",
      "At file: 270\n",
      "\n",
      "\n",
      "At file: 275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [05:55, 92.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis/p02388\n",
      "\n",
      "\n",
      "Number of files: 244\n",
      "\n",
      "\n",
      "At file: 0\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [07:01, 84.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of the dataset:  1288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FOLDER_LOCATION = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/temp/solution-classification-analysis\"\n",
    "folders = [os.path.join(OUTPUT_FOLDER_LOCATION, name) for name in os.listdir(OUTPUT_FOLDER_LOCATION) if os.path.isdir(os.path.join(OUTPUT_FOLDER_LOCATION, name))]\n",
    "print(folders)\n",
    "\n",
    "gnn_dataset = create_graph_dataset(folders[:5])\n",
    "print(\"\\nLength of the dataset: \", len(gnn_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build/Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model!!\n"
     ]
    }
   ],
   "source": [
    "from model import GNN, GNN_graphpred\n",
    "\n",
    "#set up model\n",
    "num_layer = 3\n",
    "emb_dim = 768\n",
    "gnn_type = \"gcn\"\n",
    "num_tasks = 1\n",
    "JK = \"last\"\n",
    "dropout_ratio = 0.5\n",
    "graph_pooling = \"mean\"\n",
    "input_model_file = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/output/saved_models/gcn_1_3_5_e100_model_ck_code2seq.pth\"\n",
    "\n",
    "gnn_graphpred_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gnn_graphpred_model.from_pretrained(input_model_file)\n",
    "\n",
    "gnn_model = GNN(num_layer, emb_dim, JK, drop_ratio = dropout_ratio, gnn_type = gnn_type)\n",
    "gnn_model.load_state_dict(torch.load(input_model_file))\n",
    "\n",
    "print(\"Loaded the model!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone-Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model!!\n"
     ]
    }
   ],
   "source": [
    "from model_ng import CustomGCN\n",
    "\n",
    "\n",
    "input_model_file = \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Graph-Models/MuGNN/output/saved_models/clone_detection_GCN_L3_e10_850k_model.pth\"\n",
    "gnn_model = CustomGCN(num_node_features= 768)\n",
    "gnn_model.load_state_dict(torch.load(input_model_file))\n",
    "\n",
    "print(\"Loaded the model!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_embeddings = []\n",
    "model_name = \"clone-detection\" # \"context-prediction\" or \"clone-detection\"\n",
    "for i in range(len(gnn_dataset)):\n",
    "    if model_name == \"clone-detection\":\n",
    "        graph_representation = gnn_model(gnn_dataset[i].x, gnn_dataset[i].edge_index, batch = torch.tensor([0]*(len(gnn_dataset[i].x))))[0]\n",
    "    else:\n",
    "        node_representation = gnn_model(gnn_dataset[i].x, gnn_dataset[i].edge_index, gnn_dataset[i].edge_attr)\n",
    "        graph_representation = global_mean_pool(x = node_representation, batch = torch.tensor([0]*(len(node_representation))))[0]\n",
    "    gnn_dataset[i].embedding = graph_representation.detach().numpy()\n",
    "    problem_name = gnn_dataset[i].api.split(\"_\")[2].strip()\n",
    "    gnn_embeddings.append([gnn_dataset[i].api, problem_name, gnn_dataset[i].embedding])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def cluster_and_compare(embeddings, ground_truth_cluster_number, clustering_algorithm = \"Birch\"):\n",
    "    \n",
    "    if(clustering_algorithm == \"Birch\"):\n",
    "        birch_model = Birch(n_clusters = ground_truth_cluster_number)\n",
    "        clusters_result = birch_model.fit_predict([emb[2] for emb in embeddings])\n",
    "    elif(clustering_algorithm == \"Agglomerative\"):\n",
    "        agglomerative_model = AgglomerativeClustering(n_clusters = ground_truth_cluster_number)\n",
    "        clusters_result = agglomerative_model.fit_predict([emb[2] for emb in embeddings])\n",
    "    elif(clustering_algorithm == \"KMeans\"):\n",
    "        kmeans_model = KMeans(n_clusters = ground_truth_cluster_number)\n",
    "        clusters_result = kmeans_model.fit_predict([emb[2] for emb in embeddings])\n",
    "    elif(clustering_algorithm == \"GM\"):\n",
    "        gaussian_model = GaussianMixture(n_components = ground_truth_cluster_number)\n",
    "        clusters_result = gaussian_model.fit_predict([emb[2] for emb in embeddings])\n",
    "        \n",
    "    cluster_count = {}\n",
    "    \n",
    "    project_names = list(set([emb[1] for emb in embeddings]))\n",
    "    cluster_mapping = {}\n",
    "    for cluster_no in list(set(clusters_result)):\n",
    "        cluster_mapping[cluster_no] = {}\n",
    "        \n",
    "    for i in range(len(clusters_result)):\n",
    "        try:\n",
    "            cluster_count[clusters_result[i]] += 1\n",
    "        except:\n",
    "            cluster_count[clusters_result[i]] = 1\n",
    "            \n",
    "        try:\n",
    "            cluster_mapping[clusters_result[i]][embeddings[i][1]] += 1\n",
    "        except:\n",
    "            cluster_mapping[clusters_result[i]][embeddings[i][1]] = 1\n",
    "    print(\"Cluster Counts: \", cluster_count)\n",
    "    print(\"Project Names: \", project_names)\n",
    "    print(\"Cluster Mapping: \", cluster_mapping)\n",
    "    \n",
    "    total_count, currect_count, wrong_count = 0, 0, 0\n",
    "    both_right, both_wrong = 0, 0\n",
    "    confusion_matrix = {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0}\n",
    "\n",
    "    original_one_final_two = []\n",
    "    original_two_final_one = []\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(embeddings))):\n",
    "        for j in range(i+1, len(embeddings)):\n",
    "            total_count += 1\n",
    "            if (embeddings[i][1] == embeddings[j][1]):\n",
    "                if (clusters_result[i] == clusters_result[j]):\n",
    "                    both_right += 1\n",
    "                    currect_count += 1\n",
    "                    confusion_matrix[\"TP\"] += 1\n",
    "                else:\n",
    "                    #original_one_final_two.append([embeddings[i][0], embeddings[j][0], embeddings[i][1], clusters_result[i], clusters_result[j]])\n",
    "                    wrong_count += 1\n",
    "                    confusion_matrix[\"FN\"] += 1\n",
    "            else:\n",
    "                if (clusters_result[i] != clusters_result[j]):\n",
    "                    both_wrong += 1\n",
    "                    currect_count += 1\n",
    "                    confusion_matrix[\"TN\"] += 1\n",
    "                else:\n",
    "                    #original_two_final_one.append([embeddings[i][0], embeddings[j][0], embeddings[i][1], embeddings[j][1], clusters_result[i]])\n",
    "                    wrong_count += 1\n",
    "                    confusion_matrix[\"FP\"] += 1\n",
    "                    \n",
    "    print(\"total_count = {}, currect_count = {}, wrong_count = {}, both_right = {}, both_wrong = {}\".format(total_count, currect_count, wrong_count, both_right, both_wrong))\n",
    "    print(confusion_matrix)\n",
    "    precision = float(format(confusion_matrix[\"TP\"] / (confusion_matrix[\"TP\"] + confusion_matrix[\"FP\"]), \".3f\"))\n",
    "    recall = float(format(confusion_matrix[\"TP\"] / (confusion_matrix[\"TP\"] + confusion_matrix[\"FN\"]), \".3f\"))\n",
    "    f1_score = float(format(2 * (precision * recall) / (precision + recall), \".3f\"))\n",
    "    accuracy = float(format(currect_count/total_count, \".3f\"))\n",
    "    print(\"Precision: {}, Recall: {} and F1-Score: {}\".format(precision, recall, f1_score))\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    \n",
    "    return precision, recall, f1_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Counts:  {0: 1182, 4: 46, 1: 42, 2: 11, 3: 7}\n",
      "Project Names:  ['p00002', 'p02381', 'p02388', 'p00006', 'p00001']\n",
      "Cluster Mapping:  {0: {'p00001': 260, 'p00002': 185, 'p00006': 226, 'p02381': 280, 'p02388': 231}, 1: {'p00002': 29, 'p00006': 13}, 2: {'p00002': 8, 'p00006': 3}, 3: {'p00006': 6, 'p02388': 1}, 4: {'p00001': 3, 'p00002': 24, 'p00006': 7, 'p02388': 12}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1288 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1288/1288 [00:00<00:00, 3180.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 828828, currect_count = 248478, wrong_count = 580350, both_right = 142636, both_wrong = 105842\n",
      "{'TP': 142636, 'TN': 105842, 'FP': 557307, 'FN': 23043}\n",
      "Precision: 0.204, Recall: 0.861 and F1-Score: 0.33\n",
      "Accuracy: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1_score, accuracy = cluster_and_compare(gnn_embeddings, ground_truth_cluster_number = 5, clustering_algorithm = \"Birch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalute Using CodeBERT/UnixCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "#Set GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "# Initialize the models\n",
    "codebert_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "codebert_model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "codebert_model = codebert_model.to(device)\n",
    "max_source_length= 512\n",
    "\n",
    "def get_code_embeddings_from_codebert(codelines):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    code = \" \".join(codelines)\n",
    "    source_tokens = codebert_tokenizer.tokenize(code)[:max_source_length-2]\n",
    "    source_tokens = [codebert_tokenizer.cls_token]+source_tokens+[codebert_tokenizer.sep_token]\n",
    "    source_ids =  codebert_tokenizer.convert_tokens_to_ids(source_tokens) \n",
    "    padding_length = max_source_length - len(source_ids)\n",
    "    source_ids+=[codebert_tokenizer.pad_token_id]*padding_length\n",
    "    source_ids = torch.tensor(source_ids)\n",
    "    \n",
    "    # tokens = []\n",
    "    # for code_line in codelines:\n",
    "    #     code_tokens = codebert_tokenizer.tokenize(code_line, truncation=True, max_length=510)\n",
    "    #     if tokens == []:\n",
    "    #         tokens = [codebert_tokenizer.cls_token] + code_tokens\n",
    "    #     else:\n",
    "    #         tokens = tokens + [codebert_tokenizer.sep_token] + code_tokens\n",
    "    # tokens = tokens + [codebert_tokenizer.eos_token]\n",
    "    # tokens_ids = torch.tensor(codebert_tokenizer.convert_tokens_to_ids(tokens))\n",
    "    source_ids = source_ids.to(device)\n",
    "    context_embeddings = codebert_model(source_ids[None,:])\n",
    "    cls_token_embedding = context_embeddings.last_hidden_state[0,0,:]\n",
    "    return cls_token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from unixcoder import UniXcoder\n",
    "\n",
    "#Set GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "# Initialize the models\n",
    "unixcoder_model = UniXcoder(\"microsoft/unixcoder-base\")\n",
    "unixcoder_model = unixcoder_model.to(device)\n",
    "max_source_length= 512\n",
    "\n",
    "def get_code_embeddings_from_unixcoder(codelines):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    code = \" \".join(codelines)\n",
    "    tokens_ids = unixcoder_model.tokenize([code], max_length=512, mode=\"<encoder-only>\")\n",
    "    source_ids = torch.tensor(tokens_ids).to(device)\n",
    "    tokens_embeddings, code_embedding = unixcoder_model(source_ids)\n",
    "    return torch.flatten(code_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_llms(folders, model):\n",
    "  embeddings = []\n",
    "  for label, folder in tqdm.tqdm(enumerate(folders)):\n",
    "    folder_name = folder.strip().split(\"/\")[-1]\n",
    "    print(\"\\nProcessing: {}\\n\".format(folder_name))\n",
    "    files = glob.glob(os.path.join(folder, '*.java'))\n",
    "    print(\"\\nNumber of files: {}\\n\".format(len(files)))\n",
    "    count = 1\n",
    "    for file in files:\n",
    "      sample_name = file.split(\"/\")[-2].strip()\n",
    "      file_name = file.split(\"/\")[-1].strip()\n",
    "      if(count % 5 == 0):\n",
    "          print(\"\\nAt file: {}\\n\".format(count))\n",
    "                        \n",
    "      fp = open(file,'r')\n",
    "      lines = fp.readlines()\n",
    "      lines = [line for line in lines if not line.startswith(\"import\") and not len(line.strip('\\n')) == 0]\n",
    "      lines = [line.strip('\\n').strip(\"\\t\").strip(\" \") for line in lines]\n",
    "      if model == \"codebert\":\n",
    "        embedding = get_code_embeddings_from_codebert(lines)\n",
    "      elif model == \"unixcoder\":\n",
    "        embedding = get_code_embeddings_from_unixcoder(lines)\n",
    "      embedding = embedding.detach().cpu().numpy()\n",
    "      embeddings.append([file_name, folder_name, embedding])\n",
    "      count += 1\n",
    "    \n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folders_2 = [\"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/processed_data_for_pdg_java250_100_class/p00001\",\n",
    "                     \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/processed_data_for_pdg_java250_100_class/p00002\"]\n",
    "\n",
    "project_folders_5 = [\"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/processed_data_for_pdg_java250_100_class/p00001\",\n",
    "                   \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/processed_data_for_pdg_java250_100_class/p00002\",\n",
    "                   \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/processed_data_for_pdg_java250_100_class/p00006\",\n",
    "                   \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/processed_data_for_pdg_java250_100_class/p02381\",\n",
    "                   \"/home/siddharthsa/cs21mtech12001-Tamal/API-Misuse-Prediction/PDG-gen/Repository/Benchmarks/CodeNet/processed_data_for_pdg_java250_100_class/p02388\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: p00001\n",
      "\n",
      "\n",
      "Number of files: 264\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n",
      "\n",
      "At file: 250\n",
      "\n",
      "\n",
      "At file: 255\n",
      "\n",
      "\n",
      "At file: 260\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:19, 79.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: p00002\n",
      "\n",
      "\n",
      "Number of files: 248\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [02:26, 72.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: p00006\n",
      "\n",
      "\n",
      "Number of files: 258\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n",
      "\n",
      "At file: 250\n",
      "\n",
      "\n",
      "At file: 255\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [03:45, 75.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: p02381\n",
      "\n",
      "\n",
      "Number of files: 280\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (597 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n",
      "\n",
      "At file: 250\n",
      "\n",
      "\n",
      "At file: 255\n",
      "\n",
      "\n",
      "At file: 260\n",
      "\n",
      "\n",
      "At file: 265\n",
      "\n",
      "\n",
      "At file: 270\n",
      "\n",
      "\n",
      "At file: 275\n",
      "\n",
      "\n",
      "At file: 280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [05:11, 79.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: p02388\n",
      "\n",
      "\n",
      "Number of files: 246\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [06:27, 77.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Counts:  {2: 607, 1: 249, 4: 261, 3: 138, 0: 41}\n",
      "Project Names:  ['p00002', 'p02381', 'p02388', 'p00006', 'p00001']\n",
      "Cluster Mapping:  {0: {'p00001': 11, 'p00002': 4, 'p02381': 26}, 1: {'p00001': 75, 'p00002': 19, 'p00006': 7, 'p02381': 148}, 2: {'p00001': 71, 'p00002': 114, 'p00006': 194, 'p02388': 228}, 3: {'p00001': 24, 'p00002': 11, 'p00006': 2, 'p02381': 101}, 4: {'p00001': 83, 'p00002': 100, 'p00006': 55, 'p02381': 5, 'p02388': 18}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1296/1296 [00:00<00:00, 2631.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 839160, currect_count = 578746, wrong_count = 260414, both_right = 83139, both_wrong = 495607\n",
      "{'TP': 83139, 'TN': 495607, 'FP': 175861, 'FN': 84553}\n",
      "Precision: 0.321, Recall: 0.496 and F1-Score: 0.39\n",
      "Accuracy: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    codebert_embeddings = get_embeddings_from_llms(project_folders_5, \"codebert\")\n",
    "\n",
    "precision, recall, f1_score, accuracy = cluster_and_compare(codebert_embeddings, ground_truth_cluster_number = 5, clustering_algorithm = \"Birch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: p00001\n",
      "\n",
      "\n",
      "Number of files: 264\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n",
      "\n",
      "At file: 250\n",
      "\n",
      "\n",
      "At file: 255\n",
      "\n",
      "\n",
      "At file: 260\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:02, 62.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: p00002\n",
      "\n",
      "\n",
      "Number of files: 248\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:59, 59.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: p00006\n",
      "\n",
      "\n",
      "Number of files: 258\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n",
      "\n",
      "At file: 250\n",
      "\n",
      "\n",
      "At file: 255\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [02:57, 58.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: p02381\n",
      "\n",
      "\n",
      "Number of files: 280\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n",
      "\n",
      "At file: 250\n",
      "\n",
      "\n",
      "At file: 255\n",
      "\n",
      "\n",
      "At file: 260\n",
      "\n",
      "\n",
      "At file: 265\n",
      "\n",
      "\n",
      "At file: 270\n",
      "\n",
      "\n",
      "At file: 275\n",
      "\n",
      "\n",
      "At file: 280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [04:05, 62.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: p02388\n",
      "\n",
      "\n",
      "Number of files: 246\n",
      "\n",
      "\n",
      "At file: 5\n",
      "\n",
      "\n",
      "At file: 10\n",
      "\n",
      "\n",
      "At file: 15\n",
      "\n",
      "\n",
      "At file: 20\n",
      "\n",
      "\n",
      "At file: 25\n",
      "\n",
      "\n",
      "At file: 30\n",
      "\n",
      "\n",
      "At file: 35\n",
      "\n",
      "\n",
      "At file: 40\n",
      "\n",
      "\n",
      "At file: 45\n",
      "\n",
      "\n",
      "At file: 50\n",
      "\n",
      "\n",
      "At file: 55\n",
      "\n",
      "\n",
      "At file: 60\n",
      "\n",
      "\n",
      "At file: 65\n",
      "\n",
      "\n",
      "At file: 70\n",
      "\n",
      "\n",
      "At file: 75\n",
      "\n",
      "\n",
      "At file: 80\n",
      "\n",
      "\n",
      "At file: 85\n",
      "\n",
      "\n",
      "At file: 90\n",
      "\n",
      "\n",
      "At file: 95\n",
      "\n",
      "\n",
      "At file: 100\n",
      "\n",
      "\n",
      "At file: 105\n",
      "\n",
      "\n",
      "At file: 110\n",
      "\n",
      "\n",
      "At file: 115\n",
      "\n",
      "\n",
      "At file: 120\n",
      "\n",
      "\n",
      "At file: 125\n",
      "\n",
      "\n",
      "At file: 130\n",
      "\n",
      "\n",
      "At file: 135\n",
      "\n",
      "\n",
      "At file: 140\n",
      "\n",
      "\n",
      "At file: 145\n",
      "\n",
      "\n",
      "At file: 150\n",
      "\n",
      "\n",
      "At file: 155\n",
      "\n",
      "\n",
      "At file: 160\n",
      "\n",
      "\n",
      "At file: 165\n",
      "\n",
      "\n",
      "At file: 170\n",
      "\n",
      "\n",
      "At file: 175\n",
      "\n",
      "\n",
      "At file: 180\n",
      "\n",
      "\n",
      "At file: 185\n",
      "\n",
      "\n",
      "At file: 190\n",
      "\n",
      "\n",
      "At file: 195\n",
      "\n",
      "\n",
      "At file: 200\n",
      "\n",
      "\n",
      "At file: 205\n",
      "\n",
      "\n",
      "At file: 210\n",
      "\n",
      "\n",
      "At file: 215\n",
      "\n",
      "\n",
      "At file: 220\n",
      "\n",
      "\n",
      "At file: 225\n",
      "\n",
      "\n",
      "At file: 230\n",
      "\n",
      "\n",
      "At file: 235\n",
      "\n",
      "\n",
      "At file: 240\n",
      "\n",
      "\n",
      "At file: 245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [05:01, 60.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Counts:  {0: 263, 2: 572, 1: 290, 3: 99, 4: 72}\n",
      "Project Names:  ['p00002', 'p02381', 'p02388', 'p00006', 'p00001']\n",
      "Cluster Mapping:  {0: {'p00001': 258, 'p00002': 2, 'p00006': 3}, 1: {'p00002': 11, 'p02381': 279}, 2: {'p00001': 6, 'p00002': 235, 'p00006': 156, 'p02381': 1, 'p02388': 174}, 3: {'p00006': 99}, 4: {'p02388': 72}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1296/1296 [00:00<00:00, 2700.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count = 839160, currect_count = 692499, wrong_count = 146661, both_right = 134051, both_wrong = 558448\n",
      "{'TP': 134051, 'TN': 558448, 'FP': 113020, 'FN': 33641}\n",
      "Precision: 0.543, Recall: 0.799 and F1-Score: 0.647\n",
      "Accuracy: 0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    unixcoder_embeddings = get_embeddings_from_llms(project_folders_5, \"unixcoder\")\n",
    "\n",
    "precision, recall, f1_score, accuracy = cluster_and_compare(unixcoder_embeddings, ground_truth_cluster_number = 5, clustering_algorithm = \"Birch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MuGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
