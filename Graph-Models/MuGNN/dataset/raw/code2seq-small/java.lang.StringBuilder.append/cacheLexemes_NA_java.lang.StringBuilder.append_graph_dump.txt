Line_27 $$ if (parentCachingNode instanceof LazyParseableElement) -->Line_31 $$ if (!doLexingOptimizationCorrectionCheck && cachedTokens != null) [ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_38 $$ int approxLexCount = Math.max(10, myText.length() / 5)[ CD ]
Line_31 $$ if (!doLexingOptimizationCorrectionCheck && cachedTokens != null) -->Line_35 $$ return[ CD ]
Line_9 $$ int tokenCount = parentToken.myEndIndex - parentToken.myStartIndex-->Line_12 $$ System.arraycopy(parentToken.myBuilder.myLexStarts, parentToken.myStartIndex, lexStarts, 0, tokenCount)[ FD ]
Line_58 $$ final int prevStart = myLexStarts[i - 1]-->Line_59 $$ sb.append("\n  prev: '").append(myText.subSequence(prevStart, offset)).append("' (").append(myLexTypes[i - 1]).append(':').append(myLexTypes[i - 1].getLanguage()).append(") ").append(prevStart).append(":").append(offset)[ FD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_7 $$ if (parentCachingNode instanceof LazyParseableToken) [ FD ]
Line_43 $$ int offset = 0-->Line_59 $$ sb.append("\n  prev: '").append(myText.subSequence(prevStart, offset)).append("' (").append(myLexTypes[i - 1]).append(':').append(myLexTypes[i - 1].getLanguage()).append(") ").append(prevStart).append(":").append(offset)[ FD ]
Line_44 $$ while (true) -->Line_46 $$ IElementType type = myLexer.getTokenType()[ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_74 $$ if (doLexingOptimizationCorrectionCheck && lexemeCount != -1) [ CD ]
Line_76 $$ for (int j = 0; j < lexemeCount; ++j) -->Line_77 $$ if (myLexStarts[j] != lexStarts[j] || myLexTypes[j] != lexTypes[j]) [ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_3 $$ int[] lexStarts = null[ CD ]
Line_54 $$ final StringBuilder sb = new StringBuilder()-->Line_56 $$ sb.append("Token sequence broken").append("\n  this: '").append(myLexer.getTokenText()).append("' (").append(tokenType).append(':').append(tokenType != null ? tokenType.getLanguage() : null).append(") ").append(tokenStart).append(":").append(myLexer.getTokenEnd())[ FD ]
Line_42 $$ int i = 0-->Line_49 $$ if (i >= myLexTypes.length - 1) [ FD ]
Line_7 $$ if (parentCachingNode instanceof LazyParseableToken) -->Line_20 $$ ProgressIndicatorProvider.checkCanceled()[ CD ]
Line_7 $$ if (parentCachingNode instanceof LazyParseableToken) -->Line_8 $$ final LazyParseableToken parentToken = (LazyParseableToken) parentCachingNode[ CD ]
Line_7 $$ if (parentCachingNode instanceof LazyParseableToken) -->Line_10 $$ if (tokenCount != 1) [ CD ]
Line_9 $$ int tokenCount = parentToken.myEndIndex - parentToken.myStartIndex-->Line_10 $$ if (tokenCount != 1) [ FD ]
Line_10 $$ if (tokenCount != 1) -->Line_16 $$ lexTypes = new IElementType[tokenCount][ CD ]
Line_28 $$ final LazyParseableElement parentElement = (LazyParseableElement) parentCachingNode-->Line_30 $$ parentElement.putUserData(LAZY_PARSEABLE_TOKENS, null)[ FD ]
Line_53 $$ if (tokenStart < offset) -->Line_57 $$ if (i > 0) [ CD ]
Line_57 $$ if (i > 0) -->Line_59 $$ sb.append("\n  prev: '").append(myText.subSequence(prevStart, offset)).append("' (").append(myLexTypes[i - 1]).append(':').append(myLexTypes[i - 1].getLanguage()).append(") ").append(prevStart).append(":").append(offset)[ CD ]
Line_14 $$ for (int i = 0; i < tokenCount; ++i) lexStarts[i] -= diff-->Line_57 $$ if (i > 0) [ FD ]
Line_44 $$ while (true) -->Line_47 $$ if (type == null)[ CD ]
Line_7 $$ if (parentCachingNode instanceof LazyParseableToken) -->Line_21 $$ if (!doLexingOptimizationCorrectionCheck && lexemeCount != -1) [ CD ]
Line_6 $$ boolean doLexingOptimizationCorrectionCheck = true-->Line_74 $$ if (doLexingOptimizationCorrectionCheck && lexemeCount != -1) [ FD ]
Line_66 $$ myLexStarts[i] = offset = tokenStart-->Line_59 $$ sb.append("\n  prev: '").append(myText.subSequence(prevStart, offset)).append("' (").append(myLexTypes[i - 1]).append(':').append(myLexTypes[i - 1].getLanguage()).append(") ").append(prevStart).append(":").append(offset)[ FD ]
Line_7 $$ if (parentCachingNode instanceof LazyParseableToken) -->Line_9 $$ int tokenCount = parentToken.myEndIndex - parentToken.myStartIndex[ CD ]
Line_10 $$ if (tokenCount != 1) -->Line_13 $$ int diff = parentToken.myBuilder.myLexStarts[parentToken.myStartIndex][ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_6 $$ boolean doLexingOptimizationCorrectionCheck = true[ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_71 $$ myLexStarts[i] = myText.length()[ CD ]
Line_16 $$ lexTypes = new IElementType[tokenCount]-->Line_17 $$ System.arraycopy(parentToken.myBuilder.myLexTypes, parentToken.myStartIndex, lexTypes, 0, tokenCount)[ FD ]
Line_6 $$ boolean doLexingOptimizationCorrectionCheck = true-->Line_21 $$ if (!doLexingOptimizationCorrectionCheck && lexemeCount != -1) [ FD ]
Line_3 $$ int[] lexStarts = null-->Line_11 $$ lexStarts = new int[tokenCount + 1][ FD ]
Line_5 $$ int lexemeCount = -1-->Line_18 $$ lexemeCount = tokenCount[ FD ]
Line_44 $$ while (true) -->Line_53 $$ if (tokenStart < offset) [ CD ]
Line_9 $$ int tokenCount = parentToken.myEndIndex - parentToken.myStartIndex-->Line_17 $$ System.arraycopy(parentToken.myBuilder.myLexTypes, parentToken.myStartIndex, lexTypes, 0, tokenCount)[ FD ]
Line_44 $$ while (true) -->Line_68 $$ i++[ CD ]
Line_27 $$ if (parentCachingNode instanceof LazyParseableElement) -->Line_30 $$ parentElement.putUserData(LAZY_PARSEABLE_TOKENS, null)[ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_44 $$ while (true) [ CD ]
Line_53 $$ if (tokenStart < offset) -->Line_62 $$ final int quoteEnd = Math.min(tokenStart + 256, myText.length())[ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_73 $$ clearCachedTokenType()[ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_4 $$ IElementType[] lexTypes = null[ CD ]
Line_43 $$ int offset = 0-->Line_66 $$ myLexStarts[i] = offset = tokenStart[ FD ]
Line_66 $$ myLexStarts[i] = offset = tokenStart-->Line_53 $$ if (tokenStart < offset) [ FD ]
Line_52 $$ int tokenStart = myLexer.getTokenStart()-->Line_66 $$ myLexStarts[i] = offset = tokenStart[ FD ]
Line_10 $$ if (tokenCount != 1) -->Line_12 $$ System.arraycopy(parentToken.myBuilder.myLexStarts, parentToken.myStartIndex, lexStarts, 0, tokenCount)[ CD ]
Line_14 $$ for (int i = 0; i < tokenCount; ++i) lexStarts[i] -= diff-->Line_68 $$ i++[ FD ]
Line_74 $$ if (doLexingOptimizationCorrectionCheck && lexemeCount != -1) -->Line_76 $$ for (int j = 0; j < lexemeCount; ++j) [ CD ]
Line_10 $$ if (tokenCount != 1) -->Line_17 $$ System.arraycopy(parentToken.myBuilder.myLexTypes, parentToken.myStartIndex, lexTypes, 0, tokenCount)[ CD ]
Line_62 $$ final int quoteEnd = Math.min(tokenStart + 256, myText.length())-->Line_63 $$ sb.append("\n  quote: [").append(quoteStart).append(':').append(quoteEnd).append("] '").append(myText.subSequence(quoteStart, quoteEnd)).append('\'')[ FD ]
Line_5 $$ int lexemeCount = -1-->Line_76 $$ for (int j = 0; j < lexemeCount; ++j) [ FD ]
Line_44 $$ while (true) -->Line_45 $$ ProgressIndicatorProvider.checkCanceled()[ CD ]
Line_13 $$ int diff = parentToken.myBuilder.myLexStarts[parentToken.myStartIndex]-->Line_14 $$ lexStarts[i] -= diff[ FD ]
Line_53 $$ if (tokenStart < offset) -->Line_63 $$ sb.append("\n  quote: [").append(quoteStart).append(':').append(quoteEnd).append("] '").append(myText.subSequence(quoteStart, quoteEnd)).append('\'')[ CD ]
Line_46 $$ IElementType type = myLexer.getTokenType()-->Line_67 $$ myLexTypes[i] = type[ FD ]
Line_42 $$ int i = 0-->Line_68 $$ i++[ FD ]
Line_7 $$ if (parentCachingNode instanceof LazyParseableToken) -->Line_27 $$ if (parentCachingNode instanceof LazyParseableElement) [ CD ]
Line_61 $$ final int quoteStart = Math.max(tokenStart - 256, 0)-->Line_63 $$ sb.append("\n  quote: [").append(quoteStart).append(':').append(quoteEnd).append("] '").append(myText.subSequence(quoteStart, quoteEnd)).append('\'')[ FD ]
Line_44 $$ while (true) -->Line_69 $$ myLexer.advance()[ CD ]
Line_3 $$ int[] lexStarts = null-->Line_12 $$ System.arraycopy(parentToken.myBuilder.myLexStarts, parentToken.myStartIndex, lexStarts, 0, tokenCount)[ FD ]
Line_4 $$ IElementType[] lexTypes = null-->Line_16 $$ lexTypes = new IElementType[tokenCount][ FD ]
Line_44 $$ while (true) -->Line_66 $$ myLexStarts[i] = offset = tokenStart[ CD ]
Line_10 $$ if (tokenCount != 1) -->Line_11 $$ lexStarts = new int[tokenCount + 1][ CD ]
Line_10 $$ if (tokenCount != 1) -->Line_18 $$ lexemeCount = tokenCount[ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_42 $$ int i = 0[ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_43 $$ int offset = 0[ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_5 $$ int lexemeCount = -1[ CD ]
Line_21 $$ if (!doLexingOptimizationCorrectionCheck && lexemeCount != -1) -->Line_25 $$ return[ CD ]
Line_53 $$ if (tokenStart < offset) -->Line_55 $$ final IElementType tokenType = myLexer.getTokenType()[ CD ]
Line_57 $$ if (i > 0) -->Line_58 $$ final int prevStart = myLexStarts[i - 1][ CD ]
Line_6 $$ boolean doLexingOptimizationCorrectionCheck = true-->Line_31 $$ if (!doLexingOptimizationCorrectionCheck && cachedTokens != null) [ FD ]
Line_49 $$ if (i >= myLexTypes.length - 1) -->Line_50 $$ resizeLexemes(i * 3 / 2)[ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_41 $$ myLexer.start(myText)[ CD ]
Line_14 $$ for (int i = 0; i < tokenCount; ++i) lexStarts[i] -= diff-->Line_49 $$ if (i >= myLexTypes.length - 1) [ FD ]
Line_9 $$ int tokenCount = parentToken.myEndIndex - parentToken.myStartIndex-->Line_18 $$ lexemeCount = tokenCount[ FD ]
Line_43 $$ int offset = 0-->Line_53 $$ if (tokenStart < offset) [ FD ]
Line_10 $$ if (tokenCount != 1) -->Line_14 $$ for (int i = 0; i < tokenCount; ++i) lexStarts[i] -= diff[ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_7 $$ if (parentCachingNode instanceof LazyParseableToken) [ CD ]
Line_54 $$ final StringBuilder sb = new StringBuilder()-->Line_59 $$ sb.append("\n  prev: '").append(myText.subSequence(prevStart, offset)).append("' (").append(myLexTypes[i - 1]).append(':').append(myLexTypes[i - 1].getLanguage()).append(") ").append(prevStart).append(":").append(offset)[ FD ]
Line_55 $$ final IElementType tokenType = myLexer.getTokenType()-->Line_56 $$ sb.append("Token sequence broken").append("\n  this: '").append(myLexer.getTokenText()).append("' (").append(tokenType).append(':').append(tokenType != null ? tokenType.getLanguage() : null).append(") ").append(tokenStart).append(":").append(myLexer.getTokenEnd())[ FD ]
Line_53 $$ if (tokenStart < offset) -->Line_56 $$ sb.append("Token sequence broken").append("\n  this: '").append(myLexer.getTokenText()).append("' (").append(tokenType).append(':').append(tokenType != null ? tokenType.getLanguage() : null).append(") ").append(tokenStart).append(":").append(myLexer.getTokenEnd())[ CD ]
Line_27 $$ if (parentCachingNode instanceof LazyParseableElement) -->Line_28 $$ final LazyParseableElement parentElement = (LazyParseableElement) parentCachingNode[ CD ]
Line_44 $$ while (true) -->Line_49 $$ if (i >= myLexTypes.length - 1) [ CD ]
Line_44 $$ while (true) -->Line_67 $$ myLexTypes[i] = type[ CD ]
Line_54 $$ final StringBuilder sb = new StringBuilder()-->Line_64 $$ LOG.error(sb)[ FD ]
Line_11 $$ lexStarts = new int[tokenCount + 1]-->Line_12 $$ System.arraycopy(parentToken.myBuilder.myLexStarts, parentToken.myStartIndex, lexStarts, 0, tokenCount)[ FD ]
Line_9 $$ int tokenCount = parentToken.myEndIndex - parentToken.myStartIndex-->Line_14 $$ for (int i = 0; i < tokenCount; ++i) lexStarts[i] -= diff[ FD ]
Line_42 $$ int i = 0-->Line_57 $$ if (i > 0) [ FD ]
Line_44 $$ while (true) -->Line_52 $$ int tokenStart = myLexer.getTokenStart()[ CD ]
Line_2 $$ private void cacheLexemes(@Nullable Object parentCachingNode) -->Line_27 $$ if (parentCachingNode instanceof LazyParseableElement) [ FD ]
Line_53 $$ if (tokenStart < offset) -->Line_64 $$ LOG.error(sb)[ CD ]
Line_4 $$ IElementType[] lexTypes = null-->Line_17 $$ System.arraycopy(parentToken.myBuilder.myLexTypes, parentToken.myStartIndex, lexTypes, 0, tokenCount)[ FD ]
Line_27 $$ if (parentCachingNode instanceof LazyParseableElement) -->Line_29 $$ final LazyParseableTokensCache cachedTokens = parentElement.getUserData(LAZY_PARSEABLE_TOKENS)[ CD ]
Line_46 $$ IElementType type = myLexer.getTokenType()-->Line_47 $$ if (type == null)[ FD ]
Line_52 $$ int tokenStart = myLexer.getTokenStart()-->Line_56 $$ sb.append("Token sequence broken").append("\n  this: '").append(myLexer.getTokenText()).append("' (").append(tokenType).append(':').append(tokenType != null ? tokenType.getLanguage() : null).append(") ").append(tokenStart).append(":").append(myLexer.getTokenEnd())[ FD ]
Line_52 $$ int tokenStart = myLexer.getTokenStart()-->Line_53 $$ if (tokenStart < offset) [ FD ]
Line_53 $$ if (tokenStart < offset) -->Line_61 $$ final int quoteStart = Math.max(tokenStart - 256, 0)[ CD ]
Line_28 $$ final LazyParseableElement parentElement = (LazyParseableElement) parentCachingNode-->Line_29 $$ final LazyParseableTokensCache cachedTokens = parentElement.getUserData(LAZY_PARSEABLE_TOKENS)[ FD ]
Line_54 $$ final StringBuilder sb = new StringBuilder()-->Line_63 $$ sb.append("\n  quote: [").append(quoteStart).append(':').append(quoteEnd).append("] '").append(myText.subSequence(quoteStart, quoteEnd)).append('\'')[ FD ]
Line_53 $$ if (tokenStart < offset) -->Line_54 $$ final StringBuilder sb = new StringBuilder()[ CD ]
Line_10 $$ if (tokenCount != 1) -->Line_15 $$ lexStarts[tokenCount] = myText.length()[ CD ]
Line_18 $$ lexemeCount = tokenCount-->Line_76 $$ for (int j = 0; j < lexemeCount; ++j) [ FD ]
