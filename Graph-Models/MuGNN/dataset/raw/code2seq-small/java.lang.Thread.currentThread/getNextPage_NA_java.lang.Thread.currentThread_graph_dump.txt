Line_7 $$ int batchSize = parquetReader.nextBatch()-->Line_17 $$ blocks[fieldId] = constantBlocks[fieldId].getRegion(0, batchSize)[ FD ]
Line_14 $$ for (int fieldId = 0; fieldId < blocks.length; fieldId++) -->Line_15 $$ Type type = types.get(fieldId)[ FD ]
Line_14 $$ for (int fieldId = 0; fieldId < blocks.length; fieldId++) -->Line_15 $$ Type type = types.get(fieldId)[ CD ]
Line_14 $$ for (int fieldId = 0; fieldId < blocks.length; fieldId++) -->Line_19 $$ ColumnDescriptor columnDescriptor = this.requestedSchema.getColumns().get(fieldId)[ FD ]
Line_9 $$ if (closed || batchSize <= 0) -->Line_11 $$ return null[ CD ]
Line_16 $$ if (constantBlocks[fieldId] != null) -->Line_19 $$ ColumnDescriptor columnDescriptor = this.requestedSchema.getColumns().get(fieldId)[ CD ]
Line_9 $$ if (closed || batchSize <= 0) -->Line_10 $$ close()[ CD ]
Line_24 $$ long newCompletedBytes = (long) (totalBytes * parquetReader.getProgress())-->Line_25 $$ completedBytes = min(totalBytes, max(completedBytes, newCompletedBytes))[ FD ]
Line_16 $$ if (constantBlocks[fieldId] != null) -->Line_17 $$ blocks[fieldId] = constantBlocks[fieldId].getRegion(0, batchSize)[ CD ]
Line_14 $$ for (int fieldId = 0; fieldId < blocks.length; fieldId++) -->Line_16 $$ if (constantBlocks[fieldId] != null) [ CD ]
Line_16 $$ if (constantBlocks[fieldId] != null) -->Line_20 $$ blocks[fieldId] = new LazyBlock(batchSize, new ParquetBlockLoader(columnDescriptor, batchSize, type))[ CD ]
Line_31 $$ if (e instanceof InterruptedException) -->Line_32 $$ Thread.currentThread().interrupt()[ CD ]
