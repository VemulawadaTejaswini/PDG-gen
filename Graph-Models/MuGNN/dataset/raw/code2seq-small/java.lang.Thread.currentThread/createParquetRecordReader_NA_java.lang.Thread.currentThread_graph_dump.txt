Line_18 $$ if (predicatePushdownEnabled) -->Line_20 $$ splitGroup = splitGroup.stream().filter( block -> predicateMatches(parquetPredicate, block, configuration, path, requestedSchema, effectivePredicate)).collect(toList())[ CD ]
Line_24 $$ BlockMetaData block = splitGroup.get(i)-->Line_25 $$ offsets[i] = block.getStartingPos()[ FD ]
Line_12 $$ for (BlockMetaData block : blocks) -->Line_20 $$ predicateMatches(parquetPredicate, block, configuration, path, requestedSchema, effectivePredicate)[ FD ]
Line_29 $$ ParquetRecordReader<FakeParquetRecord> realReader = new PrestoParquetRecordReader(readSupport)-->Line_30 $$ realReader.initialize(split, taskContext)[ FD ]
Line_12 $$ for (BlockMetaData block : blocks) -->Line_25 $$ offsets[i] = block.getStartingPos()[ FD ]
Line_23 $$ for (int i = 0; i < splitGroup.size(); i++) -->Line_24 $$ BlockMetaData block = splitGroup.get(i)[ FD ]
Line_20 $$ splitGroup = splitGroup.stream().filter( block -> predicateMatches(parquetPredicate, block, configuration, path, requestedSchema, effectivePredicate)).collect(toList())-->Line_24 $$ BlockMetaData block = splitGroup.get(i)[ FD ]
Line_2 $$ private ParquetRecordReader<FakeParquetRecord> createParquetRecordReader(Configuration configuration, Path path, long start, long length, List<HiveColumnHandle> columns, boolean useParquetColumnNames, TypeManager typeManager, boolean predicatePushdownEnabled, TupleDomain<HiveColumnHandle> effectivePredicate) -->Line_4 $$ ParquetMetadata parquetMetadata = ParquetFileReader.readFooter(configuration, path, NO_FILTER)[ FD ]
Line_2 $$ private ParquetRecordReader<FakeParquetRecord> createParquetRecordReader(Configuration configuration, Path path, long start, long length, List<HiveColumnHandle> columns, boolean useParquetColumnNames, TypeManager typeManager, boolean predicatePushdownEnabled, TupleDomain<HiveColumnHandle> effectivePredicate) -->Line_20 $$ predicateMatches(parquetPredicate, block, configuration, path, requestedSchema, effectivePredicate)[ FD ]
Line_7 $$ MessageType fileSchema = fileMetaData.getSchema()-->Line_9 $$ getParquetType(column, fileSchema, useParquetColumnNames)[ FD ]
Line_4 $$ ParquetMetadata parquetMetadata = ParquetFileReader.readFooter(configuration, path, NO_FILTER)-->Line_6 $$ FileMetaData fileMetaData = parquetMetadata.getFileMetaData()[ FD ]
Line_7 $$ MessageType fileSchema = fileMetaData.getSchema()-->Line_10 $$ MessageType requestedSchema = new MessageType(fileSchema.getName(), fields)[ FD ]
Line_28 $$ TaskAttemptContext taskContext = ContextUtil.newTaskAttemptContext(configuration, new TaskAttemptID())-->Line_30 $$ realReader.initialize(split, taskContext)[ FD ]
Line_4 $$ ParquetMetadata parquetMetadata = ParquetFileReader.readFooter(configuration, path, NO_FILTER)-->Line_5 $$ List<BlockMetaData> blocks = parquetMetadata.getBlocks()[ FD ]
Line_12 $$ for (BlockMetaData block : blocks) -->Line_15 $$ splitGroup.add(block)[ FD ]
Line_14 $$ if (firstDataPage >= start && firstDataPage < start + length) -->Line_15 $$ splitGroup.add(block)[ CD ]
Line_27 $$ ParquetInputSplit split = new ParquetInputSplit(path, start, start + length, length, null, offsets)-->Line_30 $$ realReader.initialize(split, taskContext)[ FD ]
Line_2 $$ private ParquetRecordReader<FakeParquetRecord> createParquetRecordReader(Configuration configuration, Path path, long start, long length, List<HiveColumnHandle> columns, boolean useParquetColumnNames, TypeManager typeManager, boolean predicatePushdownEnabled, TupleDomain<HiveColumnHandle> effectivePredicate) -->Line_28 $$ TaskAttemptContext taskContext = ContextUtil.newTaskAttemptContext(configuration, new TaskAttemptID())[ FD ]
Line_6 $$ FileMetaData fileMetaData = parquetMetadata.getFileMetaData()-->Line_7 $$ MessageType fileSchema = fileMetaData.getSchema()[ FD ]
Line_20 $$ splitGroup = splitGroup.stream().filter( block -> predicateMatches(parquetPredicate, block, configuration, path, requestedSchema, effectivePredicate)).collect(toList())-->Line_22 $$ long[] offsets = new long[splitGroup.size()][ FD ]
Line_11 $$ List<BlockMetaData> splitGroup = new ArrayList()-->Line_15 $$ splitGroup.add(block)[ FD ]
Line_11 $$ List<BlockMetaData> splitGroup = new ArrayList()-->Line_20 $$ splitGroup = splitGroup.stream().filter( block -> predicateMatches(parquetPredicate, block, configuration, path, requestedSchema, effectivePredicate)).collect(toList())[ FD ]
Line_11 $$ List<BlockMetaData> splitGroup = new ArrayList()-->Line_23 $$ for (int i = 0; i < splitGroup.size(); i++) [ FD ]
Line_23 $$ for (int i = 0; i < splitGroup.size(); i++) -->Line_24 $$ BlockMetaData block = splitGroup.get(i)[ CD ]
Line_19 $$ ParquetPredicate parquetPredicate = buildParquetPredicate(columns, effectivePredicate, fileMetaData.getSchema(), typeManager)-->Line_20 $$ predicateMatches(parquetPredicate, block, configuration, path, requestedSchema, effectivePredicate)[ FD ]
Line_2 $$ private ParquetRecordReader<FakeParquetRecord> createParquetRecordReader(Configuration configuration, Path path, long start, long length, List<HiveColumnHandle> columns, boolean useParquetColumnNames, TypeManager typeManager, boolean predicatePushdownEnabled, TupleDomain<HiveColumnHandle> effectivePredicate) -->Line_9 $$ getParquetType(column, fileSchema, useParquetColumnNames)[ FD ]
Line_20 $$ splitGroup = splitGroup.stream().filter( block -> predicateMatches(parquetPredicate, block, configuration, path, requestedSchema, effectivePredicate)).collect(toList())-->Line_23 $$ for (int i = 0; i < splitGroup.size(); i++) [ FD ]
Line_11 $$ List<BlockMetaData> splitGroup = new ArrayList()-->Line_24 $$ BlockMetaData block = splitGroup.get(i)[ FD ]
Line_6 $$ FileMetaData fileMetaData = parquetMetadata.getFileMetaData()-->Line_19 $$ ParquetPredicate parquetPredicate = buildParquetPredicate(columns, effectivePredicate, fileMetaData.getSchema(), typeManager)[ FD ]
Line_18 $$ if (predicatePushdownEnabled) -->Line_19 $$ ParquetPredicate parquetPredicate = buildParquetPredicate(columns, effectivePredicate, fileMetaData.getSchema(), typeManager)[ CD ]
Line_23 $$ for (int i = 0; i < splitGroup.size(); i++) -->Line_25 $$ offsets[i] = block.getStartingPos()[ CD ]
Line_10 $$ MessageType requestedSchema = new MessageType(fileSchema.getName(), fields)-->Line_20 $$ predicateMatches(parquetPredicate, block, configuration, path, requestedSchema, effectivePredicate)[ FD ]
Line_2 $$ private ParquetRecordReader<FakeParquetRecord> createParquetRecordReader(Configuration configuration, Path path, long start, long length, List<HiveColumnHandle> columns, boolean useParquetColumnNames, TypeManager typeManager, boolean predicatePushdownEnabled, TupleDomain<HiveColumnHandle> effectivePredicate) -->Line_9 $$ List<parquet.schema.Type> fields = columns.stream().filter( column -> !column.isPartitionKey()).map( column -> getParquetType(column, fileSchema, useParquetColumnNames)).filter(Objects::<>nonNull).collect(toList())[ FD ]
Line_11 $$ List<BlockMetaData> splitGroup = new ArrayList()-->Line_22 $$ long[] offsets = new long[splitGroup.size()][ FD ]
Line_12 $$ for (BlockMetaData block : blocks) -->Line_13 $$ long firstDataPage = block.getColumns().get(0).getFirstDataPageOffset()[ FD ]
Line_2 $$ private ParquetRecordReader<FakeParquetRecord> createParquetRecordReader(Configuration configuration, Path path, long start, long length, List<HiveColumnHandle> columns, boolean useParquetColumnNames, TypeManager typeManager, boolean predicatePushdownEnabled, TupleDomain<HiveColumnHandle> effectivePredicate) -->Line_19 $$ ParquetPredicate parquetPredicate = buildParquetPredicate(columns, effectivePredicate, fileMetaData.getSchema(), typeManager)[ FD ]
