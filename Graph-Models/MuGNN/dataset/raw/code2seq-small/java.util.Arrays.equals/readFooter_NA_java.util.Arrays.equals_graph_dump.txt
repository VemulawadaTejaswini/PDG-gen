Line_4 $$ FileStatus fileStatus = fileSystem.getFileStatus(file)-->Line_6 $$ long length = fileStatus.getLen()[ FD ]
Line_25 $$ BlockMetaData blockMetaData = new BlockMetaData()-->Line_26 $$ blockMetaData.setRowCount(rowGroup.getNum_rows())[ FD ]
Line_17 $$ FileMetaData fileMetaData = readFileMetaData(inputStream)-->Line_44 $$ List<KeyValue> keyValueList = fileMetaData.getKey_value_metadata()[ FD ]
Line_11 $$ byte[] magic = new byte[MAGIC.length]-->Line_12 $$ inputStream.readFully(magic)[ FD ]
Line_25 $$ BlockMetaData blockMetaData = new BlockMetaData()-->Line_37 $$ blockMetaData.addColumn(column)[ FD ]
Line_3 $$ FileSystem fileSystem = file.getFileSystem(configuration)-->Line_5 $$ try (FSDataInputStream inputStream = fileSystem.open(file)) [ FD ]
Line_2 $$ public static ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException -->Line_19 $$ checkArgument(!schema.isEmpty(), "Empty Parquet schema in file: %s", file)[ FD ]
Line_31 $$ for (ColumnChunk columnChunk : columns) -->Line_32 $$ checkArgument((filePath == null && columnChunk.getFile_path() == null) || (filePath != null && filePath.equals(columnChunk.getFile_path())), "all column chunks of the same row group must be in the same file")[ FD ]
Line_3 $$ FileSystem fileSystem = file.getFileSystem(configuration)-->Line_4 $$ FileStatus fileStatus = fileSystem.getFileStatus(file)[ FD ]
Line_5 $$ try (FSDataInputStream inputStream = fileSystem.open(file)) -->Line_17 $$ FileMetaData fileMetaData = readFileMetaData(inputStream)[ FD ]
Line_2 $$ public static ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException -->Line_4 $$ FileStatus fileStatus = fileSystem.getFileStatus(file)[ FD ]
Line_18 $$ List<SchemaElement> schema = fileMetaData.getSchema()-->Line_20 $$ MessageType messageType = readParquetSchema(schema)[ FD ]
Line_44 $$ List<KeyValue> keyValueList = fileMetaData.getKey_value_metadata()-->Line_45 $$ if (keyValueList != null) [ FD ]
Line_43 $$ Map<String, String> keyValueMetaData = new HashMap()-->Line_47 $$ keyValueMetaData.put(keyValue.key, keyValue.value)[ FD ]
Line_22 $$ List<RowGroup> rowGroups = fileMetaData.getRow_groups()-->Line_23 $$ if (rowGroups != null) [ FD ]
Line_17 $$ FileMetaData fileMetaData = readFileMetaData(inputStream)-->Line_18 $$ List<SchemaElement> schema = fileMetaData.getSchema()[ FD ]
Line_34 $$ String[] path = metaData.path_in_schema.toArray(new String[metaData.path_in_schema.size()])-->Line_35 $$ ColumnPath columnPath = ColumnPath.get(path)[ FD ]
Line_14 $$ long metadataIndex = metadataLengthIndex - metadataLength-->Line_16 $$ inputStream.seek(metadataIndex)[ FD ]
Line_2 $$ public static ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException -->Line_4 $$ FileStatus fileStatus = fileSystem.getFileStatus(file)[ CD ]
Line_11 $$ byte[] magic = new byte[MAGIC.length]-->Line_13 $$ checkArgument(Arrays.equals(MAGIC, magic), "Not valid Parquet file: %s expected magic number: %s got: %s", file, Arrays.toString(MAGIC), Arrays.toString(magic))[ FD ]
Line_35 $$ ColumnPath columnPath = ColumnPath.get(path)-->Line_36 $$ ColumnChunkMetaData column = ColumnChunkMetaData.get(columnPath, messageType.getType(columnPath.toArray()).asPrimitiveType().getPrimitiveTypeName(), CompressionCodecName.fromParquet(metaData.codec), readEncodings(metaData.encodings), readStats(metaData.statistics, messageType.getType(columnPath.toArray()).asPrimitiveType().getPrimitiveTypeName()), metaData.data_page_offset, metaData.dictionary_page_offset, metaData.num_values, metaData.total_compressed_size, metaData.total_uncompressed_size)[ FD ]
Line_2 $$ public static ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException -->Line_15 $$ checkArgument(metadataIndex >= MAGIC.length && metadataIndex < metadataLengthIndex, "Corrupted Parquet file: %s metadata index: %s out of range", file, metadataIndex)[ FD ]
Line_5 $$ try (FSDataInputStream inputStream = fileSystem.open(file)) -->Line_16 $$ inputStream.seek(metadataIndex)[ FD ]
Line_14 $$ long metadataIndex = metadataLengthIndex - metadataLength-->Line_15 $$ checkArgument(metadataIndex >= MAGIC.length && metadataIndex < metadataLengthIndex, "Corrupted Parquet file: %s metadata index: %s out of range", file, metadataIndex)[ FD ]
Line_24 $$ for (RowGroup rowGroup : rowGroups) -->Line_28 $$ List<ColumnChunk> columns = rowGroup.getColumns()[ FD ]
Line_21 $$ List<BlockMetaData> blocks = new ArrayList()-->Line_40 $$ blocks.add(blockMetaData)[ FD ]
Line_18 $$ List<SchemaElement> schema = fileMetaData.getSchema()-->Line_19 $$ checkArgument(!schema.isEmpty(), "Empty Parquet schema in file: %s", file)[ FD ]
Line_2 $$ public static ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException -->Line_3 $$ FileSystem fileSystem = file.getFileSystem(configuration)[ CD ]
Line_8 $$ long metadataLengthIndex = length - PARQUET_METADATA_LENGTH - MAGIC.length-->Line_9 $$ inputStream.seek(metadataLengthIndex)[ FD ]
Line_2 $$ public static ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException -->Line_7 $$ checkArgument(length >= MAGIC.length + PARQUET_METADATA_LENGTH + MAGIC.length, "%s is not a valid Parquet File", file)[ FD ]
Line_24 $$ for (RowGroup rowGroup : rowGroups) -->Line_29 $$ checkArgument(!columns.isEmpty(), "No columns in row group: %s", rowGroup)[ FD ]
Line_30 $$ String filePath = columns.get(0).getFile_path()-->Line_32 $$ checkArgument((filePath == null && columnChunk.getFile_path() == null) || (filePath != null && filePath.equals(columnChunk.getFile_path())), "all column chunks of the same row group must be in the same file")[ FD ]
Line_2 $$ public static ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException -->Line_13 $$ checkArgument(Arrays.equals(MAGIC, magic), "Not valid Parquet file: %s expected magic number: %s got: %s", file, Arrays.toString(MAGIC), Arrays.toString(magic))[ FD ]
Line_2 $$ public static ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException -->Line_5 $$ try (FSDataInputStream inputStream = fileSystem.open(file)) [ FD ]
Line_17 $$ FileMetaData fileMetaData = readFileMetaData(inputStream)-->Line_22 $$ List<RowGroup> rowGroups = fileMetaData.getRow_groups()[ FD ]
Line_2 $$ public static ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException -->Line_3 $$ FileSystem fileSystem = file.getFileSystem(configuration)[ FD ]
Line_24 $$ for (RowGroup rowGroup : rowGroups) -->Line_27 $$ blockMetaData.setTotalByteSize(rowGroup.getTotal_byte_size())[ FD ]
Line_30 $$ String filePath = columns.get(0).getFile_path()-->Line_39 $$ blockMetaData.setPath(filePath)[ FD ]
Line_25 $$ BlockMetaData blockMetaData = new BlockMetaData()-->Line_27 $$ blockMetaData.setTotalByteSize(rowGroup.getTotal_byte_size())[ FD ]
Line_5 $$ try (FSDataInputStream inputStream = fileSystem.open(file)) -->Line_10 $$ int metadataLength = readIntLittleEndian(inputStream)[ FD ]
Line_17 $$ FileMetaData fileMetaData = readFileMetaData(inputStream)-->Line_50 $$ return new ParquetMetadata(new parquet.hadoop.metadata.FileMetaData(messageType, keyValueMetaData, fileMetaData.getCreated_by()), blocks)[ FD ]
Line_25 $$ BlockMetaData blockMetaData = new BlockMetaData()-->Line_40 $$ blocks.add(blockMetaData)[ FD ]
Line_25 $$ BlockMetaData blockMetaData = new BlockMetaData()-->Line_39 $$ blockMetaData.setPath(filePath)[ FD ]
Line_28 $$ List<ColumnChunk> columns = rowGroup.getColumns()-->Line_30 $$ String filePath = columns.get(0).getFile_path()[ FD ]
Line_5 $$ try (FSDataInputStream inputStream = fileSystem.open(file)) -->Line_9 $$ inputStream.seek(metadataLengthIndex)[ FD ]
Line_20 $$ MessageType messageType = readParquetSchema(schema)-->Line_36 $$ ColumnChunkMetaData column = ColumnChunkMetaData.get(columnPath, messageType.getType(columnPath.toArray()).asPrimitiveType().getPrimitiveTypeName(), CompressionCodecName.fromParquet(metaData.codec), readEncodings(metaData.encodings), readStats(metaData.statistics, messageType.getType(columnPath.toArray()).asPrimitiveType().getPrimitiveTypeName()), metaData.data_page_offset, metaData.dictionary_page_offset, metaData.num_values, metaData.total_compressed_size, metaData.total_uncompressed_size)[ FD ]
Line_28 $$ List<ColumnChunk> columns = rowGroup.getColumns()-->Line_29 $$ checkArgument(!columns.isEmpty(), "No columns in row group: %s", rowGroup)[ FD ]
Line_5 $$ try (FSDataInputStream inputStream = fileSystem.open(file)) -->Line_12 $$ inputStream.readFully(magic)[ FD ]
Line_24 $$ for (RowGroup rowGroup : rowGroups) -->Line_26 $$ blockMetaData.setRowCount(rowGroup.getNum_rows())[ FD ]
Line_36 $$ ColumnChunkMetaData column = ColumnChunkMetaData.get(columnPath, messageType.getType(columnPath.toArray()).asPrimitiveType().getPrimitiveTypeName(), CompressionCodecName.fromParquet(metaData.codec), readEncodings(metaData.encodings), readStats(metaData.statistics, messageType.getType(columnPath.toArray()).asPrimitiveType().getPrimitiveTypeName()), metaData.data_page_offset, metaData.dictionary_page_offset, metaData.num_values, metaData.total_compressed_size, metaData.total_uncompressed_size)-->Line_37 $$ blockMetaData.addColumn(column)[ FD ]
