Line_39 $$ isNew = true-->Line_60 $$ isNew = false[ FD ]
Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_31 $$ if (HiveWriteUtils.pathExists(hdfsEnvironment, target)) [ FD ]
Line_6 $$ Optional<String> partitionName-->Line_38 $$ if (partitionName.isPresent()) [ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_29 $$ write = locationService.writePath(locationHandle, partitionName).get()[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_23 $$ if (table == null) -->Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)[ CD ]
Line_22 $$ if (!partition.isPresent()) -->Line_66 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ CD ]
Line_22 $$ if (!partition.isPresent()) -->Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ CD ]
Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_47 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_22 $$ if (!partition.isPresent()) -->Line_62 $$ StorageDescriptor storageDescriptor = partition.get().getSd()[ CD ]
Line_20 $$ String outputFormat-->Line_52 $$ outputFormat = tableStorageFormat.getOutputFormat()[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_19 $$ Path write[ CD ]
Line_20 $$ String outputFormat-->Line_63 $$ outputFormat = storageDescriptor.getOutputFormat()[ FD ]
Line_23 $$ if (table == null) -->Line_25 $$ schema = new Properties()[ CD ]
Line_14 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())-->Line_66 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_12 $$ Optional<Partition> partition = Optional.empty()-->Line_14 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())[ FD ]
Line_38 $$ if (partitionName.isPresent()) -->Line_41 $$ if (immutablePartitions) [ CD ]
Line_66 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())-->Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_13 $$ if (!partitionRow.isEmpty() && table != null) [ FD ]
Line_6 $$ Optional<String> partitionName-->Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))[ FD ]
Line_13 $$ if (!partitionRow.isEmpty() && table != null) -->Line_14 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())[ CD ]
Line_48 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_18 $$ Path target[ CD ]
Line_20 $$ String outputFormat-->Line_50 $$ outputFormat = table.getSd().getOutputFormat()[ FD ]
Line_12 $$ Optional<Partition> partition = Optional.empty()-->Line_61 $$ HiveWriteUtils.checkPartitionIsWritable(partitionName.get(), partition.get())[ FD ]
Line_18 $$ Path target-->Line_48 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_17 $$ Properties schema[ CD ]
Line_22 $$ if (!partition.isPresent()) -->Line_65 $$ schema = MetaStoreUtils.getSchema(partition.get(), table)[ CD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_20 $$ String outputFormat[ CD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_16 $$ boolean isNew-->Line_60 $$ isNew = false[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_32 $$ throw new PrestoException(HIVE_PATH_ALREADY_EXISTS, format("Target directory for new partition '%s' of table '%s.%s' already exists: %s", partitionName, schemaName, tableName, target))[ FD ]
Line_38 $$ if (partitionName.isPresent()) -->Line_39 $$ isNew = true[ CD ]
Line_10 $$ partitionName = Optional.empty()-->Line_14 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_32 $$ throw new PrestoException(HIVE_PATH_ALREADY_EXISTS, format("Target directory for new partition '%s' of table '%s.%s' already exists: %s", partitionName, schemaName, tableName, target))[ FD ]
Line_25 $$ schema = new Properties()-->Line_27 $$ schema.setProperty(META_TABLE_COLUMN_TYPES, dataColumnTypes.stream().map(HiveType::<>toHiveType).map(HiveType::<>getHiveTypeName).collect(Collectors.joining(":")))[ FD ]
Line_19 $$ Path write-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_52 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_50 $$ outputFormat = table.getSd().getOutputFormat()-->Line_52 $$ outputFormat = tableStorageFormat.getOutputFormat()[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_14 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())[ FD ]
Line_18 $$ Path target-->Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_62 $$ StorageDescriptor storageDescriptor = partition.get().getSd()-->Line_63 $$ outputFormat = storageDescriptor.getOutputFormat()[ FD ]
Line_36 $$ serDe = tableStorageFormat.getSerDe()-->Line_54 $$ serDe = table.getSd().getSerdeInfo().getSerializationLib()[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_32 $$ throw new PrestoException(HIVE_PATH_ALREADY_EXISTS, format("Target directory for new partition '%s' of table '%s.%s' already exists: %s", partitionName, schemaName, tableName, target))[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_47 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_22 $$ if (!partition.isPresent()) -->Line_63 $$ outputFormat = storageDescriptor.getOutputFormat()[ CD ]
Line_36 $$ serDe = tableStorageFormat.getSerDe()-->Line_64 $$ serDe = storageDescriptor.getSerdeInfo().getSerializationLib()[ FD ]
Line_25 $$ schema = new Properties()-->Line_65 $$ schema = MetaStoreUtils.getSchema(partition.get(), table)[ FD ]
Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_19 $$ Path write-->Line_48 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_25 $$ schema = new Properties()-->Line_46 $$ schema = MetaStoreUtils.getSchema(table.getSd(), table.getSd(), table.getParameters(), schemaName, tableName, table.getPartitionKeys())[ FD ]
Line_35 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_50 $$ outputFormat = table.getSd().getOutputFormat()[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_12 $$ Optional<Partition> partition = Optional.empty()[ CD ]
Line_10 $$ partitionName = Optional.empty()-->Line_48 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_23 $$ if (table == null) -->Line_30 $$ if (partitionName.isPresent()) [ CD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ CD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_16 $$ boolean isNew[ CD ]
Line_30 $$ if (partitionName.isPresent()) -->Line_31 $$ if (HiveWriteUtils.pathExists(hdfsEnvironment, target)) [ CD ]
Line_10 $$ partitionName = Optional.empty()-->Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_48 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_7 $$ if (!partitionColumnNames.isEmpty()) -->Line_10 $$ partitionName = Optional.empty()[ CD ]
Line_6 $$ Optional<String> partitionName-->Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_16 $$ boolean isNew-->Line_44 $$ isNew = false[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_14 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())[ FD ]
Line_21 $$ String serDe-->Line_64 $$ serDe = storageDescriptor.getSerdeInfo().getSerializationLib()[ FD ]
Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_66 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_47 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_66 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_19 $$ Path write-->Line_29 $$ write = locationService.writePath(locationHandle, partitionName).get()[ FD ]
Line_39 $$ isNew = true-->Line_44 $$ isNew = false[ FD ]
Line_50 $$ outputFormat = table.getSd().getOutputFormat()-->Line_63 $$ outputFormat = storageDescriptor.getOutputFormat()[ FD ]
Line_17 $$ Properties schema-->Line_65 $$ schema = MetaStoreUtils.getSchema(partition.get(), table)[ FD ]
Line_29 $$ write = locationService.writePath(locationHandle, partitionName).get()-->Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_46 $$ schema = MetaStoreUtils.getSchema(table.getSd(), table.getSd(), table.getParameters(), schemaName, tableName, table.getPartitionKeys())-->Line_65 $$ schema = MetaStoreUtils.getSchema(partition.get(), table)[ FD ]
Line_17 $$ Properties schema-->Line_26 $$ schema.setProperty(META_TABLE_COLUMNS, Joiner.on(',').join(dataColumnNames))[ FD ]
Line_21 $$ String serDe-->Line_54 $$ serDe = table.getSd().getSerdeInfo().getSerializationLib()[ FD ]
Line_16 $$ boolean isNew-->Line_24 $$ isNew = true[ FD ]
Line_35 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_63 $$ outputFormat = storageDescriptor.getOutputFormat()[ FD ]
Line_49 $$ if (respectTableFormat) -->Line_52 $$ outputFormat = tableStorageFormat.getOutputFormat()[ CD ]
Line_38 $$ if (partitionName.isPresent()) -->Line_44 $$ isNew = false[ CD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_29 $$ write = locationService.writePath(locationHandle, partitionName).get()[ FD ]
Line_20 $$ String outputFormat-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_12 $$ Optional<Partition> partition = Optional.empty()-->Line_22 $$ if (!partition.isPresent()) [ FD ]
Line_35 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_52 $$ outputFormat = tableStorageFormat.getOutputFormat()[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_38 $$ if (partitionName.isPresent()) [ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_4 $$ List<String> partitionValues = partitionRow.stream().map(Object::<>toString).collect(toList())[ CD ]
Line_23 $$ if (table == null) -->Line_49 $$ if (respectTableFormat) [ CD ]
Line_10 $$ partitionName = Optional.empty()-->Line_47 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_35 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_18 $$ Path target-->Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_19 $$ Path write-->Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_17 $$ Properties schema-->Line_27 $$ schema.setProperty(META_TABLE_COLUMN_TYPES, dataColumnTypes.stream().map(HiveType::<>toHiveType).map(HiveType::<>getHiveTypeName).collect(Collectors.joining(":")))[ FD ]
Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_50 $$ outputFormat = table.getSd().getOutputFormat()-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_25 $$ schema = new Properties()-->Line_26 $$ schema.setProperty(META_TABLE_COLUMNS, Joiner.on(',').join(dataColumnNames))[ FD ]
Line_23 $$ if (table == null) -->Line_48 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ CD ]
Line_18 $$ Path target-->Line_66 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_44 $$ isNew = false-->Line_60 $$ isNew = false[ FD ]
Line_14 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())-->Line_22 $$ if (!partition.isPresent()) [ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_61 $$ HiveWriteUtils.checkPartitionIsWritable(partitionName.get(), partition.get())[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_7 $$ if (!partitionColumnNames.isEmpty()) [ CD ]
Line_23 $$ if (table == null) -->Line_46 $$ schema = MetaStoreUtils.getSchema(table.getSd(), table.getSd(), table.getParameters(), schemaName, tableName, table.getPartitionKeys())[ CD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_24 $$ isNew = true-->Line_44 $$ isNew = false[ FD ]
Line_52 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_63 $$ outputFormat = storageDescriptor.getOutputFormat()[ FD ]
Line_48 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)-->Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_63 $$ outputFormat = storageDescriptor.getOutputFormat()-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_22 $$ if (!partition.isPresent()) -->Line_60 $$ isNew = false[ CD ]
Line_24 $$ isNew = true-->Line_39 $$ isNew = true[ FD ]
Line_17 $$ Properties schema-->Line_25 $$ schema = new Properties()[ FD ]
Line_16 $$ boolean isNew-->Line_39 $$ isNew = true[ FD ]
Line_7 $$ if (!partitionColumnNames.isEmpty()) -->Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))[ CD ]
Line_18 $$ Path target-->Line_47 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_12 $$ Optional<Partition> partition = Optional.empty()-->Line_65 $$ schema = MetaStoreUtils.getSchema(partition.get(), table)[ FD ]
Line_12 $$ Optional<Partition> partition = Optional.empty()-->Line_66 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_66 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_10 $$ partitionName = Optional.empty()[ FD ]
Line_18 $$ Path target-->Line_32 $$ throw new PrestoException(HIVE_PATH_ALREADY_EXISTS, format("Target directory for new partition '%s' of table '%s.%s' already exists: %s", partitionName, schemaName, tableName, target))[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_30 $$ if (partitionName.isPresent()) [ FD ]
Line_20 $$ String outputFormat-->Line_35 $$ outputFormat = tableStorageFormat.getOutputFormat()[ FD ]
Line_23 $$ if (table == null) -->Line_29 $$ write = locationService.writePath(locationHandle, partitionName).get()[ CD ]
Line_23 $$ if (table == null) -->Line_47 $$ target = locationService.targetPath(locationHandle, partitionName)[ CD ]
Line_62 $$ StorageDescriptor storageDescriptor = partition.get().getSd()-->Line_64 $$ serDe = storageDescriptor.getSerdeInfo().getSerializationLib()[ FD ]
Line_14 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())-->Line_65 $$ schema = MetaStoreUtils.getSchema(partition.get(), table)[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_3 $$ checkArgument(partitionRow.size() == partitionColumnNames.size(), "size of partitionRow is different from partitionColumnNames")[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_22 $$ if (!partition.isPresent()) [ CD ]
Line_47 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_66 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_18 $$ Path target-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_23 $$ if (table == null) -->Line_24 $$ isNew = true[ CD ]
Line_23 $$ if (table == null) -->Line_35 $$ outputFormat = tableStorageFormat.getOutputFormat()[ CD ]
Line_23 $$ if (table == null) -->Line_38 $$ if (partitionName.isPresent()) [ CD ]
Line_18 $$ Path target-->Line_31 $$ if (HiveWriteUtils.pathExists(hdfsEnvironment, target)) [ FD ]
Line_14 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())-->Line_61 $$ HiveWriteUtils.checkPartitionIsWritable(partitionName.get(), partition.get())[ FD ]
Line_14 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())-->Line_62 $$ StorageDescriptor storageDescriptor = partition.get().getSd()[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_61 $$ HiveWriteUtils.checkPartitionIsWritable(partitionName.get(), partition.get())[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_6 $$ Optional<String> partitionName[ CD ]
Line_12 $$ Optional<Partition> partition = Optional.empty()-->Line_62 $$ StorageDescriptor storageDescriptor = partition.get().getSd()[ FD ]
Line_47 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_66 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_61 $$ HiveWriteUtils.checkPartitionIsWritable(partitionName.get(), partition.get())[ FD ]
Line_24 $$ isNew = true-->Line_60 $$ isNew = false[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_30 $$ if (partitionName.isPresent()) [ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_4 $$ List<String> partitionValues = partitionRow.stream().map(Object::<>toString).collect(toList())[ FD ]
Line_22 $$ if (!partition.isPresent()) -->Line_23 $$ if (table == null) [ CD ]
Line_23 $$ if (table == null) -->Line_26 $$ schema.setProperty(META_TABLE_COLUMNS, Joiner.on(',').join(dataColumnNames))[ CD ]
Line_6 $$ Optional<String> partitionName-->Line_30 $$ if (partitionName.isPresent()) [ FD ]
Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_32 $$ throw new PrestoException(HIVE_PATH_ALREADY_EXISTS, format("Target directory for new partition '%s' of table '%s.%s' already exists: %s", partitionName, schemaName, tableName, target))[ FD ]
Line_22 $$ if (!partition.isPresent()) -->Line_61 $$ HiveWriteUtils.checkPartitionIsWritable(partitionName.get(), partition.get())[ CD ]
Line_23 $$ if (table == null) -->Line_36 $$ serDe = tableStorageFormat.getSerDe()[ CD ]
Line_22 $$ if (!partition.isPresent()) -->Line_64 $$ serDe = storageDescriptor.getSerdeInfo().getSerializationLib()[ CD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_48 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_13 $$ if (!partitionRow.isEmpty() && table != null) [ CD ]
Line_6 $$ Optional<String> partitionName-->Line_29 $$ write = locationService.writePath(locationHandle, partitionName).get()[ FD ]
Line_49 $$ if (respectTableFormat) -->Line_50 $$ outputFormat = table.getSd().getOutputFormat()[ CD ]
Line_4 $$ List<String> partitionValues = partitionRow.stream().map(Object::<>toString).collect(toList())-->Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))[ FD ]
Line_54 $$ serDe = table.getSd().getSerdeInfo().getSerializationLib()-->Line_64 $$ serDe = storageDescriptor.getSerdeInfo().getSerializationLib()[ FD ]
Line_47 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_48 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_3 $$ checkArgument(partitionRow.size() == partitionColumnNames.size(), "size of partitionRow is different from partitionColumnNames")[ CD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_47 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_66 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_29 $$ write = locationService.writePath(locationHandle, partitionName).get()-->Line_69 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_17 $$ Properties schema-->Line_46 $$ schema = MetaStoreUtils.getSchema(table.getSd(), table.getSd(), table.getParameters(), schemaName, tableName, table.getPartitionKeys())[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_10 $$ partitionName = Optional.empty()[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_21 $$ String serDe[ CD ]
Line_23 $$ if (table == null) -->Line_54 $$ serDe = table.getSd().getSerdeInfo().getSerializationLib()[ CD ]
Line_29 $$ write = locationService.writePath(locationHandle, partitionName).get()-->Line_48 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_22 $$ if (!partition.isPresent()) -->Line_57 $$ if (immutablePartitions) [ CD ]
Line_23 $$ if (table == null) -->Line_27 $$ schema.setProperty(META_TABLE_COLUMN_TYPES, dataColumnTypes.stream().map(HiveType::<>toHiveType).map(HiveType::<>getHiveTypeName).collect(Collectors.joining(":")))[ CD ]
Line_6 $$ Optional<String> partitionName-->Line_48 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_38 $$ if (partitionName.isPresent()) [ FD ]
Line_21 $$ String serDe-->Line_36 $$ serDe = tableStorageFormat.getSerDe()[ FD ]
Line_28 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_67 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
