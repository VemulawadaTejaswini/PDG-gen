Line_3 $$ long globalSubsetSize = 0-->Line_7 $$ globalSubsetSize += terms.subsetSize[ FD ]
Line_4 $$ long globalSupersetSize = 0-->Line_19 $$ existingBuckets.add(bucket.newBucket(bucket.getSubsetDf(), globalSubsetSize, bucket.getSupersetDf(), globalSupersetSize, bucket.aggregations))[ FD ]
Line_14 $$ List<Bucket> existingBuckets = buckets.get(bucket.getKeyAsString())-->Line_17 $$ buckets.put(bucket.getKeyAsString(), existingBuckets)[ FD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_24 $$ BucketSignificancePriorityQueue ordered = new BucketSignificancePriorityQueue(size)[ CD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_27 $$ final Bucket b = sameTermBuckets.get(0).reduce(sameTermBuckets, reduceContext)[ FD ]
Line_14 $$ List<Bucket> existingBuckets = buckets.get(bucket.getKeyAsString())-->Line_15 $$ if (existingBuckets == null) [ FD ]
Line_27 $$ final Bucket b = sameTermBuckets.get(0).reduce(sameTermBuckets, reduceContext)-->Line_30 $$ ordered.insertWithOverflow(b)[ FD ]
Line_14 $$ List<Bucket> existingBuckets = buckets.get(bucket.getKeyAsString())-->Line_16 $$ existingBuckets = new ArrayList(aggregations.size())[ FD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_3 $$ long globalSubsetSize = 0[ CD ]
Line_3 $$ long globalSubsetSize = 0-->Line_19 $$ existingBuckets.add(bucket.newBucket(bucket.getSubsetDf(), globalSubsetSize, bucket.getSupersetDf(), globalSupersetSize, bucket.aggregations))[ FD ]
Line_7 $$ globalSubsetSize += terms.subsetSize-->Line_19 $$ existingBuckets.add(bucket.newBucket(bucket.getSubsetDf(), globalSubsetSize, bucket.getSupersetDf(), globalSupersetSize, bucket.aggregations))[ FD ]
Line_10 $$ Map<String, List<InternalSignificantTerms.Bucket>> buckets = new HashMap()-->Line_25 $$ for (Map.Entry<String, List<Bucket>> entry : buckets.entrySet()) [ FD ]
Line_10 $$ Map<String, List<InternalSignificantTerms.Bucket>> buckets = new HashMap()-->Line_17 $$ buckets.put(bucket.getKeyAsString(), existingBuckets)[ FD ]
Line_4 $$ long globalSupersetSize = 0-->Line_8 $$ globalSupersetSize += terms.supersetSize[ FD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_22 $$ significanceHeuristic.initialize(reduceContext)[ CD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_22 $$ significanceHeuristic.initialize(reduceContext)[ FD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_33 $$ Bucket[] list = new Bucket[ordered.size()][ CD ]
Line_16 $$ existingBuckets = new ArrayList(aggregations.size())-->Line_19 $$ existingBuckets.add(bucket.newBucket(bucket.getSubsetDf(), globalSubsetSize, bucket.getSupersetDf(), globalSupersetSize, bucket.aggregations))[ FD ]
Line_7 $$ globalSubsetSize += terms.subsetSize-->Line_37 $$ return create(globalSubsetSize, globalSupersetSize, Arrays.asList(list), this)[ FD ]
Line_29 $$ if ((b.score > 0) && (b.subsetDf >= minDocCount)) -->Line_30 $$ ordered.insertWithOverflow(b)[ CD ]
Line_33 $$ Bucket[] list = new Bucket[ordered.size()]-->Line_37 $$ return create(globalSubsetSize, globalSupersetSize, Arrays.asList(list), this)[ FD ]
Line_15 $$ if (existingBuckets == null) -->Line_17 $$ buckets.put(bucket.getKeyAsString(), existingBuckets)[ CD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_4 $$ long globalSupersetSize = 0[ CD ]
Line_25 $$ for (Map.Entry<String, List<Bucket>> entry : buckets.entrySet()) -->Line_26 $$ List<Bucket> sameTermBuckets = entry.getValue()[ FD ]
Line_24 $$ BucketSignificancePriorityQueue ordered = new BucketSignificancePriorityQueue(size)-->Line_34 $$ for (int i = ordered.size() - 1; i >= 0; i--) [ FD ]
Line_8 $$ globalSupersetSize += terms.supersetSize-->Line_37 $$ return create(globalSubsetSize, globalSupersetSize, Arrays.asList(list), this)[ FD ]
Line_13 $$ for (Bucket bucket : terms.buckets) -->Line_14 $$ List<Bucket> existingBuckets = buckets.get(bucket.getKeyAsString())[ FD ]
Line_15 $$ if (existingBuckets == null) -->Line_16 $$ existingBuckets = new ArrayList(aggregations.size())[ CD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_16 $$ existingBuckets = new ArrayList(aggregations.size())[ FD ]
Line_10 $$ Map<String, List<InternalSignificantTerms.Bucket>> buckets = new HashMap()-->Line_23 $$ final int size = Math.min(requiredSize, buckets.size())[ FD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_37 $$ return create(globalSubsetSize, globalSupersetSize, Arrays.asList(list), this)[ CD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_23 $$ final int size = Math.min(requiredSize, buckets.size())[ CD ]
Line_14 $$ List<Bucket> existingBuckets = buckets.get(bucket.getKeyAsString())-->Line_19 $$ existingBuckets.add(bucket.newBucket(bucket.getSubsetDf(), globalSubsetSize, bucket.getSupersetDf(), globalSupersetSize, bucket.aggregations))[ FD ]
Line_13 $$ for (Bucket bucket : terms.buckets) -->Line_17 $$ buckets.put(bucket.getKeyAsString(), existingBuckets)[ FD ]
Line_8 $$ globalSupersetSize += terms.supersetSize-->Line_19 $$ existingBuckets.add(bucket.newBucket(bucket.getSubsetDf(), globalSubsetSize, bucket.getSupersetDf(), globalSupersetSize, bucket.aggregations))[ FD ]
Line_16 $$ existingBuckets = new ArrayList(aggregations.size())-->Line_17 $$ buckets.put(bucket.getKeyAsString(), existingBuckets)[ FD ]
Line_24 $$ BucketSignificancePriorityQueue ordered = new BucketSignificancePriorityQueue(size)-->Line_30 $$ ordered.insertWithOverflow(b)[ FD ]
Line_24 $$ BucketSignificancePriorityQueue ordered = new BucketSignificancePriorityQueue(size)-->Line_35 $$ list[i] = (Bucket) ordered.pop()[ FD ]
Line_34 $$ for (int i = ordered.size() - 1; i >= 0; i--) -->Line_35 $$ list[i] = (Bucket) ordered.pop()[ CD ]
Line_27 $$ final Bucket b = sameTermBuckets.get(0).reduce(sameTermBuckets, reduceContext)-->Line_28 $$ b.updateScore(significanceHeuristic)[ FD ]
Line_24 $$ BucketSignificancePriorityQueue ordered = new BucketSignificancePriorityQueue(size)-->Line_33 $$ Bucket[] list = new Bucket[ordered.size()][ FD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_34 $$ for (int i = ordered.size() - 1; i >= 0; i--) [ CD ]
Line_3 $$ long globalSubsetSize = 0-->Line_37 $$ return create(globalSubsetSize, globalSupersetSize, Arrays.asList(list), this)[ FD ]
Line_26 $$ List<Bucket> sameTermBuckets = entry.getValue()-->Line_27 $$ final Bucket b = sameTermBuckets.get(0).reduce(sameTermBuckets, reduceContext)[ FD ]
Line_4 $$ long globalSupersetSize = 0-->Line_37 $$ return create(globalSubsetSize, globalSupersetSize, Arrays.asList(list), this)[ FD ]
Line_13 $$ for (Bucket bucket : terms.buckets) -->Line_19 $$ existingBuckets.add(bucket.newBucket(bucket.getSubsetDf(), globalSubsetSize, bucket.getSupersetDf(), globalSupersetSize, bucket.aggregations))[ FD ]
Line_2 $$ public InternalAggregation doReduce(List<InternalAggregation> aggregations, ReduceContext reduceContext) -->Line_10 $$ Map<String, List<InternalSignificantTerms.Bucket>> buckets = new HashMap()[ CD ]
Line_10 $$ Map<String, List<InternalSignificantTerms.Bucket>> buckets = new HashMap()-->Line_14 $$ List<Bucket> existingBuckets = buckets.get(bucket.getKeyAsString())[ FD ]
