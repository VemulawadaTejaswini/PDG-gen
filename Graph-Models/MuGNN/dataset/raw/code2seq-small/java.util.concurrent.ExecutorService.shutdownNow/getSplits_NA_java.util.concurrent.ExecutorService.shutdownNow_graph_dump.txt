Line_3 $$ Configuration conf = HadoopCompat.getConfiguration(context)-->Line_6 $$ cfName = ConfigHelper.getInputColumnFamily(conf)[ FD ]
Line_33 $$ for (TokenRange range : masterRangeNodes.keySet()) -->Line_39 $$ for (TokenRange intersection : range.intersectWith(jobTokenRange)) [ FD ]
Line_2 $$ public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException -->Line_10 $$ List<org.apache.hadoop.mapreduce.InputSplit> splits = new ArrayList()[ CD ]
Line_2 $$ public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException -->Line_8 $$ logger.trace("partitioner is [ CD ]
Line_14 $$ KeyRange jobKeyRange = ConfigHelper.getInputKeyRange(conf)-->Line_16 $$ if (jobKeyRange != null) [ FD ]
Line_10 $$ List<org.apache.hadoop.mapreduce.InputSplit> splits = new ArrayList()-->Line_56 $$ Collections.shuffle(splits, new Random(System.nanoTime()))[ FD ]
Line_33 $$ for (TokenRange range : masterRangeNodes.keySet()) -->Line_40 $$ splitfutures.add(executor.submit(new SplitCallable(intersection, masterRangeNodes.get(range), conf, session)))[ FD ]
Line_15 $$ Range<Token> jobRange = null-->Line_26 $$ jobRange = new Range(partitioner.getTokenFactory().fromString(jobKeyRange.start_token), partitioner.getTokenFactory().fromString(jobKeyRange.end_token))[ FD ]
Line_16 $$ if (jobKeyRange != null) -->Line_17 $$ if (jobKeyRange.start_key != null) [ CD ]
Line_2 $$ public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException -->Line_5 $$ keyspace = ConfigHelper.getInputKeyspace(conf)[ CD ]
Line_3 $$ Configuration conf = HadoopCompat.getConfiguration(context)-->Line_4 $$ validateConfiguration(conf)[ FD ]
Line_37 $$ TokenRange jobTokenRange = rangeToTokenRange(metadata, jobRange)-->Line_38 $$ if (range.intersects(jobTokenRange)) [ FD ]
Line_33 $$ for (TokenRange range : masterRangeNodes.keySet()) -->Line_35 $$ splitfutures.add(executor.submit(new SplitCallable(range, masterRangeNodes.get(range), conf, session)))[ FD ]
Line_34 $$ if (jobRange == null) -->Line_37 $$ TokenRange jobTokenRange = rangeToTokenRange(metadata, jobRange)[ CD ]
Line_9 $$ ExecutorService executor = new ThreadPoolExecutor(0, 128, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>())-->Line_40 $$ splitfutures.add(executor.submit(new SplitCallable(intersection, masterRangeNodes.get(range), conf, session)))[ FD ]
Line_24 $$ jobRange = new Range(partitioner.getToken(jobKeyRange.start_key), partitioner.getToken(jobKeyRange.end_key))-->Line_34 $$ if (jobRange == null) [ FD ]
Line_26 $$ jobRange = new Range(partitioner.getTokenFactory().fromString(jobKeyRange.start_token), partitioner.getTokenFactory().fromString(jobKeyRange.end_token))-->Line_34 $$ if (jobRange == null) [ FD ]
Line_25 $$ if (jobKeyRange.start_token != null) -->Line_28 $$ logger.warn("ignoring jobKeyRange specified without start_key or start_token")[ CD ]
Line_15 $$ Range<Token> jobRange = null-->Line_37 $$ TokenRange jobTokenRange = rangeToTokenRange(metadata, jobRange)[ FD ]
Line_2 $$ public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException -->Line_57 $$ return splits[ CD ]
Line_17 $$ if (jobKeyRange.start_key != null) -->Line_25 $$ if (jobKeyRange.start_token != null) [ CD ]
Line_15 $$ Range<Token> jobRange = null-->Line_34 $$ if (jobRange == null) [ FD ]
Line_2 $$ public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException -->Line_56 $$ Collections.shuffle(splits, new Random(System.nanoTime()))[ CD ]
Line_9 $$ ExecutorService executor = new ThreadPoolExecutor(0, 128, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>())-->Line_53 $$ executor.shutdownNow()[ FD ]
Line_17 $$ if (jobKeyRange.start_key != null) -->Line_20 $$ if (jobKeyRange.start_token != null)[ CD ]
Line_2 $$ public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException -->Line_6 $$ cfName = ConfigHelper.getInputColumnFamily(conf)[ CD ]
Line_2 $$ public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException -->Line_4 $$ validateConfiguration(conf)[ CD ]
Line_32 $$ Map<TokenRange, Set<Host>> masterRangeNodes = getRangeMap(keyspace, metadata)-->Line_33 $$ for (TokenRange range : masterRangeNodes.keySet()) [ FD ]
Line_13 $$ List<Future<List<org.apache.hadoop.mapreduce.InputSplit>>> splitfutures = new ArrayList()-->Line_40 $$ splitfutures.add(executor.submit(new SplitCallable(intersection, masterRangeNodes.get(range), conf, session)))[ FD ]
Line_2 $$ public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException -->Line_3 $$ Configuration conf = HadoopCompat.getConfiguration(context)[ CD ]
Line_17 $$ if (jobKeyRange.start_key != null) -->Line_24 $$ jobRange = new Range(partitioner.getToken(jobKeyRange.start_key), partitioner.getToken(jobKeyRange.end_key))[ CD ]
Line_2 $$ public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException -->Line_3 $$ Configuration conf = HadoopCompat.getConfiguration(context)[ FD ]
Line_34 $$ if (jobRange == null) -->Line_38 $$ if (range.intersects(jobTokenRange)) [ CD ]
Line_10 $$ List<org.apache.hadoop.mapreduce.InputSplit> splits = new ArrayList()-->Line_47 $$ splits.addAll(futureInputSplits.get())[ FD ]
Line_24 $$ jobRange = new Range(partitioner.getToken(jobKeyRange.start_key), partitioner.getToken(jobKeyRange.end_key))-->Line_37 $$ TokenRange jobTokenRange = rangeToTokenRange(metadata, jobRange)[ FD ]
Line_15 $$ Range<Token> jobRange = null-->Line_24 $$ jobRange = new Range(partitioner.getToken(jobKeyRange.start_key), partitioner.getToken(jobKeyRange.end_key))[ FD ]
Line_9 $$ ExecutorService executor = new ThreadPoolExecutor(0, 128, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>())-->Line_35 $$ splitfutures.add(executor.submit(new SplitCallable(range, masterRangeNodes.get(range), conf, session)))[ FD ]
Line_17 $$ if (jobKeyRange.start_key != null) -->Line_18 $$ if (!partitioner.preservesOrder())[ CD ]
Line_3 $$ Configuration conf = HadoopCompat.getConfiguration(context)-->Line_11 $$ try (Cluster cluster = CqlConfigHelper.getInputCluster(ConfigHelper.getInputInitialAddress(conf).split(","), conf);[ FD ]
Line_2 $$ public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException -->Line_9 $$ ExecutorService executor = new ThreadPoolExecutor(0, 128, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>())[ CD ]
Line_37 $$ TokenRange jobTokenRange = rangeToTokenRange(metadata, jobRange)-->Line_39 $$ for (TokenRange intersection : range.intersectWith(jobTokenRange)) [ FD ]
Line_45 $$ for (Future<List<org.apache.hadoop.mapreduce.InputSplit>> futureInputSplits : splitfutures) -->Line_47 $$ splits.addAll(futureInputSplits.get())[ FD ]
Line_24 $$ jobRange = new Range(partitioner.getToken(jobKeyRange.start_key), partitioner.getToken(jobKeyRange.end_key))-->Line_26 $$ jobRange = new Range(partitioner.getTokenFactory().fromString(jobKeyRange.start_token), partitioner.getTokenFactory().fromString(jobKeyRange.end_token))[ FD ]
Line_33 $$ for (TokenRange range : masterRangeNodes.keySet()) -->Line_38 $$ if (range.intersects(jobTokenRange)) [ FD ]
Line_3 $$ Configuration conf = HadoopCompat.getConfiguration(context)-->Line_5 $$ keyspace = ConfigHelper.getInputKeyspace(conf)[ FD ]
Line_3 $$ Configuration conf = HadoopCompat.getConfiguration(context)-->Line_7 $$ partitioner = ConfigHelper.getInputPartitioner(conf)[ FD ]
Line_17 $$ if (jobKeyRange.start_key != null) -->Line_22 $$ if (jobKeyRange.end_token != null)[ CD ]
Line_26 $$ jobRange = new Range(partitioner.getTokenFactory().fromString(jobKeyRange.start_token), partitioner.getTokenFactory().fromString(jobKeyRange.end_token))-->Line_37 $$ TokenRange jobTokenRange = rangeToTokenRange(metadata, jobRange)[ FD ]
Line_32 $$ Map<TokenRange, Set<Host>> masterRangeNodes = getRangeMap(keyspace, metadata)-->Line_40 $$ splitfutures.add(executor.submit(new SplitCallable(intersection, masterRangeNodes.get(range), conf, session)))[ FD ]
Line_32 $$ Map<TokenRange, Set<Host>> masterRangeNodes = getRangeMap(keyspace, metadata)-->Line_35 $$ splitfutures.add(executor.submit(new SplitCallable(range, masterRangeNodes.get(range), conf, session)))[ FD ]
Line_34 $$ if (jobRange == null) -->Line_35 $$ splitfutures.add(executor.submit(new SplitCallable(range, masterRangeNodes.get(range), conf, session)))[ CD ]
Line_11 $$ try (Cluster cluster = CqlConfigHelper.getInputCluster(ConfigHelper.getInputInitialAddress(conf).split(","), conf);-->Line_31 $$ Metadata metadata = cluster.getMetadata()[ FD ]
Line_31 $$ Metadata metadata = cluster.getMetadata()-->Line_37 $$ TokenRange jobTokenRange = rangeToTokenRange(metadata, jobRange)[ FD ]
Line_10 $$ List<org.apache.hadoop.mapreduce.InputSplit> splits = new ArrayList()-->Line_55 $$ assert splits.size() > 0[ FD ]
Line_31 $$ Metadata metadata = cluster.getMetadata()-->Line_32 $$ Map<TokenRange, Set<Host>> masterRangeNodes = getRangeMap(keyspace, metadata)[ FD ]
Line_13 $$ List<Future<List<org.apache.hadoop.mapreduce.InputSplit>>> splitfutures = new ArrayList()-->Line_35 $$ splitfutures.add(executor.submit(new SplitCallable(range, masterRangeNodes.get(range), conf, session)))[ FD ]
Line_3 $$ Configuration conf = HadoopCompat.getConfiguration(context)-->Line_14 $$ KeyRange jobKeyRange = ConfigHelper.getInputKeyRange(conf)[ FD ]
Line_2 $$ public List<org.apache.hadoop.mapreduce.InputSplit> getSplits(JobContext context) throws IOException -->Line_7 $$ partitioner = ConfigHelper.getInputPartitioner(conf)[ CD ]
Line_25 $$ if (jobKeyRange.start_token != null) -->Line_26 $$ jobRange = new Range(partitioner.getTokenFactory().fromString(jobKeyRange.start_token), partitioner.getTokenFactory().fromString(jobKeyRange.end_token))[ CD ]
