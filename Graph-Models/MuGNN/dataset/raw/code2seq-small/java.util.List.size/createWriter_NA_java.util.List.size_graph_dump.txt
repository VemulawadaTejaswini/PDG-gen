Line_19 $$ Path target-->Line_74 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_24 $$ if (table == null) -->Line_55 $$ if (respectTableFormat) [ CD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_23 $$ if (!partition.isPresent()) [ CD ]
Line_53 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_54 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_54 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_23 $$ if (!partition.isPresent()) -->Line_71 $$ outputFormat = storageDescriptor.getOutputFormat()[ CD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_14 $$ if (!partitionRow.isEmpty() && table != null) [ CD ]
Line_6 $$ Optional<String> partitionName-->Line_74 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_32 $$ write = locationService.writePath(locationHandle, partitionName).get()-->Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_31 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_15 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())-->Line_70 $$ StorageDescriptor storageDescriptor = partition.get().getSd()[ FD ]
Line_7 $$ if (!partitionColumnNames.isEmpty()) -->Line_10 $$ partitionName = Optional.empty()[ CD ]
Line_50 $$ isNew = false-->Line_67 $$ isNew = false[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_20 $$ Path write-->Line_32 $$ write = locationService.writePath(locationHandle, partitionName).get()[ FD ]
Line_23 $$ if (!partition.isPresent()) -->Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ CD ]
Line_15 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())-->Line_73 $$ schema = MetaStoreUtils.getSchema(partition.get(), table)[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_3 $$ checkArgument(partitionRow.size() == partitionColumnNames.size(), "size of partitionRow is different from partitionColumnNames")[ CD ]
Line_28 $$ schema = new Properties()-->Line_52 $$ schema = MetaStoreUtils.getSchema(table.getSd(), table.getSd(), table.getParameters(), schemaName, tableName, table.getPartitionKeys())[ FD ]
Line_55 $$ if (respectTableFormat) -->Line_56 $$ outputFormat = table.getSd().getOutputFormat()[ CD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ CD ]
Line_55 $$ if (respectTableFormat) -->Line_58 $$ outputFormat = tableStorageFormat.getOutputFormat()[ CD ]
Line_21 $$ String outputFormat-->Line_56 $$ outputFormat = table.getSd().getOutputFormat()[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_4 $$ List<String> partitionValues = // todo this seems wrong[ CD ]
Line_39 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_58 $$ outputFormat = tableStorageFormat.getOutputFormat()[ FD ]
Line_4 $$ List<String> partitionValues = // todo this seems wrong-->Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_53 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_45 $$ isNew = true-->Line_67 $$ isNew = false[ FD ]
Line_17 $$ boolean isNew-->Line_67 $$ isNew = false[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_10 $$ partitionName = Optional.empty()[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_33 $$ if (partitionName.isPresent()) [ FD ]
Line_39 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_71 $$ outputFormat = storageDescriptor.getOutputFormat()[ FD ]
Line_23 $$ if (!partition.isPresent()) -->Line_70 $$ StorageDescriptor storageDescriptor = partition.get().getSd()[ CD ]
Line_56 $$ outputFormat = table.getSd().getOutputFormat()-->Line_71 $$ outputFormat = storageDescriptor.getOutputFormat()[ FD ]
Line_21 $$ String outputFormat-->Line_39 $$ outputFormat = tableStorageFormat.getOutputFormat()[ FD ]
Line_53 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_14 $$ if (!partitionRow.isEmpty() && table != null) [ FD ]
Line_24 $$ if (table == null) -->Line_40 $$ serDe = tableStorageFormat.getSerDe()[ CD ]
Line_24 $$ if (table == null) -->Line_52 $$ schema = MetaStoreUtils.getSchema(table.getSd(), table.getSd(), table.getParameters(), schemaName, tableName, table.getPartitionKeys())[ CD ]
Line_19 $$ Path target-->Line_54 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_22 $$ String serDe-->Line_72 $$ serDe = storageDescriptor.getSerdeInfo().getSerializationLib()[ FD ]
Line_19 $$ Path target-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_31 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_3 $$ checkArgument(partitionRow.size() == partitionColumnNames.size(), "size of partitionRow is different from partitionColumnNames")[ FD ]
Line_24 $$ if (table == null) -->Line_33 $$ if (partitionName.isPresent()) [ CD ]
Line_24 $$ if (table == null) -->Line_60 $$ serDe = table.getSd().getSerdeInfo().getSerializationLib()[ CD ]
Line_28 $$ schema = new Properties()-->Line_73 $$ schema = MetaStoreUtils.getSchema(partition.get(), table)[ FD ]
Line_23 $$ if (!partition.isPresent()) -->Line_67 $$ isNew = false[ CD ]
Line_6 $$ Optional<String> partitionName-->Line_36 $$ throw new PrestoException(HIVE_PATH_ALREADY_EXISTS, format("Target directory for new partition '%s' of table '%s.%s' already exists: %s", partitionName, schemaName, tableName, target))[ FD ]
Line_40 $$ serDe = tableStorageFormat.getSerDe()-->Line_60 $$ serDe = table.getSd().getSerdeInfo().getSerializationLib()[ FD ]
Line_31 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_36 $$ throw new PrestoException(HIVE_PATH_ALREADY_EXISTS, format("Target directory for new partition '%s' of table '%s.%s' already exists: %s", partitionName, schemaName, tableName, target))[ FD ]
Line_23 $$ if (!partition.isPresent()) -->Line_24 $$ if (table == null) [ CD ]
Line_23 $$ if (!partition.isPresent()) -->Line_74 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ CD ]
Line_58 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_71 $$ outputFormat = storageDescriptor.getOutputFormat()[ FD ]
Line_24 $$ if (table == null) -->Line_29 $$ schema.setProperty(META_TABLE_COLUMNS, Joiner.on(',').join(dataColumnNames))[ CD ]
Line_52 $$ schema = MetaStoreUtils.getSchema(table.getSd(), table.getSd(), table.getParameters(), schemaName, tableName, table.getPartitionKeys())-->Line_73 $$ schema = MetaStoreUtils.getSchema(partition.get(), table)[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_31 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_23 $$ if (!partition.isPresent()) -->Line_72 $$ serDe = storageDescriptor.getSerdeInfo().getSerializationLib()[ CD ]
Line_24 $$ if (table == null) -->Line_54 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ CD ]
Line_15 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())-->Line_74 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_19 $$ Path target-->Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_10 $$ partitionName = Optional.empty()[ FD ]
Line_18 $$ Properties schema-->Line_73 $$ schema = MetaStoreUtils.getSchema(partition.get(), table)[ FD ]
Line_54 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)-->Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_6 $$ Optional<String> partitionName[ CD ]
Line_18 $$ Properties schema-->Line_30 $$ schema.setProperty(META_TABLE_COLUMN_TYPES, dataColumnTypes.stream().map(HiveType::<>toHiveType).map(HiveType::<>getHiveTypeName).collect(Collectors.joining(":")))[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_14 $$ if (!partitionRow.isEmpty() && table != null) -->Line_15 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())[ CD ]
Line_19 $$ Path target-->Line_36 $$ throw new PrestoException(HIVE_PATH_ALREADY_EXISTS, format("Target directory for new partition '%s' of table '%s.%s' already exists: %s", partitionName, schemaName, tableName, target))[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_53 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_19 $$ Path target-->Line_53 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_24 $$ if (table == null) -->Line_53 $$ target = locationService.targetPath(locationHandle, partitionName)[ CD ]
Line_53 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_74 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_17 $$ boolean isNew[ CD ]
Line_71 $$ outputFormat = storageDescriptor.getOutputFormat()-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_54 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_15 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())[ FD ]
Line_74 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_74 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())-->Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_36 $$ throw new PrestoException(HIVE_PATH_ALREADY_EXISTS, format("Target directory for new partition '%s' of table '%s.%s' already exists: %s", partitionName, schemaName, tableName, target))[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_36 $$ throw new PrestoException(HIVE_PATH_ALREADY_EXISTS, format("Target directory for new partition '%s' of table '%s.%s' already exists: %s", partitionName, schemaName, tableName, target))[ FD ]
Line_22 $$ String serDe-->Line_40 $$ serDe = tableStorageFormat.getSerDe()[ FD ]
Line_24 $$ if (table == null) -->Line_30 $$ schema.setProperty(META_TABLE_COLUMN_TYPES, dataColumnTypes.stream().map(HiveType::<>toHiveType).map(HiveType::<>getHiveTypeName).collect(Collectors.joining(":")))[ CD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_19 $$ Path target[ CD ]
Line_21 $$ String outputFormat-->Line_58 $$ outputFormat = tableStorageFormat.getOutputFormat()[ FD ]
Line_7 $$ if (!partitionColumnNames.isEmpty()) -->Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))[ CD ]
Line_6 $$ Optional<String> partitionName-->Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_21 $$ String outputFormat[ CD ]
Line_6 $$ Optional<String> partitionName-->Line_32 $$ write = locationService.writePath(locationHandle, partitionName).get()[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_31 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_21 $$ String outputFormat-->Line_71 $$ outputFormat = storageDescriptor.getOutputFormat()[ FD ]
Line_40 $$ serDe = tableStorageFormat.getSerDe()-->Line_72 $$ serDe = storageDescriptor.getSerdeInfo().getSerializationLib()[ FD ]
Line_31 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_54 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_15 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())[ FD ]
Line_60 $$ serDe = table.getSd().getSerdeInfo().getSerializationLib()-->Line_72 $$ serDe = storageDescriptor.getSerdeInfo().getSerializationLib()[ FD ]
Line_31 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_74 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_15 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())-->Line_23 $$ if (!partition.isPresent()) [ FD ]
Line_53 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_58 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_33 $$ if (partitionName.isPresent()) [ FD ]
Line_6 $$ Optional<String> partitionName-->Line_53 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_18 $$ Properties schema-->Line_52 $$ schema = MetaStoreUtils.getSchema(table.getSd(), table.getSd(), table.getParameters(), schemaName, tableName, table.getPartitionKeys())[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_7 $$ if (!partitionColumnNames.isEmpty()) [ CD ]
Line_23 $$ if (!partition.isPresent()) -->Line_73 $$ schema = MetaStoreUtils.getSchema(partition.get(), table)[ CD ]
Line_24 $$ if (table == null) -->Line_31 $$ target = locationService.targetPath(locationHandle, partitionName)[ CD ]
Line_10 $$ partitionName = Optional.empty()-->Line_32 $$ write = locationService.writePath(locationHandle, partitionName).get()[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_22 $$ String serDe[ CD ]
Line_10 $$ partitionName = Optional.empty()-->Line_31 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_28 $$ schema = new Properties()-->Line_29 $$ schema.setProperty(META_TABLE_COLUMNS, Joiner.on(',').join(dataColumnNames))[ FD ]
Line_20 $$ Path write-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_32 $$ write = locationService.writePath(locationHandle, partitionName).get()-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_39 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_56 $$ outputFormat = table.getSd().getOutputFormat()[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_18 $$ Properties schema[ CD ]
Line_17 $$ boolean isNew-->Line_50 $$ isNew = false[ FD ]
Line_20 $$ Path write-->Line_54 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_54 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_18 $$ Properties schema-->Line_28 $$ schema = new Properties()[ FD ]
Line_31 $$ target = locationService.targetPath(locationHandle, partitionName)-->Line_53 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_6 $$ Optional<String> partitionName-->Line_54 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_56 $$ outputFormat = table.getSd().getOutputFormat()-->Line_58 $$ outputFormat = tableStorageFormat.getOutputFormat()[ FD ]
Line_45 $$ isNew = true-->Line_50 $$ isNew = false[ FD ]
Line_39 $$ outputFormat = tableStorageFormat.getOutputFormat()-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_20 $$ Path write[ CD ]
Line_24 $$ if (table == null) -->Line_39 $$ outputFormat = tableStorageFormat.getOutputFormat()[ CD ]
Line_24 $$ if (table == null) -->Line_28 $$ schema = new Properties()[ CD ]
Line_17 $$ boolean isNew-->Line_45 $$ isNew = true[ FD ]
Line_21 $$ String outputFormat-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_19 $$ Path target-->Line_31 $$ target = locationService.targetPath(locationHandle, partitionName)[ FD ]
Line_70 $$ StorageDescriptor storageDescriptor = partition.get().getSd()-->Line_72 $$ serDe = storageDescriptor.getSerdeInfo().getSerializationLib()[ FD ]
Line_22 $$ String serDe-->Line_60 $$ serDe = table.getSd().getSerdeInfo().getSerializationLib()[ FD ]
Line_56 $$ outputFormat = table.getSd().getOutputFormat()-->Line_77 $$ return new HiveRecordWriter(schemaName, tableName, partitionName.orElse(""), isNew, dataColumnNames, dataColumnTypes, outputFormat, serDe, schema, generateRandomFileName(outputFormat), write.toString(), target.toString(), typeManager, conf)[ FD ]
Line_18 $$ Properties schema-->Line_29 $$ schema.setProperty(META_TABLE_COLUMNS, Joiner.on(',').join(dataColumnNames))[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_15 $$ partition = metastore.getPartition(schemaName, tableName, partitionName.get())[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_33 $$ if (partitionName.isPresent()) [ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_32 $$ write = locationService.writePath(locationHandle, partitionName).get()[ FD ]
Line_70 $$ StorageDescriptor storageDescriptor = partition.get().getSd()-->Line_71 $$ outputFormat = storageDescriptor.getOutputFormat()[ FD ]
Line_32 $$ write = locationService.writePath(locationHandle, partitionName).get()-->Line_54 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_10 $$ partitionName = Optional.empty()-->Line_74 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_2 $$ private HiveRecordWriter createWriter(List<Object> partitionRow) -->Line_4 $$ List<String> partitionValues = // todo this seems wrong[ FD ]
Line_8 $$ partitionName = Optional.of(FileUtils.makePartName(partitionColumnNames, partitionValues))-->Line_74 $$ target = locationService.targetPath(locationHandle, partition.get(), partitionName.get())[ FD ]
Line_24 $$ if (table == null) -->Line_32 $$ write = locationService.writePath(locationHandle, partitionName).get()[ CD ]
Line_20 $$ Path write-->Line_75 $$ write = locationService.writePath(locationHandle, partitionName).orElse(target)[ FD ]
Line_28 $$ schema = new Properties()-->Line_30 $$ schema.setProperty(META_TABLE_COLUMN_TYPES, dataColumnTypes.stream().map(HiveType::<>toHiveType).map(HiveType::<>getHiveTypeName).collect(Collectors.joining(":")))[ FD ]
