Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_9 $$ tableProperties.setProperty("columns.types", Joiner.on(',').join(transform(testColumns, TestColumn::<>getType)))[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_11 $$ if (compressionCodec != null) [ CD ]
Line_33 $$ writeValue = ((Slice) writeValue).getBytes()-->Line_35 $$ objectInspector.setStructFieldData(row, fields.get(i), writeValue)[ FD ]
Line_5 $$ JobConf jobConf = new JobConf()-->Line_15 $$ jobConf.set("parquet.compression", compressionCodec)[ FD ]
Line_29 $$ for (int rowNumber = 0; rowNumber < numRows; rowNumber++) -->Line_37 $$ Writable record = serDe.serialize(row, objectInspector)[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_37 $$ Writable record = serDe.serialize(row, objectInspector)[ FD ]
Line_7 $$ Properties tableProperties = new Properties()-->Line_8 $$ tableProperties.setProperty("columns", Joiner.on(',').join(transform(testColumns, TestColumn::<>getName)))[ FD ]
Line_31 $$ Object writeValue = testColumns.get(i).getWriteValue()-->Line_32 $$ if (writeValue instanceof Slice) [ FD ]
Line_11 $$ if (compressionCodec != null) -->Line_13 $$ jobConf.set(COMPRESS_CODEC, codec.getClass().getName())[ CD ]
Line_11 $$ if (compressionCodec != null) -->Line_14 $$ jobConf.set(COMPRESS_TYPE, SequenceFile.CompressionType.BLOCK.toString())[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_31 $$ Object writeValue = testColumns.get(i).getWriteValue()[ FD ]
Line_31 $$ Object writeValue = testColumns.get(i).getWriteValue()-->Line_35 $$ objectInspector.setStructFieldData(row, fields.get(i), writeValue)[ FD ]
Line_31 $$ Object writeValue = testColumns.get(i).getWriteValue()-->Line_33 $$ writeValue = ((Slice) writeValue).getBytes()[ FD ]
Line_11 $$ if (compressionCodec != null) -->Line_16 $$ jobConf.set("parquet.enable.dictionary", "true")[ CD ]
Line_18 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() -->Line_41 $$ recordWriter.close(false)[ FD ]
Line_30 $$ for (int i = 0; i < testColumns.size(); i++) -->Line_32 $$ if (writeValue instanceof Slice) [ CD ]
Line_30 $$ for (int i = 0; i < testColumns.size(); i++) -->Line_31 $$ Object writeValue = testColumns.get(i).getWriteValue()[ FD ]
Line_5 $$ JobConf jobConf = new JobConf()-->Line_18 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() [ FD ]
Line_29 $$ for (int rowNumber = 0; rowNumber < numRows; rowNumber++) -->Line_38 $$ recordWriter.write(record)[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_18 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() [ FD ]
Line_5 $$ JobConf jobConf = new JobConf()-->Line_14 $$ jobConf.set(COMPRESS_TYPE, SequenceFile.CompressionType.BLOCK.toString())[ FD ]
Line_32 $$ if (writeValue instanceof Slice) -->Line_33 $$ writeValue = ((Slice) writeValue).getBytes()[ CD ]
Line_7 $$ Properties tableProperties = new Properties()-->Line_9 $$ tableProperties.setProperty("columns.types", Joiner.on(',').join(transform(testColumns, TestColumn::<>getType)))[ FD ]
Line_30 $$ for (int i = 0; i < testColumns.size(); i++) -->Line_31 $$ Object writeValue = testColumns.get(i).getWriteValue()[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_10 $$ serDe.initialize(new Configuration(), tableProperties)[ FD ]
Line_5 $$ JobConf jobConf = new JobConf()-->Line_6 $$ ReaderWriterProfiler.setProfilerOptions(jobConf)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_9 $$ tableProperties.setProperty("columns.types", Joiner.on(',').join(transform(testColumns, TestColumn::<>getType)))[ CD ]
Line_5 $$ JobConf jobConf = new JobConf()-->Line_13 $$ jobConf.set(COMPRESS_CODEC, codec.getClass().getName())[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_46 $$ return new FileSplit(path, 0, file.length(), new String[0])[ CD ]
Line_33 $$ writeValue = ((Slice) writeValue).getBytes()-->Line_32 $$ if (writeValue instanceof Slice) [ FD ]
Line_11 $$ if (compressionCodec != null) -->Line_12 $$ CompressionCodec codec = new CompressionCodecFactory(new Configuration()).getCodecByName(compressionCodec)[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_12 $$ CompressionCodec codec = new CompressionCodecFactory(new Configuration()).getCodecByName(compressionCodec)[ FD ]
Line_5 $$ JobConf jobConf = new JobConf()-->Line_16 $$ jobConf.set("parquet.enable.dictionary", "true")[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_5 $$ JobConf jobConf = new JobConf()[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_10 $$ serDe.initialize(new Configuration(), tableProperties)[ CD ]
Line_11 $$ if (compressionCodec != null) -->Line_15 $$ jobConf.set("parquet.compression", compressionCodec)[ CD ]
Line_7 $$ Properties tableProperties = new Properties()-->Line_25 $$ serDe.initialize(new Configuration(), tableProperties)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_29 $$ for (int rowNumber = 0; rowNumber < numRows; rowNumber++) [ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_11 $$ if (compressionCodec != null) [ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_44 $$ path.getFileSystem(new Configuration()).setVerifyChecksum(true)[ CD ]
Line_30 $$ for (int i = 0; i < testColumns.size(); i++) -->Line_35 $$ objectInspector.setStructFieldData(row, fields.get(i), writeValue)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_26 $$ SettableStructObjectInspector objectInspector = getStandardStructObjectInspector(ImmutableList.copyOf(transform(testColumns, TestColumn::<>getName)), ImmutableList.copyOf(transform(testColumns, TestColumn::<>getObjectInspector)))[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_7 $$ Properties tableProperties = new Properties()[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_8 $$ tableProperties.setProperty("columns", Joiner.on(',').join(transform(testColumns, TestColumn::<>getName)))[ CD ]
Line_7 $$ Properties tableProperties = new Properties()-->Line_18 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() [ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_18 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() [ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_6 $$ ReaderWriterProfiler.setProfilerOptions(jobConf)[ CD ]
Line_7 $$ Properties tableProperties = new Properties()-->Line_10 $$ serDe.initialize(new Configuration(), tableProperties)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_30 $$ for (int i = 0; i < testColumns.size(); i++) [ FD ]
Line_30 $$ for (int i = 0; i < testColumns.size(); i++) -->Line_35 $$ objectInspector.setStructFieldData(row, fields.get(i), writeValue)[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_25 $$ serDe.initialize(new Configuration(), tableProperties)[ FD ]
Line_29 $$ for (int rowNumber = 0; rowNumber < numRows; rowNumber++) -->Line_30 $$ for (int i = 0; i < testColumns.size(); i++) [ CD ]
Line_12 $$ CompressionCodec codec = new CompressionCodecFactory(new Configuration()).getCodecByName(compressionCodec)-->Line_13 $$ jobConf.set(COMPRESS_CODEC, codec.getClass().getName())[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_15 $$ jobConf.set("parquet.compression", compressionCodec)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_8 $$ tableProperties.setProperty("columns", Joiner.on(',').join(transform(testColumns, TestColumn::<>getName)))[ FD ]
Line_18 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() -->Line_38 $$ recordWriter.write(record)[ FD ]
