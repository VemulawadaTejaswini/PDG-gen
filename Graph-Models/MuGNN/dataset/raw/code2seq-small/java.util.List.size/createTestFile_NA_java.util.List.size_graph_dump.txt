Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_23 $$ SettableStructObjectInspector objectInspector = getStandardStructObjectInspector(ImmutableList.copyOf(transform(testColumns, TestColumn::<>getName)), ImmutableList.copyOf(transform(testColumns, TestColumn::<>getObjectInspector)))[ FD ]
Line_3 $$ testColumns = ImmutableList.copyOf(filter(testColumns, not(TestColumn::<>isPartitionKey)))-->Line_8 $$ tableProperties.setProperty("columns.types", Joiner.on(',').join(transform(testColumns, TestColumn::<>getType)))[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_9 $$ serDe.initialize(new Configuration(), tableProperties)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_43 $$ return new FileSplit(path, 0, file.length(), new String[0])[ CD ]
Line_4 $$ JobConf jobConf = new JobConf()-->Line_5 $$ ReaderWriterProfiler.setProfilerOptions(jobConf)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_9 $$ serDe.initialize(new Configuration(), tableProperties)[ CD ]
Line_4 $$ JobConf jobConf = new JobConf()-->Line_14 $$ jobConf.set("parquet.compression", compressionCodec)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_14 $$ jobConf.set("parquet.compression", compressionCodec)[ FD ]
Line_3 $$ testColumns = ImmutableList.copyOf(filter(testColumns, not(TestColumn::<>isPartitionKey)))-->Line_27 $$ for (int i = 0; i < testColumns.size(); i++) [ FD ]
Line_27 $$ for (int i = 0; i < testColumns.size(); i++) -->Line_29 $$ if (writeValue instanceof Slice) [ CD ]
Line_10 $$ if (compressionCodec != null) -->Line_12 $$ jobConf.set(COMPRESS_CODEC, codec.getClass().getName())[ CD ]
Line_27 $$ for (int i = 0; i < testColumns.size(); i++) -->Line_28 $$ Object writeValue = testColumns.get(i).getWriteValue()[ CD ]
Line_26 $$ for (int rowNumber = 0; rowNumber < numRows; rowNumber++) -->Line_35 $$ recordWriter.write(record)[ CD ]
Line_4 $$ JobConf jobConf = new JobConf()-->Line_17 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() [ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_10 $$ if (compressionCodec != null) [ FD ]
Line_28 $$ Object writeValue = testColumns.get(i).getWriteValue()-->Line_29 $$ if (writeValue instanceof Slice) [ FD ]
Line_6 $$ Properties tableProperties = new Properties()-->Line_7 $$ tableProperties.setProperty("columns", Joiner.on(',').join(transform(testColumns, TestColumn::<>getName)))[ FD ]
Line_3 $$ testColumns = ImmutableList.copyOf(filter(testColumns, not(TestColumn::<>isPartitionKey)))-->Line_23 $$ SettableStructObjectInspector objectInspector = getStandardStructObjectInspector(ImmutableList.copyOf(transform(testColumns, TestColumn::<>getName)), ImmutableList.copyOf(transform(testColumns, TestColumn::<>getObjectInspector)))[ FD ]
Line_6 $$ Properties tableProperties = new Properties()-->Line_17 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() [ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_17 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() [ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_22 $$ serDe.initialize(new Configuration(), tableProperties)[ FD ]
Line_26 $$ for (int rowNumber = 0; rowNumber < numRows; rowNumber++) -->Line_27 $$ for (int i = 0; i < testColumns.size(); i++) [ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_28 $$ Object writeValue = testColumns.get(i).getWriteValue()[ FD ]
Line_28 $$ Object writeValue = testColumns.get(i).getWriteValue()-->Line_32 $$ objectInspector.setStructFieldData(row, fields.get(i), writeValue)[ FD ]
Line_6 $$ Properties tableProperties = new Properties()-->Line_9 $$ serDe.initialize(new Configuration(), tableProperties)[ FD ]
Line_10 $$ if (compressionCodec != null) -->Line_14 $$ jobConf.set("parquet.compression", compressionCodec)[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_27 $$ for (int i = 0; i < testColumns.size(); i++) [ FD ]
Line_3 $$ testColumns = ImmutableList.copyOf(filter(testColumns, not(TestColumn::<>isPartitionKey)))-->Line_28 $$ Object writeValue = testColumns.get(i).getWriteValue()[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_8 $$ tableProperties.setProperty("columns.types", Joiner.on(',').join(transform(testColumns, TestColumn::<>getType)))[ FD ]
Line_17 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() -->Line_38 $$ recordWriter.close(false)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_41 $$ path.getFileSystem(new Configuration()).setVerifyChecksum(true)[ CD ]
Line_10 $$ if (compressionCodec != null) -->Line_11 $$ CompressionCodec codec = new CompressionCodecFactory(new Configuration()).getCodecByName(compressionCodec)[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_5 $$ ReaderWriterProfiler.setProfilerOptions(jobConf)[ CD ]
Line_6 $$ Properties tableProperties = new Properties()-->Line_8 $$ tableProperties.setProperty("columns.types", Joiner.on(',').join(transform(testColumns, TestColumn::<>getType)))[ FD ]
Line_27 $$ for (int i = 0; i < testColumns.size(); i++) -->Line_32 $$ objectInspector.setStructFieldData(row, fields.get(i), writeValue)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_3 $$ testColumns = ImmutableList.copyOf(filter(testColumns, not(TestColumn::<>isPartitionKey)))[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_7 $$ tableProperties.setProperty("columns", Joiner.on(',').join(transform(testColumns, TestColumn::<>getName)))[ FD ]
Line_10 $$ if (compressionCodec != null) -->Line_15 $$ jobConf.set("parquet.enable.dictionary", "true")[ CD ]
Line_28 $$ Object writeValue = testColumns.get(i).getWriteValue()-->Line_30 $$ writeValue = ((Slice) writeValue).getBytes()[ FD ]
Line_3 $$ testColumns = ImmutableList.copyOf(filter(testColumns, not(TestColumn::<>isPartitionKey)))-->Line_7 $$ tableProperties.setProperty("columns", Joiner.on(',').join(transform(testColumns, TestColumn::<>getName)))[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_6 $$ Properties tableProperties = new Properties()[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_26 $$ for (int rowNumber = 0; rowNumber < numRows; rowNumber++) [ FD ]
Line_30 $$ writeValue = ((Slice) writeValue).getBytes()-->Line_32 $$ objectInspector.setStructFieldData(row, fields.get(i), writeValue)[ FD ]
Line_29 $$ if (writeValue instanceof Slice) -->Line_30 $$ writeValue = ((Slice) writeValue).getBytes()[ CD ]
Line_26 $$ for (int rowNumber = 0; rowNumber < numRows; rowNumber++) -->Line_34 $$ Writable record = serDe.serialize(row, objectInspector)[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_10 $$ if (compressionCodec != null) [ CD ]
Line_4 $$ JobConf jobConf = new JobConf()-->Line_12 $$ jobConf.set(COMPRESS_CODEC, codec.getClass().getName())[ FD ]
Line_27 $$ for (int i = 0; i < testColumns.size(); i++) -->Line_28 $$ Object writeValue = testColumns.get(i).getWriteValue()[ FD ]
Line_30 $$ writeValue = ((Slice) writeValue).getBytes()-->Line_29 $$ if (writeValue instanceof Slice) [ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_7 $$ tableProperties.setProperty("columns", Joiner.on(',').join(transform(testColumns, TestColumn::<>getName)))[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_8 $$ tableProperties.setProperty("columns.types", Joiner.on(',').join(transform(testColumns, TestColumn::<>getType)))[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_4 $$ JobConf jobConf = new JobConf()[ CD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_34 $$ Writable record = serDe.serialize(row, objectInspector)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_3 $$ testColumns = ImmutableList.copyOf(filter(testColumns, not(TestColumn::<>isPartitionKey)))[ CD ]
Line_4 $$ JobConf jobConf = new JobConf()-->Line_15 $$ jobConf.set("parquet.enable.dictionary", "true")[ FD ]
Line_17 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() -->Line_35 $$ recordWriter.write(record)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_11 $$ CompressionCodec codec = new CompressionCodecFactory(new Configuration()).getCodecByName(compressionCodec)[ FD ]
Line_10 $$ if (compressionCodec != null) -->Line_13 $$ jobConf.set(COMPRESS_TYPE, SequenceFile.CompressionType.BLOCK.toString())[ CD ]
Line_6 $$ Properties tableProperties = new Properties()-->Line_22 $$ serDe.initialize(new Configuration(), tableProperties)[ FD ]
Line_2 $$ public static FileSplit createTestFile(String filePath, HiveOutputFormat<?, ?> outputFormat, @SuppressWarnings("deprecation") SerDe serDe, String compressionCodec, List<TestColumn> testColumns, int numRows) throws Exception -->Line_17 $$ RecordWriter recordWriter = outputFormat.getHiveRecordWriter(jobConf, new Path(filePath), Text.class, compressionCodec != null, tableProperties, new Progressable() [ FD ]
Line_4 $$ JobConf jobConf = new JobConf()-->Line_13 $$ jobConf.set(COMPRESS_TYPE, SequenceFile.CompressionType.BLOCK.toString())[ FD ]
Line_27 $$ for (int i = 0; i < testColumns.size(); i++) -->Line_32 $$ objectInspector.setStructFieldData(row, fields.get(i), writeValue)[ CD ]
Line_11 $$ CompressionCodec codec = new CompressionCodecFactory(new Configuration()).getCodecByName(compressionCodec)-->Line_12 $$ jobConf.set(COMPRESS_CODEC, codec.getClass().getName())[ FD ]
