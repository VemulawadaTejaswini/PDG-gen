Line_2 $$ public static void main(String[] args) throws Exception -->Line_13 $$ Configuration cfg = new Configuration()[ CD ]
Line_8 $$ Path hadoopHome = baseDir.resolve("hadoop-home")-->Line_9 $$ Files.createDirectories(hadoopHome)[ FD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_23 $$ Files.move(tmp, baseDir.resolve(PORT_FILE_NAME), StandardCopyOption.ATOMIC_MOVE)[ CD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_20 $$ Files.move(tmp, baseDir.resolve(PID_FILE_NAME), StandardCopyOption.ATOMIC_MOVE)[ CD ]
Line_18 $$ String pid = ManagementFactory.getRuntimeMXBean().getName().split("@")[0]-->Line_19 $$ Files.write(tmp, pid.getBytes(StandardCharsets.UTF_8))[ FD ]
Line_6 $$ Path baseDir = Paths.get(args[0])-->Line_8 $$ Path hadoopHome = baseDir.resolve("hadoop-home")[ FD ]
Line_8 $$ Path hadoopHome = baseDir.resolve("hadoop-home")-->Line_10 $$ System.setProperty("hadoop.home.dir", hadoopHome.toAbsolutePath().toString())[ FD ]
Line_13 $$ Configuration cfg = new Configuration()-->Line_15 $$ cfg.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_PERMISSION_KEY, "766")[ FD ]
Line_16 $$ MiniDFSCluster dfs = new MiniDFSCluster.Builder(cfg).nameNodePort(9999).build()-->Line_22 $$ Files.write(tmp, Integer.toString(dfs.getNameNodePort()).getBytes(StandardCharsets.UTF_8))[ FD ]
Line_7 $$ if (System.getenv("HADOOP_HOME") == null) -->Line_8 $$ Path hadoopHome = baseDir.resolve("hadoop-home")[ CD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_18 $$ String pid = ManagementFactory.getRuntimeMXBean().getName().split("@")[0][ CD ]
Line_13 $$ Configuration cfg = new Configuration()-->Line_14 $$ cfg.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, hdfsHome.toAbsolutePath().toString())[ FD ]
Line_17 $$ Path tmp = Files.createTempFile(baseDir, null, null)-->Line_20 $$ Files.move(tmp, baseDir.resolve(PID_FILE_NAME), StandardCopyOption.ATOMIC_MOVE)[ FD ]
Line_6 $$ Path baseDir = Paths.get(args[0])-->Line_12 $$ Path hdfsHome = baseDir.resolve("hdfs-data")[ FD ]
Line_17 $$ Path tmp = Files.createTempFile(baseDir, null, null)-->Line_19 $$ Files.write(tmp, pid.getBytes(StandardCharsets.UTF_8))[ FD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_3 $$ if (args.length != 1) [ CD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_7 $$ if (System.getenv("HADOOP_HOME") == null) [ CD ]
Line_6 $$ Path baseDir = Paths.get(args[0])-->Line_20 $$ Files.move(tmp, baseDir.resolve(PID_FILE_NAME), StandardCopyOption.ATOMIC_MOVE)[ FD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_22 $$ Files.write(tmp, Integer.toString(dfs.getNameNodePort()).getBytes(StandardCharsets.UTF_8))[ CD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_16 $$ MiniDFSCluster dfs = new MiniDFSCluster.Builder(cfg).nameNodePort(9999).build()[ CD ]
Line_17 $$ Path tmp = Files.createTempFile(baseDir, null, null)-->Line_23 $$ Files.move(tmp, baseDir.resolve(PORT_FILE_NAME), StandardCopyOption.ATOMIC_MOVE)[ FD ]
Line_21 $$ tmp = Files.createTempFile(baseDir, null, null)-->Line_22 $$ Files.write(tmp, Integer.toString(dfs.getNameNodePort()).getBytes(StandardCharsets.UTF_8))[ FD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_17 $$ Path tmp = Files.createTempFile(baseDir, null, null)[ CD ]
Line_17 $$ Path tmp = Files.createTempFile(baseDir, null, null)-->Line_21 $$ tmp = Files.createTempFile(baseDir, null, null)[ FD ]
Line_21 $$ tmp = Files.createTempFile(baseDir, null, null)-->Line_23 $$ Files.move(tmp, baseDir.resolve(PORT_FILE_NAME), StandardCopyOption.ATOMIC_MOVE)[ FD ]
Line_17 $$ Path tmp = Files.createTempFile(baseDir, null, null)-->Line_22 $$ Files.write(tmp, Integer.toString(dfs.getNameNodePort()).getBytes(StandardCharsets.UTF_8))[ FD ]
Line_6 $$ Path baseDir = Paths.get(args[0])-->Line_23 $$ Files.move(tmp, baseDir.resolve(PORT_FILE_NAME), StandardCopyOption.ATOMIC_MOVE)[ FD ]
Line_7 $$ if (System.getenv("HADOOP_HOME") == null) -->Line_10 $$ System.setProperty("hadoop.home.dir", hadoopHome.toAbsolutePath().toString())[ CD ]
Line_12 $$ Path hdfsHome = baseDir.resolve("hdfs-data")-->Line_14 $$ cfg.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, hdfsHome.toAbsolutePath().toString())[ FD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_15 $$ cfg.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_PERMISSION_KEY, "766")[ CD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_12 $$ Path hdfsHome = baseDir.resolve("hdfs-data")[ CD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_21 $$ tmp = Files.createTempFile(baseDir, null, null)[ CD ]
Line_6 $$ Path baseDir = Paths.get(args[0])-->Line_17 $$ Path tmp = Files.createTempFile(baseDir, null, null)[ FD ]
Line_6 $$ Path baseDir = Paths.get(args[0])-->Line_21 $$ tmp = Files.createTempFile(baseDir, null, null)[ FD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_19 $$ Files.write(tmp, pid.getBytes(StandardCharsets.UTF_8))[ CD ]
Line_7 $$ if (System.getenv("HADOOP_HOME") == null) -->Line_9 $$ Files.createDirectories(hadoopHome)[ CD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_14 $$ cfg.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, hdfsHome.toAbsolutePath().toString())[ CD ]
Line_2 $$ public static void main(String[] args) throws Exception -->Line_6 $$ Path baseDir = Paths.get(args[0])[ CD ]
