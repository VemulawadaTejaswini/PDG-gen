Line_19 $$ JobConf job = new JobConf()-->Line_31 $$ job.setCombinerKeyGroupingComparator(GroupComparator.class)[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_37 $$ Counters counters = runningJob.getCounters()[ CD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_47 $$ line = br.readLine()[ CD ]
Line_42 $$ BufferedReader br = new BufferedReader(new FileReader(new File(out, "part-00000")))-->Line_52 $$ br.close()[ FD ]
Line_53 $$ Set<String> expected = new HashSet<String>()-->Line_54 $$ expected.add("A2")[ FD ]
Line_19 $$ JobConf job = new JobConf()-->Line_25 $$ job.setInputFormat(TextInputFormat.class)[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_55 $$ expected.add("B5")[ CD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_38 $$ long combinerInputRecords = counters.getGroup("org.apache.hadoop.mapreduce.TaskCounter").getCounter("COMBINE_INPUT_RECORDS")[ CD ]
Line_44 $$ String line = br.readLine()-->Line_49 $$ output.add(line.substring(0, 1) + line.substring(4, 5))[ FD ]
Line_47 $$ line = br.readLine()-->Line_49 $$ output.add(line.substring(0, 1) + line.substring(4, 5))[ FD ]
Line_19 $$ JobConf job = new JobConf()-->Line_27 $$ job.setMapOutputValueClass(LongWritable.class)[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_53 $$ Set<String> expected = new HashSet<String>()[ CD ]
Line_19 $$ JobConf job = new JobConf()-->Line_26 $$ job.setMapOutputKeyClass(Text.class)[ FD ]
Line_7 $$ File in = new File(TEST_ROOT_DIR, "input")-->Line_8 $$ if (!in.mkdirs()) [ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_54 $$ expected.add("A2")[ CD ]
Line_47 $$ line = br.readLine()-->Line_48 $$ Assert.assertNotNull(line)[ FD ]
Line_12 $$ PrintWriter pw = new PrintWriter(new FileWriter(new File(in, "data.txt")))-->Line_16 $$ pw.println("B|b,4")[ FD ]
Line_12 $$ PrintWriter pw = new PrintWriter(new FileWriter(new File(in, "data.txt")))-->Line_13 $$ pw.println("A|a,1")[ FD ]
Line_7 $$ File in = new File(TEST_ROOT_DIR, "input")-->Line_21 $$ TextInputFormat.setInputPaths(job, new Path(in.getPath()))[ FD ]
Line_19 $$ JobConf job = new JobConf()-->Line_29 $$ job.setOutputValueGroupingComparator(GroupComparator.class)[ FD ]
Line_12 $$ PrintWriter pw = new PrintWriter(new FileWriter(new File(in, "data.txt")))-->Line_17 $$ pw.println("B|c,5")[ FD ]
Line_44 $$ String line = br.readLine()-->Line_48 $$ Assert.assertNotNull(line)[ FD ]
Line_34 $$ RunningJob runningJob = client.submitJob(job)-->Line_37 $$ Counters counters = runningJob.getCounters()[ FD ]
Line_19 $$ JobConf job = new JobConf()-->Line_23 $$ job.setMapperClass(Map.class)[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_48 $$ Assert.assertNotNull(line)[ CD ]
Line_42 $$ BufferedReader br = new BufferedReader(new FileReader(new File(out, "part-00000")))-->Line_47 $$ line = br.readLine()[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_50 $$ line = br.readLine()[ CD ]
Line_50 $$ line = br.readLine()-->Line_51 $$ Assert.assertNull(line)[ FD ]
Line_19 $$ JobConf job = new JobConf()-->Line_20 $$ job.set("mapreduce.framework.name", "local")[ FD ]
Line_11 $$ File out = new File(TEST_ROOT_DIR, "output")-->Line_22 $$ TextOutputFormat.setOutputPath(job, new Path(out.getPath()))[ FD ]
Line_34 $$ RunningJob runningJob = client.submitJob(job)-->Line_36 $$ if (runningJob.isSuccessful()) [ FD ]
Line_19 $$ JobConf job = new JobConf()-->Line_32 $$ job.setInt("min.num.spills.for.combine", 0)[ FD ]
Line_33 $$ JobClient client = new JobClient(job)-->Line_34 $$ RunningJob runningJob = client.submitJob(job)[ FD ]
Line_44 $$ String line = br.readLine()-->Line_45 $$ Assert.assertNotNull(line)[ FD ]
Line_44 $$ String line = br.readLine()-->Line_46 $$ output.add(line.substring(0, 1) + line.substring(4, 5))[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_49 $$ output.add(line.substring(0, 1) + line.substring(4, 5))[ CD ]
Line_12 $$ PrintWriter pw = new PrintWriter(new FileWriter(new File(in, "data.txt")))-->Line_15 $$ pw.println("B|a,3")[ FD ]
Line_43 $$ Set<String> output = new HashSet<String>()-->Line_49 $$ output.add(line.substring(0, 1) + line.substring(4, 5))[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_56 $$ Assert.assertEquals(expected, output)[ CD ]
Line_12 $$ PrintWriter pw = new PrintWriter(new FileWriter(new File(in, "data.txt")))-->Line_18 $$ pw.close()[ FD ]
Line_19 $$ JobConf job = new JobConf()-->Line_21 $$ TextInputFormat.setInputPaths(job, new Path(in.getPath()))[ FD ]
Line_19 $$ JobConf job = new JobConf()-->Line_34 $$ RunningJob runningJob = client.submitJob(job)[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_41 $$ Assert.assertTrue(combinerInputRecords > combinerOutputRecords)[ CD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_42 $$ BufferedReader br = new BufferedReader(new FileReader(new File(out, "part-00000")))[ CD ]
Line_12 $$ PrintWriter pw = new PrintWriter(new FileWriter(new File(in, "data.txt")))-->Line_14 $$ pw.println("A|b,2")[ FD ]
Line_37 $$ Counters counters = runningJob.getCounters()-->Line_38 $$ long combinerInputRecords = counters.getGroup("org.apache.hadoop.mapreduce.TaskCounter").getCounter("COMBINE_INPUT_RECORDS")[ FD ]
Line_19 $$ JobConf job = new JobConf()-->Line_22 $$ TextOutputFormat.setOutputPath(job, new Path(out.getPath()))[ FD ]
Line_47 $$ line = br.readLine()-->Line_51 $$ Assert.assertNull(line)[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_43 $$ Set<String> output = new HashSet<String>()[ CD ]
Line_53 $$ Set<String> expected = new HashSet<String>()-->Line_56 $$ Assert.assertEquals(expected, output)[ FD ]
Line_44 $$ String line = br.readLine()-->Line_50 $$ line = br.readLine()[ FD ]
Line_19 $$ JobConf job = new JobConf()-->Line_30 $$ job.setCombinerClass(Combiner.class)[ FD ]
Line_34 $$ RunningJob runningJob = client.submitJob(job)-->Line_35 $$ runningJob.waitForCompletion()[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_44 $$ String line = br.readLine()[ CD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_52 $$ br.close()[ CD ]
Line_43 $$ Set<String> output = new HashSet<String>()-->Line_56 $$ Assert.assertEquals(expected, output)[ FD ]
Line_42 $$ BufferedReader br = new BufferedReader(new FileReader(new File(out, "part-00000")))-->Line_44 $$ String line = br.readLine()[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_40 $$ Assert.assertTrue(combinerInputRecords > 0)[ CD ]
Line_47 $$ line = br.readLine()-->Line_50 $$ line = br.readLine()[ FD ]
Line_53 $$ Set<String> expected = new HashSet<String>()-->Line_55 $$ expected.add("B5")[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_58 $$ Assert.fail("Job failed")[ CD ]
Line_19 $$ JobConf job = new JobConf()-->Line_24 $$ job.setReducerClass(Reduce.class)[ FD ]
Line_19 $$ JobConf job = new JobConf()-->Line_28 $$ job.setOutputFormat(TextOutputFormat.class)[ FD ]
Line_44 $$ String line = br.readLine()-->Line_51 $$ Assert.assertNull(line)[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_51 $$ Assert.assertNull(line)[ CD ]
Line_44 $$ String line = br.readLine()-->Line_47 $$ line = br.readLine()[ FD ]
Line_42 $$ BufferedReader br = new BufferedReader(new FileReader(new File(out, "part-00000")))-->Line_50 $$ line = br.readLine()[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_39 $$ long combinerOutputRecords = counters.getGroup("org.apache.hadoop.mapreduce.TaskCounter").getCounter("COMBINE_OUTPUT_RECORDS")[ CD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_45 $$ Assert.assertNotNull(line)[ CD ]
Line_43 $$ Set<String> output = new HashSet<String>()-->Line_46 $$ output.add(line.substring(0, 1) + line.substring(4, 5))[ FD ]
Line_36 $$ if (runningJob.isSuccessful()) -->Line_46 $$ output.add(line.substring(0, 1) + line.substring(4, 5))[ CD ]
Line_37 $$ Counters counters = runningJob.getCounters()-->Line_39 $$ long combinerOutputRecords = counters.getGroup("org.apache.hadoop.mapreduce.TaskCounter").getCounter("COMBINE_OUTPUT_RECORDS")[ FD ]
