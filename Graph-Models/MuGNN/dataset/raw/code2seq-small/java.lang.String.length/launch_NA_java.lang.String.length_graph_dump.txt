Line_2 $$ public static void launch() throws Exception -->Line_38 $$ job.setMapOutputKeyClass(Text.class)[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_39 $$ job.setMapOutputValueClass(Text.class)[ CD ]
Line_52 $$ String outdata = MapReduceTestUtil.readOutput(outPath, job)-->Line_54 $$ System.out.println(outdata.toString())[ FD ]
Line_6 $$ Path OUTPUT_DIR = new Path("build/test/output_for_aggregates_test")-->Line_11 $$ fs.delete(OUTPUT_DIR, true)[ FD ]
Line_17 $$ for (int i = 1; i < numOfInputLines; i++) -->Line_22 $$ inputData.append(" ").append(idFormat.format(i))[ FD ]
Line_4 $$ FileSystem fs = FileSystem.get(conf)-->Line_16 $$ FSDataOutputStream fileOut = fs.create(new Path(INPUT_DIR, inputFile))[ FD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_46 $$ job.setInt("aggregator.descriptor.num", 1)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_36 $$ FileOutputFormat.setOutputPath(job, OUTPUT_DIR)[ CD ]
Line_12 $$ StringBuffer inputData = new StringBuffer()-->Line_29 $$ fileOut.write(inputData.toString().getBytes("utf-8"))[ FD ]
Line_4 $$ FileSystem fs = FileSystem.get(conf)-->Line_9 $$ fs.delete(INPUT_DIR, true)[ FD ]
Line_17 $$ for (int i = 1; i < numOfInputLines; i++) -->Line_20 $$ inputData.append(idFormat.format(i))[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_29 $$ fileOut.write(inputData.toString().getBytes("utf-8"))[ CD ]
Line_12 $$ StringBuffer inputData = new StringBuffer()-->Line_24 $$ inputData.append("\n")[ FD ]
Line_4 $$ FileSystem fs = FileSystem.get(conf)-->Line_57 $$ fs.delete(INPUT_DIR, true)[ FD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_41 $$ job.setOutputValueClass(Text.class)[ FD ]
Line_5 $$ int numOfInputLines = 20-->Line_17 $$ for (int i = 1; i < numOfInputLines; i++) [ FD ]
Line_16 $$ FSDataOutputStream fileOut = fs.create(new Path(INPUT_DIR, inputFile))-->Line_29 $$ fileOut.write(inputData.toString().getBytes("utf-8"))[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_47 $$ job.set("aggregator.descriptor.0", "UserDefined,org.apache.hadoop.mapred.lib.aggregate.AggregatorTests")[ CD ]
Line_17 $$ for (int i = 1; i < numOfInputLines; i++) -->Line_19 $$ expectedOutput.append("\t").append(i).append("\n")[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_10 $$ fs.mkdirs(INPUT_DIR)[ CD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_34 $$ FileInputFormat.setInputPaths(job, INPUT_DIR)[ FD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_36 $$ FileOutputFormat.setOutputPath(job, OUTPUT_DIR)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_28 $$ expectedOutput.append("uniq_count\t15\n")[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_14 $$ expectedOutput.append("max\t19\n")[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_35 $$ job.setInputFormat(TextInputFormat.class)[ CD ]
Line_4 $$ FileSystem fs = FileSystem.get(conf)-->Line_10 $$ fs.mkdirs(INPUT_DIR)[ FD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_37 $$ job.setOutputFormat(TextOutputFormat.class)[ FD ]
Line_12 $$ StringBuffer inputData = new StringBuffer()-->Line_20 $$ inputData.append(idFormat.format(i))[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_51 $$ Path outPath = new Path(OUTPUT_DIR, "part-00000")[ CD ]
Line_17 $$ for (int i = 1; i < numOfInputLines; i++) -->Line_21 $$ for (int j = 1; j < i; j++) [ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_32 $$ System.out.println(inputData.toString())[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_3 $$ JobConf conf = new JobConf(TestAggregates.class)[ CD ]
Line_17 $$ for (int i = 1; i < numOfInputLines; i++) -->Line_18 $$ expectedOutput.append("count_").append(idFormat.format(i))[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_5 $$ int numOfInputLines = 20[ CD ]
Line_13 $$ StringBuffer expectedOutput = new StringBuffer()-->Line_19 $$ expectedOutput.append("\t").append(i).append("\n")[ FD ]
Line_4 $$ FileSystem fs = FileSystem.get(conf)-->Line_11 $$ fs.delete(OUTPUT_DIR, true)[ FD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_40 $$ job.setOutputKeyClass(Text.class)[ FD ]
Line_13 $$ StringBuffer expectedOutput = new StringBuffer()-->Line_56 $$ assertEquals(expectedOutput.toString(), outdata)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_53 $$ System.out.println("full out data:")[ CD ]
Line_17 $$ for (int i = 1; i < numOfInputLines; i++) -->Line_18 $$ expectedOutput.append("count_").append(idFormat.format(i))[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_34 $$ FileInputFormat.setInputPaths(job, INPUT_DIR)[ CD ]
Line_17 $$ for (int i = 1; i < numOfInputLines; i++) -->Line_20 $$ inputData.append(idFormat.format(i))[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_46 $$ job.setInt("aggregator.descriptor.num", 1)[ CD ]
Line_7 $$ Path INPUT_DIR = new Path("build/test/input_for_aggregates_test")-->Line_34 $$ FileInputFormat.setInputPaths(job, INPUT_DIR)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_13 $$ StringBuffer expectedOutput = new StringBuffer()[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_27 $$ expectedOutput.append("value_as_string_min\t1\n")[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_44 $$ job.setReducerClass(ValueAggregatorReducer.class)[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_15 $$ expectedOutput.append("min\t1\n")[ CD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_35 $$ job.setInputFormat(TextInputFormat.class)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_42 $$ job.setNumReduceTasks(1)[ CD ]
Line_7 $$ Path INPUT_DIR = new Path("build/test/input_for_aggregates_test")-->Line_10 $$ fs.mkdirs(INPUT_DIR)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_43 $$ job.setMapperClass(ValueAggregatorMapper.class)[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_54 $$ System.out.println(outdata.toString())[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_49 $$ JobClient.runJob(job)[ CD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_52 $$ String outdata = MapReduceTestUtil.readOutput(outPath, job)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_40 $$ job.setOutputKeyClass(Text.class)[ CD ]
Line_13 $$ StringBuffer expectedOutput = new StringBuffer()-->Line_14 $$ expectedOutput.append("max\t19\n")[ FD ]
Line_6 $$ Path OUTPUT_DIR = new Path("build/test/output_for_aggregates_test")-->Line_36 $$ FileOutputFormat.setOutputPath(job, OUTPUT_DIR)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_55 $$ outdata = outdata.substring(0, expectedOutput.toString().length())[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_41 $$ job.setOutputValueClass(Text.class)[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_48 $$ job.setLong("aggregate.max.num.unique.values", 14)[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_6 $$ Path OUTPUT_DIR = new Path("build/test/output_for_aggregates_test")[ CD ]
Line_13 $$ StringBuffer expectedOutput = new StringBuffer()-->Line_15 $$ expectedOutput.append("min\t1\n")[ FD ]
Line_13 $$ StringBuffer expectedOutput = new StringBuffer()-->Line_26 $$ expectedOutput.append("value_as_string_max\t9\n")[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_26 $$ expectedOutput.append("value_as_string_max\t9\n")[ CD ]
Line_51 $$ Path outPath = new Path(OUTPUT_DIR, "part-00000")-->Line_52 $$ String outdata = MapReduceTestUtil.readOutput(outPath, job)[ FD ]
Line_13 $$ StringBuffer expectedOutput = new StringBuffer()-->Line_18 $$ expectedOutput.append("count_").append(idFormat.format(i))[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_9 $$ fs.delete(INPUT_DIR, true)[ CD ]
Line_17 $$ for (int i = 1; i < numOfInputLines; i++) -->Line_24 $$ inputData.append("\n")[ CD ]
Line_21 $$ for (int j = 1; j < i; j++) -->Line_22 $$ inputData.append(" ").append(idFormat.format(i))[ CD ]
Line_12 $$ StringBuffer inputData = new StringBuffer()-->Line_22 $$ inputData.append(" ").append(idFormat.format(i))[ FD ]
Line_55 $$ outdata = outdata.substring(0, expectedOutput.toString().length())-->Line_56 $$ assertEquals(expectedOutput.toString(), outdata)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_8 $$ String inputFile = "input.txt"[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_31 $$ System.out.println("inputData:")[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_11 $$ fs.delete(OUTPUT_DIR, true)[ CD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_38 $$ job.setMapOutputKeyClass(Text.class)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_17 $$ for (int i = 1; i < numOfInputLines; i++) [ CD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_43 $$ job.setMapperClass(ValueAggregatorMapper.class)[ FD ]
Line_7 $$ Path INPUT_DIR = new Path("build/test/input_for_aggregates_test")-->Line_57 $$ fs.delete(INPUT_DIR, true)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_57 $$ fs.delete(INPUT_DIR, true)[ CD ]
Line_13 $$ StringBuffer expectedOutput = new StringBuffer()-->Line_27 $$ expectedOutput.append("value_as_string_min\t1\n")[ FD ]
Line_52 $$ String outdata = MapReduceTestUtil.readOutput(outPath, job)-->Line_55 $$ outdata = outdata.substring(0, expectedOutput.toString().length())[ FD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_47 $$ job.set("aggregator.descriptor.0", "UserDefined,org.apache.hadoop.mapred.lib.aggregate.AggregatorTests")[ FD ]
Line_17 $$ for (int i = 1; i < numOfInputLines; i++) -->Line_21 $$ for (int j = 1; j < i; j++) [ FD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_45 $$ job.setCombinerClass(ValueAggregatorCombiner.class)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_50 $$ boolean success = true[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_45 $$ job.setCombinerClass(ValueAggregatorCombiner.class)[ CD ]
Line_52 $$ String outdata = MapReduceTestUtil.readOutput(outPath, job)-->Line_56 $$ assertEquals(expectedOutput.toString(), outdata)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_56 $$ assertEquals(expectedOutput.toString(), outdata)[ CD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_49 $$ JobClient.runJob(job)[ FD ]
Line_13 $$ StringBuffer expectedOutput = new StringBuffer()-->Line_28 $$ expectedOutput.append("uniq_count\t15\n")[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_7 $$ Path INPUT_DIR = new Path("build/test/input_for_aggregates_test")[ CD ]
Line_12 $$ StringBuffer inputData = new StringBuffer()-->Line_32 $$ System.out.println(inputData.toString())[ FD ]
Line_7 $$ Path INPUT_DIR = new Path("build/test/input_for_aggregates_test")-->Line_9 $$ fs.delete(INPUT_DIR, true)[ FD ]
Line_17 $$ for (int i = 1; i < numOfInputLines; i++) -->Line_19 $$ expectedOutput.append("\t").append(i).append("\n")[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_12 $$ StringBuffer inputData = new StringBuffer()[ CD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_42 $$ job.setNumReduceTasks(1)[ FD ]
Line_3 $$ JobConf conf = new JobConf(TestAggregates.class)-->Line_4 $$ FileSystem fs = FileSystem.get(conf)[ FD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_39 $$ job.setMapOutputValueClass(Text.class)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_30 $$ fileOut.close()[ CD ]
Line_13 $$ StringBuffer expectedOutput = new StringBuffer()-->Line_55 $$ outdata = outdata.substring(0, expectedOutput.toString().length())[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_37 $$ job.setOutputFormat(TextOutputFormat.class)[ CD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_48 $$ job.setLong("aggregate.max.num.unique.values", 14)[ FD ]
Line_16 $$ FSDataOutputStream fileOut = fs.create(new Path(INPUT_DIR, inputFile))-->Line_30 $$ fileOut.close()[ FD ]
Line_33 $$ JobConf job = new JobConf(conf, TestAggregates.class)-->Line_44 $$ job.setReducerClass(ValueAggregatorReducer.class)[ FD ]
Line_2 $$ public static void launch() throws Exception -->Line_16 $$ FSDataOutputStream fileOut = fs.create(new Path(INPUT_DIR, inputFile))[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_4 $$ FileSystem fs = FileSystem.get(conf)[ CD ]
Line_2 $$ public static void launch() throws Exception -->Line_52 $$ String outdata = MapReduceTestUtil.readOutput(outPath, job)[ CD ]
