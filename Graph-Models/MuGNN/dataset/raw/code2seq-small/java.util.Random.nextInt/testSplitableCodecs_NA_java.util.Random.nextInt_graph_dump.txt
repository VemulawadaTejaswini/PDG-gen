Line_3 $$ final Job job = Job.getInstance(defaultConf)-->Line_45 $$ MapContext<Text, Text, Text, Text> mcontext = new MapContextImpl<Text, Text, Text, Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), splits.get(j))[ FD ]
Line_17 $$ final int MAX_LENGTH = 500000-->Line_19 $$ for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 4) + 1) [ FD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_13 $$ LOG.info("seed = " + seed)[ CD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_44 $$ Class<?> clazz = reader.getClass()[ CD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_3 $$ final Job job = Job.getInstance(defaultConf)[ CD ]
Line_7 $$ codec = (CompressionCodec) ReflectionUtils.newInstance(conf.getClassByName("org.apache.hadoop.io.compress.BZip2Codec"), conf)-->Line_21 $$ Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)))[ FD ]
Line_39 $$ BitSet bits = new BitSet(length)-->Line_72 $$ assertEquals("Some keys in no partition.", length, bits.cardinality())[ FD ]
Line_51 $$ while (reader.nextKeyValue()) -->Line_53 $$ value = reader.getCurrentValue()[ CD ]
Line_3 $$ final Job job = Job.getInstance(defaultConf)-->Line_42 $$ TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration())[ FD ]
Line_23 $$ for (int i = 0; i < length; i++) -->Line_27 $$ writer.write("\n")[ CD ]
Line_47 $$ Text key = null-->Line_54 $$ final int k = Integer.parseInt(key.toString())[ FD ]
Line_3 $$ final Job job = Job.getInstance(defaultConf)-->Line_4 $$ final Configuration conf = job.getConfiguration()[ FD ]
Line_45 $$ MapContext<Text, Text, Text, Text> mcontext = new MapContextImpl<Text, Text, Text, Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), splits.get(j))-->Line_46 $$ reader.initialize(splits.get(j), mcontext)[ FD ]
Line_37 $$ List<InputSplit> splits = format.getSplits(job)-->Line_41 $$ LOG.debug("split[" + j + "]= " + splits.get(j))[ FD ]
Line_5 $$ CompressionCodec codec = null-->Line_7 $$ codec = (CompressionCodec) ReflectionUtils.newInstance(conf.getClassByName("org.apache.hadoop.io.compress.BZip2Codec"), conf)[ FD ]
Line_34 $$ for (int i = 0; i < 3; i++) -->Line_36 $$ LOG.info("splitting: requesting = " + numSplits)[ CD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_4 $$ final Configuration conf = job.getConfiguration()[ CD ]
Line_47 $$ Text key = null-->Line_52 $$ key = reader.getCurrentKey()[ FD ]
Line_51 $$ while (reader.nextKeyValue()) -->Line_52 $$ key = reader.getCurrentKey()[ CD ]
Line_19 $$ for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 4) + 1) -->Line_23 $$ for (int i = 0; i < length; i++) [ FD ]
Line_34 $$ for (int i = 0; i < 3; i++) -->Line_72 $$ assertEquals("Some keys in no partition.", length, bits.cardinality())[ CD ]
Line_37 $$ List<InputSplit> splits = format.getSplits(job)-->Line_64 $$ LOG.info("splits[" + j + "]=" + splits.get(j) + " count=" + count)[ FD ]
Line_51 $$ while (reader.nextKeyValue()) -->Line_56 $$ assertEquals("Bad key", 0, k % 2)[ CD ]
Line_34 $$ for (int i = 0; i < 3; i++) -->Line_37 $$ List<InputSplit> splits = format.getSplits(job)[ CD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_19 $$ for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 4) + 1) [ CD ]
Line_37 $$ List<InputSplit> splits = format.getSplits(job)-->Line_40 $$ for (int j = 0; j < splits.size(); j++) [ FD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_41 $$ LOG.debug("split[" + j + "]= " + splits.get(j))[ FD ]
Line_37 $$ List<InputSplit> splits = format.getSplits(job)-->Line_45 $$ MapContext<Text, Text, Text, Text> mcontext = new MapContextImpl<Text, Text, Text, Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), splits.get(j))[ FD ]
Line_14 $$ Random random = new Random(seed)-->Line_19 $$ for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 4) + 1) [ FD ]
Line_19 $$ for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 4) + 1) -->Line_72 $$ assertEquals("Some keys in no partition.", length, bits.cardinality())[ FD ]
Line_21 $$ Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)))-->Line_25 $$ writer.write("\t")[ FD ]
Line_55 $$ final int v = Integer.parseInt(value.toString())-->Line_59 $$ assertFalse(k + "," + v + " in multiple partitions.", bits.get(v))[ FD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_46 $$ reader.initialize(splits.get(j), mcontext)[ FD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_64 $$ LOG.info("splits[" + j + "]=" + splits.get(j) + " count=" + count)[ FD ]
Line_5 $$ CompressionCodec codec = null-->Line_11 $$ Path file = new Path(workDir, "test" + codec.getDefaultExtension())[ FD ]
Line_23 $$ for (int i = 0; i < length; i++) -->Line_26 $$ writer.write(Integer.toString(i))[ FD ]
Line_63 $$ if (count > 0) -->Line_66 $$ LOG.debug("splits[" + j + "]=" + splits.get(j) + " count=" + count)[ CD ]
Line_34 $$ for (int i = 0; i < 3; i++) -->Line_35 $$ int numSplits = random.nextInt(MAX_LENGTH / 2000) + 1[ CD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_66 $$ LOG.debug("splits[" + j + "]=" + splits.get(j) + " count=" + count)[ FD ]
Line_7 $$ codec = (CompressionCodec) ReflectionUtils.newInstance(conf.getClassByName("org.apache.hadoop.io.compress.BZip2Codec"), conf)-->Line_11 $$ Path file = new Path(workDir, "test" + codec.getDefaultExtension())[ FD ]
Line_63 $$ if (count > 0) -->Line_64 $$ LOG.info("splits[" + j + "]=" + splits.get(j) + " count=" + count)[ CD ]
Line_51 $$ while (reader.nextKeyValue()) -->Line_59 $$ assertFalse(k + "," + v + " in multiple partitions.", bits.get(v))[ CD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_43 $$ RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context)[ FD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_45 $$ MapContext<Text, Text, Text, Text> mcontext = new MapContextImpl<Text, Text, Text, Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), splits.get(j))[ CD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_46 $$ reader.initialize(splits.get(j), mcontext)[ CD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_5 $$ CompressionCodec codec = null[ CD ]
Line_23 $$ for (int i = 0; i < length; i++) -->Line_34 $$ for (int i = 0; i < 3; i++) [ FD ]
Line_19 $$ for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 4) + 1) -->Line_20 $$ LOG.info("creating; entries = " + length)[ CD ]
Line_14 $$ Random random = new Random(seed)-->Line_35 $$ int numSplits = random.nextInt(MAX_LENGTH / 2000) + 1[ FD ]
Line_3 $$ final Job job = Job.getInstance(defaultConf)-->Line_37 $$ List<InputSplit> splits = format.getSplits(job)[ FD ]
Line_48 $$ Text value = null-->Line_53 $$ value = reader.getCurrentValue()[ FD ]
Line_3 $$ final Job job = Job.getInstance(defaultConf)-->Line_18 $$ FileInputFormat.setMaxInputSplitSize(job, MAX_LENGTH / 20)[ FD ]
Line_37 $$ List<InputSplit> splits = format.getSplits(job)-->Line_66 $$ LOG.debug("splits[" + j + "]=" + splits.get(j) + " count=" + count)[ FD ]
Line_11 $$ Path file = new Path(workDir, "test" + codec.getDefaultExtension())-->Line_21 $$ Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)))[ FD ]
Line_4 $$ final Configuration conf = job.getConfiguration()-->Line_7 $$ codec = (CompressionCodec) ReflectionUtils.newInstance(conf.getClassByName("org.apache.hadoop.io.compress.BZip2Codec"), conf)[ FD ]
Line_23 $$ for (int i = 0; i < length; i++) -->Line_24 $$ writer.write(Integer.toString(i * 2))[ CD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_12 $$ int seed = new Random().nextInt()[ CD ]
Line_21 $$ Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)))-->Line_26 $$ writer.write(Integer.toString(i))[ FD ]
Line_51 $$ while (reader.nextKeyValue()) -->Line_60 $$ bits.set(v)[ CD ]
Line_43 $$ RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context)-->Line_52 $$ key = reader.getCurrentKey()[ FD ]
Line_3 $$ final Job job = Job.getInstance(defaultConf)-->Line_33 $$ assertTrue("KVTIF claims not splittable", format.isSplitable(job, file))[ FD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_41 $$ LOG.debug("split[" + j + "]= " + splits.get(j))[ CD ]
Line_42 $$ TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration())-->Line_43 $$ RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context)[ FD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_16 $$ FileInputFormat.setInputPaths(job, workDir)[ CD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_18 $$ FileInputFormat.setMaxInputSplitSize(job, MAX_LENGTH / 20)[ CD ]
Line_50 $$ int count = 0-->Line_61 $$ count++[ FD ]
Line_19 $$ for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 4) + 1) -->Line_21 $$ Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)))[ CD ]
Line_34 $$ for (int i = 0; i < 3; i++) -->Line_39 $$ BitSet bits = new BitSet(length)[ CD ]
Line_23 $$ for (int i = 0; i < length; i++) -->Line_25 $$ writer.write("\t")[ CD ]
Line_34 $$ for (int i = 0; i < 3; i++) -->Line_38 $$ LOG.info("splitting: got =        " + splits.size())[ CD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_15 $$ localFs.delete(workDir, true)[ CD ]
Line_34 $$ for (int i = 0; i < 3; i++) -->Line_40 $$ for (int j = 0; j < splits.size(); j++) [ CD ]
Line_43 $$ RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context)-->Line_69 $$ reader.close()[ FD ]
Line_43 $$ RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context)-->Line_53 $$ value = reader.getCurrentValue()[ FD ]
Line_21 $$ Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)))-->Line_30 $$ writer.close()[ FD ]
Line_19 $$ for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 4) + 1) -->Line_33 $$ assertTrue("KVTIF claims not splittable", format.isSplitable(job, file))[ CD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_43 $$ RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context)[ CD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_48 $$ Text value = null[ CD ]
Line_48 $$ Text value = null-->Line_55 $$ final int v = Integer.parseInt(value.toString())[ FD ]
Line_51 $$ while (reader.nextKeyValue()) -->Line_57 $$ assertEquals("Mismatched key/value", k / 2, v)[ CD ]
Line_39 $$ BitSet bits = new BitSet(length)-->Line_60 $$ bits.set(v)[ FD ]
Line_37 $$ List<InputSplit> splits = format.getSplits(job)-->Line_43 $$ RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context)[ FD ]
Line_55 $$ final int v = Integer.parseInt(value.toString())-->Line_57 $$ assertEquals("Mismatched key/value", k / 2, v)[ FD ]
Line_11 $$ Path file = new Path(workDir, "test" + codec.getDefaultExtension())-->Line_33 $$ assertTrue("KVTIF claims not splittable", format.isSplitable(job, file))[ FD ]
Line_51 $$ while (reader.nextKeyValue()) -->Line_61 $$ count++[ CD ]
Line_55 $$ final int v = Integer.parseInt(value.toString())-->Line_60 $$ bits.set(v)[ FD ]
Line_43 $$ RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context)-->Line_46 $$ reader.initialize(splits.get(j), mcontext)[ FD ]
Line_3 $$ final Job job = Job.getInstance(defaultConf)-->Line_16 $$ FileInputFormat.setInputPaths(job, workDir)[ FD ]
Line_43 $$ RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context)-->Line_44 $$ Class<?> clazz = reader.getClass()[ FD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_47 $$ Text key = null[ CD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_45 $$ MapContext<Text, Text, Text, Text> mcontext = new MapContextImpl<Text, Text, Text, Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), splits.get(j))[ FD ]
Line_51 $$ while (reader.nextKeyValue()) -->Line_58 $$ LOG.debug("read " + k + "," + v)[ CD ]
Line_43 $$ RecordReader<Text, Text> reader = format.createRecordReader(splits.get(j), context)-->Line_51 $$ while (reader.nextKeyValue()) [ FD ]
Line_42 $$ TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration())-->Line_45 $$ MapContext<Text, Text, Text, Text> mcontext = new MapContextImpl<Text, Text, Text, Text>(job.getConfiguration(), context.getTaskAttemptID(), reader, null, null, MapReduceTestUtil.createDummyReporter(), splits.get(j))[ FD ]
Line_53 $$ value = reader.getCurrentValue()-->Line_55 $$ final int v = Integer.parseInt(value.toString())[ FD ]
Line_21 $$ Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)))-->Line_24 $$ writer.write(Integer.toString(i * 2))[ FD ]
Line_5 $$ CompressionCodec codec = null-->Line_21 $$ Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)))[ FD ]
Line_23 $$ for (int i = 0; i < length; i++) -->Line_26 $$ writer.write(Integer.toString(i))[ CD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_17 $$ final int MAX_LENGTH = 500000[ CD ]
Line_52 $$ key = reader.getCurrentKey()-->Line_54 $$ final int k = Integer.parseInt(key.toString())[ FD ]
Line_50 $$ int count = 0-->Line_63 $$ if (count > 0) [ FD ]
Line_37 $$ List<InputSplit> splits = format.getSplits(job)-->Line_38 $$ LOG.info("splitting: got =        " + splits.size())[ FD ]
Line_51 $$ while (reader.nextKeyValue()) -->Line_54 $$ final int k = Integer.parseInt(key.toString())[ CD ]
Line_37 $$ List<InputSplit> splits = format.getSplits(job)-->Line_46 $$ reader.initialize(splits.get(j), mcontext)[ FD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_11 $$ Path file = new Path(workDir, "test" + codec.getDefaultExtension())[ CD ]
Line_19 $$ for (int length = 0; length < MAX_LENGTH; length += random.nextInt(MAX_LENGTH / 4) + 1) -->Line_34 $$ for (int i = 0; i < 3; i++) [ CD ]
Line_40 $$ for (int j = 0; j < splits.size(); j++) -->Line_42 $$ TaskAttemptContext context = MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration())[ CD ]
Line_51 $$ while (reader.nextKeyValue()) -->Line_55 $$ final int v = Integer.parseInt(value.toString())[ CD ]
Line_21 $$ Writer writer = new OutputStreamWriter(codec.createOutputStream(localFs.create(file)))-->Line_27 $$ writer.write("\n")[ FD ]
Line_2 $$ public void testSplitableCodecs() throws Exception -->Line_14 $$ Random random = new Random(seed)[ CD ]
Line_39 $$ BitSet bits = new BitSet(length)-->Line_59 $$ assertFalse(k + "," + v + " in multiple partitions.", bits.get(v))[ FD ]
